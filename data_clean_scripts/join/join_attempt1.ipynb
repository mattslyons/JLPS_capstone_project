{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN\n",
    "\n",
    "Joining the schools, wind, population, pm2.5 readings, and pollutions sources datasets all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anand Notes:\n",
    "jan-2000, school1, zip, lat, lon\n",
    "...\n",
    "jan-2000, school1300, zip, lat, lon\n",
    "feb-2000, school1, zip, lat, lon\n",
    "...\n",
    "feb-2000, school1300, zip, lat, lon\n",
    "\n",
    "520,000,000\n",
    "\n",
    "use lat, lon to find nearest pollution site, get bearing as pollution_bearing (using geodesic inv library)\n",
    "\n",
    "join wind u, v onto school's zip-month-year, use trig to make school_wind_bearing\n",
    "\n",
    "join wind u, v onto pollution site's zip-month-year, use trig to make  pollution_wind_bearing\n",
    "\n",
    "\n",
    "make 3 variables \n",
    "\n",
    "- abs(pollution_bearing - school_wind_bearing) as school_wind_downstream_angle\n",
    "\n",
    "- abs(pollution_bearing - pollution_wind_bearing) as pollution_wind_downstream_angle\n",
    "\n",
    "- mean(school_wind_downstream_angle, pollution_wind_downstream_angle) as mean_downstream_angle\n",
    "\n",
    "\n",
    "---> group by zip-month-year\n",
    "\n",
    "~ 400,000 rows\n",
    "\n",
    "aggregations:\n",
    "\n",
    "- avg(school_wind_downstream_angle) as zip_avg_school_wind_downstream_angle\n",
    "\n",
    "- avg(pollution_wind_downstream_angle) as zip_avg_pollution_wind_downstream_angle\n",
    "\n",
    "- avg(mean_downstream_angle) as zip_avg_mean_downstream_angle\n",
    "\n",
    "\n",
    "year, month, zip, year-month-zip, zip_avg_school_wind_downstream_angle, zip_avg_pollution_wind_downstream_angle, zip_avg_mean_downstream_angle\n",
    "\n",
    "\n",
    "join in population stuff on year-zipcode\n",
    "\n",
    "join in pm2.5 on year-month-zipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-09-27-18-07-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.ticker as mticker\n",
    "import plotly.express as px\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "# import dotenv\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = 'work'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  gdrive_path = '/content/gdrive/MyDrive/Classes/W210_capstone'\n",
    "  env_path = '/content/gdrive/MyDrive/.env'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  gdrive_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone'\n",
    "  env_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/MyDrive/.env'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  gdrive_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone'\n",
    "  env_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = pd.read_csv(os.path.join(gdrive_path, 'W210 Capstone/Data/census/census_bureau_clean/census_bureau.csv'))\n",
    "df_wind = pd.read_parquet(os.path.join(gdrive_path, 'W210 Capstone/Data/wind'))\n",
    "df_pollution = pd.read_csv(os.path.join(gdrive_path, 'W210 Capstone/Data/AirPollution/UW_pm25_zip_monthly_anand_2000-2018-v2.csv'))\n",
    "df_point_sources = pd.read_csv(os.path.join(gdrive_path, 'W210 Capstone/Data/Point source/pollution_point_sources.csv'))\n",
    "\n",
    "file_encoding = 'utf8'\n",
    "with open(os.path.join(gdrive_path, 'JLPS_capstone_project/data/schools_data/filtered_joined_schools_data.csv'), encoding=file_encoding, errors = 'backslashreplace') as my_csv:\n",
    "  df_schools = pd.read_csv(my_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pollution point sources assumptions\n",
    "- For a given year, just find the nearest pollution source for that year. Dont try and track a pollution source across time. \n",
    "- We have data every 3 years, assume the following for interpolating:\n",
    "  - 2002 represents 2000 - 2002\n",
    "  - 2005 represents 2003 - 2005\n",
    "  - ... \n",
    "  - 2017 represents 2015 - 2019\n",
    "\n",
    "Concerns:\n",
    "- 871 schools dont have any wind observations because their zip code wasn't found. Investigate that further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean schools \n",
    "df_schools.columns = [i.lower() for i in df_schools.columns]\n",
    "# only select necessary fields\n",
    "df_schools = df_schools[['cdscode', 'statustype', 'county', 'street', 'zip_first_five', 'opendate', 'closeddate', 'eilname', 'gsoffered', \n",
    "  'latitude', 'longitude', 'lastupdate']]\\\n",
    "  .rename(columns={\n",
    "    'statustype': 'school_active_status', 'county': 'school_county', 'street': 'school_street', \n",
    "    'zip_first_five': 'school_zip', 'opendate': 'school_open_date', 'closeddate': 'school_closed_date', \n",
    "    'eilname': 'school_type', 'gsoffered': 'school_grades_offered', 'latitude': 'school_lat', 'longitude': 'school_lon', \n",
    "    'lastupdate': 'school_last_updated_date'})\n",
    "\n",
    "# clean wind\n",
    "df_wind = df_wind.rename(columns={'lat': 'wind_lat', 'lon': 'wind_lon'})\n",
    "df_wind['year_month'] = df_wind['year_month'].astype(str).map(lambda x: x[:4] + '-' + x[-2:])\n",
    "df_wind['year'] = df_wind['year_month'].map(lambda x: int(x[:4]))\n",
    "df_wind['ZCTA10'] = df_wind['ZCTA10'].astype(int)\n",
    "df_wind = df_wind[(df_wind['year'] >= 2000) & (df_wind['year'] <= 2019)]\n",
    "\n",
    "# clean pollution\n",
    "df_pollution = df_pollution.drop(columns=['Unnamed: 0', 'GEOID10', 'year_month_zip'])\n",
    "\n",
    "# clean pollution point sources\n",
    "df_point_sources = df_point_sources.rename(columns={'zip_code': 'point_source_zip'})\n",
    "df_point_sources['point_source_zip'] = df_point_sources['point_source_zip'].astype(int)\n",
    "# create an ID field for easier lookups\n",
    "df_point_sources['point_source_id'] = [i for i in range(df_point_sources.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join schools, wind, census, and pm2.5 readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_schools, df_wind, left_on = 'school_zip', right_on='ZCTA10', how='left')\\\n",
    "  .merge(df_census, left_on = ['school_zip', 'year'], right_on=['zip', 'year'], how='left')\\\n",
    "  .merge(df_pollution, left_on=['school_zip', 'year_month'], right_on=['ZIP10', 'year_month'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wind[['year_month']].drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 240 year month combos\n",
      "So most schools are repeated 240 times, for the schools that dont have a zip code in the wind data, there are no obs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cdscode\n",
       "240    12426\n",
       "1        871\n",
       "dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each school is repeated for every year-month combo. But some schools dont have wind/population data where we dont have that zip code in those datasets. \n",
    "yr_mo = df_wind[['year_month']].drop_duplicates().shape[0]\n",
    "print(f'There are {yr_mo} year month combos')\n",
    "print('So most schools are repeated 240 times, for the schools that dont have a zip code in the wind data, there are no obs')\n",
    "df_all['cdscode'].value_counts().to_frame().value_counts('cdscode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lat/Lon Join\n",
    "\n",
    "Join the above dataset to pull the nearest pollution source by year. \n",
    "\n",
    "Do so by creating a school <--> source mapping by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the year mapping since we dont have all years available in the pollution sources. \n",
    "# thus, we have to interpolate for the missing years\n",
    "year_mapping = {\n",
    "  2000: 2002, \n",
    "  2001: 2002, \n",
    "  2002: 2002,\n",
    "  2003: 2005,\n",
    "  2004: 2005,\n",
    "  2005: 2005,\n",
    "  2006: 2008,\n",
    "  2007: 2008,\n",
    "  2008: 2008,\n",
    "  2009: 2011,\n",
    "  2010: 2011,\n",
    "  2011: 2011,\n",
    "  2012: 2014,\n",
    "  2013: 2014,\n",
    "  2014: 2014,\n",
    "  2015: 2017,\n",
    "  2016: 2017,\n",
    "  2017: 2017,\n",
    "  2018: 2017,\n",
    "  2019: 2017\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checked_lat</th>\n",
       "      <th>checked_lon</th>\n",
       "      <th>point_source_zip</th>\n",
       "      <th>report_year</th>\n",
       "      <th>PM25_emissions_TPY</th>\n",
       "      <th>point_source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.811466</td>\n",
       "      <td>-117.915550</td>\n",
       "      <td>92803</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.787854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.088242</td>\n",
       "      <td>-117.470116</td>\n",
       "      <td>92335</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.789200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.911602</td>\n",
       "      <td>-118.281799</td>\n",
       "      <td>93420</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.791300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.944618</td>\n",
       "      <td>-121.325859</td>\n",
       "      <td>95203</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.797500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.221817</td>\n",
       "      <td>-121.054955</td>\n",
       "      <td>95945</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.801540</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7150</th>\n",
       "      <td>38.025100</td>\n",
       "      <td>-122.063900</td>\n",
       "      <td>94553</td>\n",
       "      <td>2017</td>\n",
       "      <td>265.824083</td>\n",
       "      <td>7150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7151</th>\n",
       "      <td>34.622200</td>\n",
       "      <td>-117.100100</td>\n",
       "      <td>92307</td>\n",
       "      <td>2017</td>\n",
       "      <td>494.738668</td>\n",
       "      <td>7151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7152</th>\n",
       "      <td>34.231230</td>\n",
       "      <td>-116.056220</td>\n",
       "      <td>92778</td>\n",
       "      <td>2017</td>\n",
       "      <td>534.734811</td>\n",
       "      <td>7152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7153</th>\n",
       "      <td>37.938779</td>\n",
       "      <td>-122.396453</td>\n",
       "      <td>94802</td>\n",
       "      <td>2017</td>\n",
       "      <td>566.232588</td>\n",
       "      <td>7153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7154</th>\n",
       "      <td>38.016594</td>\n",
       "      <td>-122.115392</td>\n",
       "      <td>94553</td>\n",
       "      <td>2017</td>\n",
       "      <td>640.156512</td>\n",
       "      <td>7154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7155 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      checked_lat  checked_lon  point_source_zip  report_year  \\\n",
       "0       33.811466  -117.915550             92803         2002   \n",
       "1       34.088242  -117.470116             92335         2002   \n",
       "2       33.911602  -118.281799             93420         2002   \n",
       "3       37.944618  -121.325859             95203         2002   \n",
       "4       39.221817  -121.054955             95945         2002   \n",
       "...           ...          ...               ...          ...   \n",
       "7150    38.025100  -122.063900             94553         2017   \n",
       "7151    34.622200  -117.100100             92307         2017   \n",
       "7152    34.231230  -116.056220             92778         2017   \n",
       "7153    37.938779  -122.396453             94802         2017   \n",
       "7154    38.016594  -122.115392             94553         2017   \n",
       "\n",
       "      PM25_emissions_TPY  point_source_id  \n",
       "0               1.787854                0  \n",
       "1               1.789200                1  \n",
       "2               1.791300                2  \n",
       "3               1.797500                3  \n",
       "4               1.801540                4  \n",
       "...                  ...              ...  \n",
       "7150          265.824083             7150  \n",
       "7151          494.738668             7151  \n",
       "7152          534.734811             7152  \n",
       "7153          566.232588             7153  \n",
       "7154          640.156512             7154  \n",
       "\n",
       "[7155 rows x 6 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_point_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(df_all, df_point_sources, data_year = 2010, partitions = 5, verbose=True):\n",
    "\n",
    "  df_school_yr = df_all[df_all.year == data_year][['cdscode', 'school_lat', 'school_lon']].drop_duplicates()\n",
    "  df_ps = df_point_sources[df_point_sources.report_year == year_mapping[data_year]]\n",
    "\n",
    "  # split data into partitions to avoid overloading memory\n",
    "  # then loop through each partition and perform the operations\n",
    "  out_list = []\n",
    "  for i in range(partitions):\n",
    "    if verbose:\n",
    "      print(f'Year: {data_year}. Partition {i+1} of {partitions}')\n",
    "      \n",
    "    df_school_yr_i = df_school_yr[df_school_yr['cdscode'] % partitions == i]\n",
    "\n",
    "    # cross join\n",
    "    df_school_yr_i['key'] = 0\n",
    "    df_ps['key'] = 0\n",
    "    df_cross = pd.merge(df_school_yr_i, df_ps, on = 'key', how='outer')\n",
    "\n",
    "    # calc distances\n",
    "    def calc_distance(lat1, lng1, lat2, lng2):\n",
    "      return ((lat1 - lat2)**2 + (lng1 - lng2)**2)**.5\n",
    "\n",
    "    df_cross['pollution_school_distance'] = df_cross\\\n",
    "      .apply(lambda df: calc_distance(df['school_lat'], df['school_lon'], df['checked_lat'], df['checked_lon']), axis=1)\n",
    "\n",
    "    # filter on closest distance per school\n",
    "    df_closest = df_cross.loc[df_cross.groupby('cdscode').pollution_school_distance.idxmin()]\n",
    "\n",
    "    # add to list and repeat for other partitions\n",
    "    out_list.append(df_closest)\n",
    "\n",
    "  df_out = pd.concat(out_list, ignore_index=True)\n",
    "\n",
    "  df_out['year'] = data_year\n",
    "  df_out = df_out.drop(columns=['key'])\n",
    "\n",
    "  return df_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on partition 1 of 5\n",
      "Working on partition 2 of 5\n",
      "Working on partition 3 of 5\n",
      "Working on partition 4 of 5\n",
      "Working on partition 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# test on 1 year. but run on all years below\n",
    "df_2000 = get_nearest(df_all, df_point_sources, data_year = 2000, partitions = 5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create School <--> Pollution Source Mapping\n",
    "\n",
    "Run the function above on all years. \n",
    "- This takes about 3.5 - 3.75 min per year\n",
    "- ended up taking 88 min for all years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2000. Partition 1 of 5\n",
      "Year: 2000. Partition 2 of 5\n",
      "Year: 2000. Partition 3 of 5\n",
      "Year: 2000. Partition 4 of 5\n",
      "Year: 2000. Partition 5 of 5\n",
      "Year: 2001. Partition 1 of 5\n",
      "Year: 2001. Partition 2 of 5\n",
      "Year: 2001. Partition 3 of 5\n",
      "Year: 2001. Partition 4 of 5\n",
      "Year: 2001. Partition 5 of 5\n",
      "Year: 2002. Partition 1 of 5\n",
      "Year: 2002. Partition 2 of 5\n",
      "Year: 2002. Partition 3 of 5\n",
      "Year: 2002. Partition 4 of 5\n",
      "Year: 2002. Partition 5 of 5\n",
      "Year: 2003. Partition 1 of 5\n",
      "Year: 2003. Partition 2 of 5\n",
      "Year: 2003. Partition 3 of 5\n",
      "Year: 2003. Partition 4 of 5\n",
      "Year: 2003. Partition 5 of 5\n",
      "Year: 2004. Partition 1 of 5\n",
      "Year: 2004. Partition 2 of 5\n",
      "Year: 2004. Partition 3 of 5\n",
      "Year: 2004. Partition 4 of 5\n",
      "Year: 2004. Partition 5 of 5\n",
      "Year: 2005. Partition 1 of 5\n",
      "Year: 2005. Partition 2 of 5\n",
      "Year: 2005. Partition 3 of 5\n",
      "Year: 2005. Partition 4 of 5\n",
      "Year: 2005. Partition 5 of 5\n",
      "Year: 2006. Partition 1 of 5\n",
      "Year: 2006. Partition 2 of 5\n",
      "Year: 2006. Partition 3 of 5\n",
      "Year: 2006. Partition 4 of 5\n",
      "Year: 2006. Partition 5 of 5\n",
      "Year: 2007. Partition 1 of 5\n",
      "Year: 2007. Partition 2 of 5\n",
      "Year: 2007. Partition 3 of 5\n",
      "Year: 2007. Partition 4 of 5\n",
      "Year: 2007. Partition 5 of 5\n",
      "Year: 2008. Partition 1 of 5\n",
      "Year: 2008. Partition 2 of 5\n",
      "Year: 2008. Partition 3 of 5\n",
      "Year: 2008. Partition 4 of 5\n",
      "Year: 2008. Partition 5 of 5\n",
      "Year: 2009. Partition 1 of 5\n",
      "Year: 2009. Partition 2 of 5\n",
      "Year: 2009. Partition 3 of 5\n",
      "Year: 2009. Partition 4 of 5\n",
      "Year: 2009. Partition 5 of 5\n",
      "Year: 2010. Partition 1 of 5\n",
      "Year: 2010. Partition 2 of 5\n",
      "Year: 2010. Partition 3 of 5\n",
      "Year: 2010. Partition 4 of 5\n",
      "Year: 2010. Partition 5 of 5\n",
      "Year: 2011. Partition 1 of 5\n",
      "Year: 2011. Partition 2 of 5\n",
      "Year: 2011. Partition 3 of 5\n",
      "Year: 2011. Partition 4 of 5\n",
      "Year: 2011. Partition 5 of 5\n",
      "Year: 2012. Partition 1 of 5\n",
      "Year: 2012. Partition 2 of 5\n",
      "Year: 2012. Partition 3 of 5\n",
      "Year: 2012. Partition 4 of 5\n",
      "Year: 2012. Partition 5 of 5\n",
      "Year: 2013. Partition 1 of 5\n",
      "Year: 2013. Partition 2 of 5\n",
      "Year: 2013. Partition 3 of 5\n",
      "Year: 2013. Partition 4 of 5\n",
      "Year: 2013. Partition 5 of 5\n",
      "Year: 2014. Partition 1 of 5\n",
      "Year: 2014. Partition 2 of 5\n",
      "Year: 2014. Partition 3 of 5\n",
      "Year: 2014. Partition 4 of 5\n",
      "Year: 2014. Partition 5 of 5\n",
      "Year: 2015. Partition 1 of 5\n",
      "Year: 2015. Partition 2 of 5\n",
      "Year: 2015. Partition 3 of 5\n",
      "Year: 2015. Partition 4 of 5\n",
      "Year: 2015. Partition 5 of 5\n",
      "Year: 2016. Partition 1 of 5\n",
      "Year: 2016. Partition 2 of 5\n",
      "Year: 2016. Partition 3 of 5\n",
      "Year: 2016. Partition 4 of 5\n",
      "Year: 2016. Partition 5 of 5\n",
      "Year: 2017. Partition 1 of 5\n",
      "Year: 2017. Partition 2 of 5\n",
      "Year: 2017. Partition 3 of 5\n",
      "Year: 2017. Partition 4 of 5\n",
      "Year: 2017. Partition 5 of 5\n",
      "Year: 2018. Partition 1 of 5\n",
      "Year: 2018. Partition 2 of 5\n",
      "Year: 2018. Partition 3 of 5\n",
      "Year: 2018. Partition 4 of 5\n",
      "Year: 2018. Partition 5 of 5\n",
      "Year: 2019. Partition 1 of 5\n",
      "Year: 2019. Partition 2 of 5\n",
      "Year: 2019. Partition 3 of 5\n",
      "Year: 2019. Partition 4 of 5\n",
      "Year: 2019. Partition 5 of 5\n",
      "CPU times: user 1h 27min 58s, sys: 38.1 s, total: 1h 28min 36s\n",
      "Wall time: 1h 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "school_ps_mapping = [get_nearest(df_all, df_point_sources, data_year = i, partitions = 5) for i in range(2000, 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this mapping table as parquet to disk\n",
    "school_ps_mapping_df = pd.concat(school_ps_mapping)\n",
    "fpath = os.path.join(gdrive_path, 'W210 Capstone/Data/school_pollution_mapping/school_pollution_mapping.parquet')\n",
    "school_ps_mapping_df.to_parquet(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248520, 11)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "school_ps_mapping_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
