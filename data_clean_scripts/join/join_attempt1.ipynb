{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN\n",
    "\n",
    "This script joins the following datasets, all using pandas:\n",
    "- Schools\n",
    "- Wind \n",
    "- PM2.5 readings\n",
    "- Population\n",
    "- Closest pollution sources for each school\n",
    "\n",
    "<br>\n",
    "\n",
    "## Diagram of how the join will work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-09-27-18-07-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.ticker as mticker\n",
    "import plotly.express as px\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "# import cartopy.crs as ccrs\n",
    "# from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "# import dotenv\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_source = 'local'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  gdrive_path = '/content/gdrive/MyDrive/Classes/W210_capstone'\n",
    "  env_path = '/content/gdrive/MyDrive/.env'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  gdrive_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone'\n",
    "  env_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/MyDrive/.env'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  gdrive_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone'\n",
    "  env_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = pd.read_csv(os.path.join(gdrive_path, 'W210_Capstone/Data/census/census_bureau_clean/census_bureau.csv'))\n",
    "df_wind = pd.read_parquet(os.path.join(gdrive_path, 'W210_Capstone/Data/wind'))\n",
    "df_pollution = pd.read_csv(os.path.join(gdrive_path, 'W210_Capstone/Data/AirPollution/UW_pm25_zip_monthly_anand_2000-2018-v2.csv'))\n",
    "df_point_sources = pd.read_csv(os.path.join(gdrive_path, 'W210_Capstone/Data/Point source/pollution_point_sources.csv'))\n",
    "\n",
    "file_encoding = 'utf8'\n",
    "with open(os.path.join(gdrive_path, 'JLPS_capstone_project/data/schools_data/filtered_joined_schools_data.csv'), encoding=file_encoding, errors = 'backslashreplace') as my_csv:\n",
    "  df_schools = pd.read_csv(my_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pollution point sources assumptions\n",
    "- For a given year, just find the nearest pollution source for that year. Dont try and track a pollution source across time, and use that same source. \n",
    "- We have data every 3 years, assume the following for interpolating the in-between years:\n",
    "  - 2002 represents 2000 - 2002\n",
    "  - 2005 represents 2003 - 2005\n",
    "  - ... \n",
    "  - 2017 represents 2015 - 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Clean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean schools \n",
    "df_schools.columns = [i.lower() for i in df_schools.columns]\n",
    "# only select necessary fields\n",
    "df_schools = df_schools[['cdscode', 'statustype', 'county', 'street', 'zip_first_five', 'opendate', 'closeddate', 'eilname', 'gsoffered', \n",
    "  'latitude', 'longitude', 'lastupdate']]\\\n",
    "  .rename(columns={\n",
    "    'statustype': 'school_active_status', 'county': 'school_county', 'street': 'school_street', \n",
    "    'zip_first_five': 'school_zip', 'opendate': 'school_open_date', 'closeddate': 'school_closed_date', \n",
    "    'eilname': 'school_type', 'gsoffered': 'school_grades_offered', 'latitude': 'school_lat', 'longitude': 'school_lon', \n",
    "    'lastupdate': 'school_last_updated_date'})\n",
    "\n",
    "# clean wind\n",
    "df_wind = df_wind.rename(columns={'lat': 'wind_lat', 'lon': 'wind_lon'})\n",
    "df_wind['year_month'] = df_wind['year_month'].astype(str).map(lambda x: x[:4] + '-' + x[-2:])\n",
    "df_wind['year'] = df_wind['year_month'].map(lambda x: int(x[:4]))\n",
    "df_wind['ZCTA10'] = df_wind['ZCTA10'].astype(int)\n",
    "df_wind = df_wind[(df_wind['year'] >= 2000) & (df_wind['year'] <= 2019)]\n",
    "\n",
    "# clean pollution\n",
    "df_pollution = df_pollution.drop(columns=['Unnamed: 0', 'GEOID10', 'year_month_zip'])\n",
    "\n",
    "# clean pollution point sources\n",
    "df_point_sources = df_point_sources.rename(columns={'zip_code': 'point_source_zip'})\n",
    "df_point_sources['point_source_zip'] = df_point_sources['point_source_zip'].astype(int)\n",
    "# create an ID field for easier lookups\n",
    "df_point_sources['point_source_id'] = [i for i in range(df_point_sources.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join schools, wind, census, and pm2.5 readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_schools, df_wind, left_on = 'school_zip', right_on='ZCTA10', how='left')\\\n",
    "  .merge(df_census, left_on = ['school_zip', 'year'], right_on=['zip', 'year'], how='left')\\\n",
    "  .merge(df_pollution, left_on=['school_zip', 'year_month'], right_on=['ZIP10', 'year_month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 240 year month combos\n",
      "So most schools are repeated 240 times, for the schools that dont have a zip code in the wind data, there are no obs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cdscode\n",
       "240    12426\n",
       "1        871\n",
       "dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each school is repeated for every year-month combo. But some schools dont have wind/population data where we dont have that zip code in those datasets. \n",
    "yr_mo = df_wind[['year_month']].drop_duplicates().shape[0]\n",
    "print(f'There are {yr_mo} year month combos')\n",
    "print('So most schools are repeated 240 times, for the schools that dont have a zip code in the wind data, there are no obs')\n",
    "df_all['cdscode'].value_counts().to_frame().value_counts('cdscode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lat/Lon Join\n",
    "\n",
    "Join the above dataset to pull the nearest pollution source by year. \n",
    "\n",
    "Do so by creating a school <--> source mapping by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the year mapping since we dont have all years available in the pollution sources. \n",
    "# thus, we have to interpolate for the missing years\n",
    "year_mapping = {\n",
    "  2000: 2002, \n",
    "  2001: 2002, \n",
    "  2002: 2002,\n",
    "  2003: 2005,\n",
    "  2004: 2005,\n",
    "  2005: 2005,\n",
    "  2006: 2008,\n",
    "  2007: 2008,\n",
    "  2008: 2008,\n",
    "  2009: 2011,\n",
    "  2010: 2011,\n",
    "  2011: 2011,\n",
    "  2012: 2014,\n",
    "  2013: 2014,\n",
    "  2014: 2014,\n",
    "  2015: 2017,\n",
    "  2016: 2017,\n",
    "  2017: 2017,\n",
    "  2018: 2017,\n",
    "  2019: 2017\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(df_all, df_point_sources, data_year = 2010, partitions = 5, verbose=True):\n",
    "\n",
    "  df_school_yr = df_all[df_all.year == data_year][['cdscode', 'school_lat', 'school_lon']].drop_duplicates()\n",
    "  df_ps = df_point_sources[df_point_sources.report_year == year_mapping[data_year]]\n",
    "\n",
    "  # split data into partitions to avoid overloading memory\n",
    "  # then loop through each partition and perform the operations\n",
    "  out_list = []\n",
    "  for i in range(partitions):\n",
    "    if verbose:\n",
    "      print(f'Year: {data_year}. Partition {i+1} of {partitions}')\n",
    "      \n",
    "    df_school_yr_i = df_school_yr[df_school_yr['cdscode'] % partitions == i]\n",
    "\n",
    "    # cross join\n",
    "    df_school_yr_i['key'] = 0\n",
    "    df_ps['key'] = 0\n",
    "    df_cross = pd.merge(df_school_yr_i, df_ps, on = 'key', how='outer')\n",
    "\n",
    "    # calc distances\n",
    "    def calc_distance(lat1, lng1, lat2, lng2):\n",
    "      return ((lat1 - lat2)**2 + (lng1 - lng2)**2)**.5\n",
    "\n",
    "    df_cross['pollution_school_distance'] = df_cross\\\n",
    "      .apply(lambda df: calc_distance(df['school_lat'], df['school_lon'], df['checked_lat'], df['checked_lon']), axis=1)\n",
    "\n",
    "    # filter on closest distance per school\n",
    "    df_closest = df_cross.loc[df_cross.groupby('cdscode').pollution_school_distance.idxmin()]\n",
    "\n",
    "    # add to list and repeat for other partitions\n",
    "    out_list.append(df_closest)\n",
    "\n",
    "  df_out = pd.concat(out_list, ignore_index=True)\n",
    "\n",
    "  df_out['year'] = data_year\n",
    "  df_out = df_out.drop(columns=['key'])\n",
    "\n",
    "  return df_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on partition 1 of 5\n",
      "Working on partition 2 of 5\n",
      "Working on partition 3 of 5\n",
      "Working on partition 4 of 5\n",
      "Working on partition 5 of 5\n"
     ]
    }
   ],
   "source": [
    "# test on 1 year\n",
    "df_2000 = get_nearest(df_all, df_point_sources, data_year = 2000, partitions = 5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create School <--> Pollution Source Mapping\n",
    "\n",
    "Run the function above on all years. \n",
    "- This takes about 3.5 - 3.75 min per year\n",
    "- ended up taking 88 min for all years, w/ 16 gb of ram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2000. Partition 1 of 5\n",
      "Year: 2000. Partition 2 of 5\n",
      "Year: 2000. Partition 3 of 5\n",
      "Year: 2000. Partition 4 of 5\n",
      "Year: 2000. Partition 5 of 5\n",
      "Year: 2001. Partition 1 of 5\n",
      "Year: 2001. Partition 2 of 5\n",
      "Year: 2001. Partition 3 of 5\n",
      "Year: 2001. Partition 4 of 5\n",
      "Year: 2001. Partition 5 of 5\n",
      "Year: 2002. Partition 1 of 5\n",
      "Year: 2002. Partition 2 of 5\n",
      "Year: 2002. Partition 3 of 5\n",
      "Year: 2002. Partition 4 of 5\n",
      "Year: 2002. Partition 5 of 5\n",
      "Year: 2003. Partition 1 of 5\n",
      "Year: 2003. Partition 2 of 5\n",
      "Year: 2003. Partition 3 of 5\n",
      "Year: 2003. Partition 4 of 5\n",
      "Year: 2003. Partition 5 of 5\n",
      "Year: 2004. Partition 1 of 5\n",
      "Year: 2004. Partition 2 of 5\n",
      "Year: 2004. Partition 3 of 5\n",
      "Year: 2004. Partition 4 of 5\n",
      "Year: 2004. Partition 5 of 5\n",
      "Year: 2005. Partition 1 of 5\n",
      "Year: 2005. Partition 2 of 5\n",
      "Year: 2005. Partition 3 of 5\n",
      "Year: 2005. Partition 4 of 5\n",
      "Year: 2005. Partition 5 of 5\n",
      "Year: 2006. Partition 1 of 5\n",
      "Year: 2006. Partition 2 of 5\n",
      "Year: 2006. Partition 3 of 5\n",
      "Year: 2006. Partition 4 of 5\n",
      "Year: 2006. Partition 5 of 5\n",
      "Year: 2007. Partition 1 of 5\n",
      "Year: 2007. Partition 2 of 5\n",
      "Year: 2007. Partition 3 of 5\n",
      "Year: 2007. Partition 4 of 5\n",
      "Year: 2007. Partition 5 of 5\n",
      "Year: 2008. Partition 1 of 5\n",
      "Year: 2008. Partition 2 of 5\n",
      "Year: 2008. Partition 3 of 5\n",
      "Year: 2008. Partition 4 of 5\n",
      "Year: 2008. Partition 5 of 5\n",
      "Year: 2009. Partition 1 of 5\n",
      "Year: 2009. Partition 2 of 5\n",
      "Year: 2009. Partition 3 of 5\n",
      "Year: 2009. Partition 4 of 5\n",
      "Year: 2009. Partition 5 of 5\n",
      "Year: 2010. Partition 1 of 5\n",
      "Year: 2010. Partition 2 of 5\n",
      "Year: 2010. Partition 3 of 5\n",
      "Year: 2010. Partition 4 of 5\n",
      "Year: 2010. Partition 5 of 5\n",
      "Year: 2011. Partition 1 of 5\n",
      "Year: 2011. Partition 2 of 5\n",
      "Year: 2011. Partition 3 of 5\n",
      "Year: 2011. Partition 4 of 5\n",
      "Year: 2011. Partition 5 of 5\n",
      "Year: 2012. Partition 1 of 5\n",
      "Year: 2012. Partition 2 of 5\n",
      "Year: 2012. Partition 3 of 5\n",
      "Year: 2012. Partition 4 of 5\n",
      "Year: 2012. Partition 5 of 5\n",
      "Year: 2013. Partition 1 of 5\n",
      "Year: 2013. Partition 2 of 5\n",
      "Year: 2013. Partition 3 of 5\n",
      "Year: 2013. Partition 4 of 5\n",
      "Year: 2013. Partition 5 of 5\n",
      "Year: 2014. Partition 1 of 5\n",
      "Year: 2014. Partition 2 of 5\n",
      "Year: 2014. Partition 3 of 5\n",
      "Year: 2014. Partition 4 of 5\n",
      "Year: 2014. Partition 5 of 5\n",
      "Year: 2015. Partition 1 of 5\n",
      "Year: 2015. Partition 2 of 5\n",
      "Year: 2015. Partition 3 of 5\n",
      "Year: 2015. Partition 4 of 5\n",
      "Year: 2015. Partition 5 of 5\n",
      "Year: 2016. Partition 1 of 5\n",
      "Year: 2016. Partition 2 of 5\n",
      "Year: 2016. Partition 3 of 5\n",
      "Year: 2016. Partition 4 of 5\n",
      "Year: 2016. Partition 5 of 5\n",
      "Year: 2017. Partition 1 of 5\n",
      "Year: 2017. Partition 2 of 5\n",
      "Year: 2017. Partition 3 of 5\n",
      "Year: 2017. Partition 4 of 5\n",
      "Year: 2017. Partition 5 of 5\n",
      "Year: 2018. Partition 1 of 5\n",
      "Year: 2018. Partition 2 of 5\n",
      "Year: 2018. Partition 3 of 5\n",
      "Year: 2018. Partition 4 of 5\n",
      "Year: 2018. Partition 5 of 5\n",
      "Year: 2019. Partition 1 of 5\n",
      "Year: 2019. Partition 2 of 5\n",
      "Year: 2019. Partition 3 of 5\n",
      "Year: 2019. Partition 4 of 5\n",
      "Year: 2019. Partition 5 of 5\n",
      "CPU times: user 1h 27min 58s, sys: 38.1 s, total: 1h 28min 36s\n",
      "Wall time: 1h 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "school_ps_mapping = [get_nearest(df_all, df_point_sources, data_year = i, partitions = 5) for i in range(2000, 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this mapping table as parquet to disk\n",
    "school_ps_mapping_df = pd.concat(school_ps_mapping)\n",
    "fpath = os.path.join(gdrive_path, 'W210_Capstone/Data/school_pollution_mapping/school_pollution_mapping.parquet')\n",
    "school_ps_mapping_df.to_parquet(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the point sources to schools dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it back in (optional)\n",
    "fpath = os.path.join(gdrive_path, 'W210_Capstone/Data/school_pollution_mapping/school_pollution_mapping.parquet')\n",
    "school_ps_mapping_df = pd.read_parquet(fpath)\n",
    "# clean up field names and select relevant fields\n",
    "school_ps_mapping_df = school_ps_mapping_df\\\n",
    "  .rename(columns={'checked_lat': 'pollution_source_lat', 'checked_lon': 'pollution_source_lon', \n",
    "    'point_source_zip': 'pollution_source_zip', 'point_source_id': 'pollution_source_id'})\n",
    "\n",
    "school_ps_mapping_df = school_ps_mapping_df[['cdscode', 'year', 'pollution_source_id', 'pollution_source_lat', \n",
    "  'pollution_source_lon', 'PM25_emissions_TPY', 'pollution_school_distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "df_all = pd.merge(df_all, school_ps_mapping_df, on=['cdscode', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "df_all.to_parquet(os.path.join(gdrive_path, 'W210_Capstone/Data/joined_data/joined_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b25cd5a5a3cd1ea9e93fd254f185f4731ffaa4421de0b98534600687fe3ed44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
