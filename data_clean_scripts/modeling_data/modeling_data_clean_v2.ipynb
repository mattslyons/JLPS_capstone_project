{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Data Clean:\n",
    "\n",
    "This script does the following:\n",
    "- Loads our main joined dataset \n",
    "- Creates features\n",
    "  - Compute rolling averages of prior pm2.5 levels\n",
    "  - One-hot encode categorical features: school_region, month\n",
    "  - Get year from date\n",
    "- Saves the dataset\n",
    "- This final dataset should be used in modeling. We should be able to load and join cornelia's data to this final dataset to have a script cornelia can run on her end. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional. I'm getting annoying warnings that I just want to ignore:\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local or gdrive\n",
    "path_source = 'local'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  data_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  #env_path = '/content/gdrive/MyDrive/.env'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  data_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  #env_path = '/content/gdrive/MyDrive/.env'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  data_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in full joined dataset:\n",
    "df = pd.read_parquet(os.path.join(data_path, 'joined_data/joined_data_with_temperatures_10-16-22/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting row count: 2471552\n",
      "Converting numeric vars\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c054fc404fb4442b92f269310cc011a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting int vars\n",
      "Mean imputing a few vars\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46387b3ba99c438586859d11b8024b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending row count: 2336707\n"
     ]
    }
   ],
   "source": [
    "print(f'starting row count: {df.shape[0]}')\n",
    "\n",
    "# clean up column names. Make them all lower cased, and replace spaces with \"_\"\n",
    "df.columns = df.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()\n",
    "\n",
    "# filter out na pm2.5 values (133,261 of them)\n",
    "df = df[~df['pm25'].isna()]\n",
    "\n",
    "# 1,584 records are missing population in the year 2000. B/c the 2000 data had less zip codes.\n",
    "# Filter them out since we aren't using 2000 anyways. \n",
    "df = df[~df['population_10_14'].isna()]\n",
    "\n",
    "# 3 zips have 0 population, just filter those out\n",
    "df = df[df['total_population'] != 0]\n",
    "# do the same for populations under 19\n",
    "df = df[df['total_pop_under19'] != 0]\n",
    "\n",
    "# fix some datatypes\n",
    "num_vars = ['angle_to_school', 'ps_elevation_m', 'pm25', 'point_source_pm25_tpy', \n",
    "            'dist_school_to_ps_m', 'angle_to_school', 'avg_wind_alignment_cosine',\n",
    "            'total_population', 'total_population_male', 'total_population_female', \n",
    "            'population_0_4', 'population_0_4_male', 'population_0_4_female',\n",
    "            'population_5_9', 'population_5_9_male', 'population_5_9_female',\n",
    "            'population_10_14', 'population_10_14_male', 'population_10_14_female',\n",
    "            'population_15_19', 'population_15_19_male', 'population_15_19_female',\n",
    "            'total_pop_under19', 'point_source_lat', 'point_source_lon', \n",
    "            'school_elevation_m', 'pop_under19_male', 'pop_under19_female', 'ps_wind_lat', 'ps_wind_lon', 'ps_wspd_merge', \n",
    "            'school_wdir_wrt_0n', 'ps_wdir_wrt_0n', 'school_wind_alignment', 'ps_wind_alignment', 'avg_wind_speed', \n",
    "            'avg_wind_alignment', 'nearby_point_source_count', 'school_wspd',\n",
    "            'ca_agi_per_returns', 'total_tax_liability']\n",
    "\n",
    "print('Converting numeric vars')\n",
    "for var in tqdm(num_vars):\n",
    "  df[var] = df[var].astype(float)\n",
    "\n",
    "print('converting int vars')\n",
    "int_vars = ['school_zip']\n",
    "for var in int_vars:\n",
    "  df[var] = df[var].astype(int)\n",
    "\n",
    "# convert to date time:\n",
    "df['year_month'] = df['year_month'].map(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "print('Mean imputing a few vars')\n",
    "mean_impute_vars = ['ca_agi_per_returns', 'total_tax_liability']\n",
    "for var in tqdm(mean_impute_vars):\n",
    "  df[var] = df[var].fillna(df[var].mean())\n",
    "\n",
    "# get tax per capita (watch out for inf and nan)\n",
    "df['tax_liability_per_capita'] = df['total_tax_liability'] / df['total_population']\n",
    "\n",
    "# don't need these fields:\n",
    "df = df.drop(columns=['index'])\n",
    "\n",
    "# change \"san diego - imperial\" to just \"san diego\"\n",
    "# the \"-\" symbol was causing issues down the line with modeling\n",
    "df['school_region_name'] = df['school_region_name'].map(lambda x: re.sub('San Diego - Imperial', 'San Diego', x))\n",
    "\n",
    "print(f'ending row count: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of distinct year-mo-zips: 312345\n",
      "count of distinct for all join vars: 312345\n"
     ]
    }
   ],
   "source": [
    "# pre-grouping checks:\n",
    "n = df[['year_month', 'school_zip']].drop_duplicates().shape[0]\n",
    "print(f'count of distinct year-mo-zips: {n}')\n",
    "\n",
    "# including the population counts in this\n",
    "n = df[['year_month', 'school_zip', 'school_county_v2', 'school_region_name',\n",
    "  'pop_under19_male', 'pop_under19_female', 'total_pop_under19', 'pm25', \n",
    "  'ca_agi_per_returns', 'total_tax_liability']]\\\n",
    "  .drop_duplicates().shape[0]\n",
    "print(f'count of distinct for all join vars: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows of grouped df: 312345\n"
     ]
    }
   ],
   "source": [
    "# maybe not all of these should use 'mean', but doing it this way for now. \n",
    "mean_vars = [\n",
    "  # lat/lon (probs dont need these)\n",
    "  #'point_source_lat', 'point_source_lon', 'school_lat', 'school_lon', 'ps_wind_lat', 'ps_wind_lon',\n",
    "\n",
    "  # elevation:\n",
    "  'school_elevation_m', 'ps_elevation_m',\n",
    "\n",
    "  # pop\n",
    "  'population_0_4', 'population_0_4_male', 'population_0_4_female', \n",
    "  'population_5_9', 'population_5_9_male', 'population_5_9_female', \n",
    "  'population_10_14', 'population_10_14_male', 'population_10_14_female', \n",
    "  'population_15_19', 'population_15_19_male','population_15_19_female', \n",
    "  'total_pop_under19', 'pop_under19_male', 'pop_under19_female', \n",
    "  'total_population', 'total_population_male', 'total_population_female', \n",
    "  \n",
    "  # pm2.5 vars\n",
    "  'point_source_pm25_tpy',\n",
    "\n",
    "  # distance/wind/angles\n",
    "  'dist_school_to_ps_m', 'angle_to_school', 'ps_wspd_merge', 'school_wdir_wrt_0n', 'ps_wdir_wrt_0n',\n",
    "  'school_wind_alignment', 'ps_wind_alignment', 'avg_wind_speed', 'avg_wind_alignment', 'avg_wind_alignment_cosine', \n",
    "  'nearby_point_source_count', 'school_wspd',\n",
    "\n",
    "  # tax\n",
    "  'ca_agi_per_returns', 'total_tax_liability', 'tax_liability_per_capita',\n",
    "\n",
    "  # temp\n",
    "  'school_temperature', 'ps_temperature'\n",
    "  ]\n",
    "\n",
    "count_vars = ['cdscode']\n",
    "\n",
    "mean_dict = {var:(var, 'mean') for var in mean_vars}\n",
    "count_dict = {var:(var, 'count') for var in count_vars}\n",
    "agg_dict = {**mean_dict, **count_dict}\n",
    "\n",
    "grp_vars = ['year_month', 'school_zip', 'school_county_v2', 'school_region_name', 'pm25']\n",
    "\n",
    "df_grp = df\\\n",
    "  .groupby(grp_vars)\\\n",
    "  .agg(**agg_dict)\\\n",
    "  .reset_index()\n",
    "\n",
    "df_grp = df_grp.rename(columns = {'cdscode':'school_count'})\n",
    "\n",
    "print(f'Num rows of grouped df: {df_grp.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create rolling avg vars\n",
    "- rolling avg of pm2.5 for prior n months\n",
    "- get best fit regression line of pm2.5 levels over prior n month periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df rolling avg\n",
    "df_avgs = df_grp[['year_month', 'school_zip', 'pm25']].sort_values(['school_zip', 'year_month'])\n",
    "\n",
    "# get rolling n month avg\n",
    "def create_rolling_avg(df, num_months=6):\n",
    "  df[f'pm25_r{num_months}'] = df.groupby('school_zip')['pm25']\\\n",
    "    .apply(lambda x: x.rolling(window=num_months, min_periods=num_months, closed='left').mean())\n",
    "    \n",
    "  return df \n",
    "\n",
    "\n",
    "df_avgs = create_rolling_avg(df_avgs, 1)\n",
    "df_avgs = create_rolling_avg(df_avgs, 6)\n",
    "df_avgs = create_rolling_avg(df_avgs, 9)\n",
    "df_avgs = create_rolling_avg(df_avgs, 12)\n",
    "df_avgs = create_rolling_avg(df_avgs, 24)\n",
    "\n",
    "# count num obs over past n months (don't need this, but keeping it commented just in case)\n",
    "# df_avgs['pm25_6mo_count'] = df_avgs.groupby('school_zip')['pm25'].apply(lambda x: x.rolling(6, 1, closed='left').apply(lambda x: len(x)))\n",
    "# df_avgs['pm25_9mo_count'] = df_avgs.groupby('school_zip')['pm25'].apply(lambda x: x.rolling(9, 1, closed='left').apply(lambda x: len(x)))\n",
    "# df_avgs['pm25_12mo_count'] = df_avgs.groupby('school_zip')['pm25'].apply(lambda x: x.rolling(12, 1, closed='left').apply(lambda x: len(x)))\n",
    "\n",
    "# get pm25 for last month\n",
    "df_avgs = df_avgs.rename(columns={'pm25_r1': 'pm25_last_month'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find trend\n",
    "- This feature will be very correlated with the `pm25_last_month` feature. Discuss with cornelia whether there is an issue with multiple colinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_pm25_per_month(df, num_months=6):\n",
    "    def calcSlope(y):\n",
    "        regr = LinearRegression()\n",
    "        x_temp = np.array(list(range(len(y)))).reshape(-1, 1)\n",
    "\n",
    "        try:\n",
    "            regr.fit(x_temp, y)\n",
    "            return regr.coef_[0]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df[f'pm25_slope{num_months}'] = df.groupby('school_zip')['pm25']\\\n",
    "      .apply(lambda x: x.rolling(window=num_months, min_periods=num_months, closed='left')\\\n",
    "      .apply(lambda y: calcSlope(y)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avgs = get_slope_pm25_per_month(df_avgs, 6)\n",
    "df_avgs = get_slope_pm25_per_month(df_avgs, 9)\n",
    "df_avgs = get_slope_pm25_per_month(df_avgs, 12)\n",
    "df_avgs = get_slope_pm25_per_month(df_avgs, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pm2.5 value from 12 months ago\n",
    "df_avgs['pm25_lag_12mo'] = df_avgs.groupby('school_zip')['pm25'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 schools have some time gaps. But only 5 of them. So not too much to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_days\n",
       "31 days      180818\n",
       "30 days      104104\n",
       "28 days       19214\n",
       "29 days        6816\n",
       "92 days           1\n",
       "458 days          1\n",
       "761 days          1\n",
       "1188 days         1\n",
       "1553 days         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# needs to be date/num type\n",
    "df_avgs['num_days'] = df_avgs.groupby('school_zip')['year_month'].apply(lambda x: x.diff())\n",
    "df_avgs.value_counts('num_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the rolling averages back to main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avgs = df_avgs.drop(columns=['pm25', 'num_days'])\n",
    "# df_avgs = df_avgs.drop(columns=['pm25'])\n",
    "df_grp = pd.merge(df_grp, df_avgs, on=['school_zip', 'year_month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical features\n",
    "- School region\n",
    "- Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create year var\n",
    "df_grp['year'] = df_grp['year_month'].dt.year\n",
    "\n",
    "# create month. convert to string and make it two digits so we can one-hot encode\n",
    "df_grp['month'] = df_grp['year_month'].dt.month.map(lambda x: str(x).rjust(2, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_cols = ['school_region_name', 'month']\n",
    "\n",
    "df_one_hot = pd.get_dummies(df_grp[['year_month', 'school_zip'] + encode_cols])\n",
    "df_one_hot.columns = df_one_hot.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_cols = ['school_region_name', 'month']\n",
    "\n",
    "df_one_hot = pd.get_dummies(df_grp[['year_month', 'school_zip'] + encode_cols])\n",
    "df_one_hot.columns = df_one_hot.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()\n",
    "\n",
    "#df_grp = pd.merge(df_grp.drop(columns=encode_cols), df_one_hot, on=['year_month', 'school_zip'], how='left')\n",
    "df_grp = pd.merge(df_grp, df_one_hot, on=['year_month', 'school_zip'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312345"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp.to_csv(os.path.join(data_path, 'modeling_data/modeling_data_2022-10-16.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b25cd5a5a3cd1ea9e93fd254f185f4731ffaa4421de0b98534600687fe3ed44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
