{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Data Clean:\n",
    "\n",
    "This script does the following:\n",
    "- Loads our main joined dataset \n",
    "- Creates features\n",
    "  - Compute rolling averages of prior pm2.5 levels\n",
    "  - One-hot encode categorical features: school_region, month\n",
    "  - Get year from date\n",
    "- Saves the dataset\n",
    "- This final dataset should be used in modeling. We should be able to load and join cornelia's data to this final dataset to have a script cornelia can run on her end. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional. I'm getting annoying warnings that I just want to ignore:\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local or gdrive\n",
    "path_source = 'local'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  data_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  #env_path = '/content/gdrive/MyDrive/.env'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  data_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  #env_path = '/content/gdrive/MyDrive/.env'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  data_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in full joined dataset:\n",
    "df = pd.read_parquet(os.path.join(data_path, 'joined_data/joined_data_with_temperatures_10-16-22/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting row count: 2471552\n",
      "Converting numeric vars\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d20227271f456591c8ca3ebfc1f6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting int vars\n",
      "Mean imputing a few vars\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a465e42386b54539a5c4efcfe05a914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending row count: 2334853\n"
     ]
    }
   ],
   "source": [
    "print(f'starting row count: {df.shape[0]}')\n",
    "\n",
    "# clean up column names. Make them all lower cased, and replace spaces with \"_\"\n",
    "df.columns = df.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()\n",
    "\n",
    "# filter out na pm2.5 values (133,261 of them)\n",
    "df = df[~df['pm25'].isna()]\n",
    "\n",
    "# 1,584 records are missing population in the year 2000. B/c the 2000 data had less zip codes.\n",
    "# Filter them out since we aren't using 2000 anyways. \n",
    "df = df[~df['population_10_14'].isna()]\n",
    "\n",
    "# fix some datatypes\n",
    "num_vars = ['angle_to_school', 'ps_elevation_m', 'pm25', 'point_source_pm25_tpy', \n",
    "            'dist_school_to_ps_m', 'angle_to_school', 'avg_wind_alignment_cosine',\n",
    "            'total_population', 'total_population_male', 'total_population_female', \n",
    "            'population_0_4', 'population_0_4_male', 'population_0_4_female',\n",
    "            'population_5_9', 'population_5_9_male', 'population_5_9_female',\n",
    "            'population_10_14', 'population_10_14_male', 'population_10_14_female',\n",
    "            'population_15_19', 'population_15_19_male', 'population_15_19_female',\n",
    "            'total_pop_under19', 'point_source_lat', 'point_source_lon', \n",
    "            'school_elevation_m', 'pop_under19_male', 'pop_under19_female', 'ps_wind_lat', 'ps_wind_lon', 'ps_wspd_merge', \n",
    "            'school_wdir_wrt_0n', 'ps_wdir_wrt_0n', 'school_wind_alignment', 'ps_wind_alignment', 'avg_wind_speed', \n",
    "            'avg_wind_alignment', 'nearby_point_source_count', 'school_wspd',\n",
    "            'ca_agi_per_returns', 'total_tax_liability']\n",
    "\n",
    "print('Converting numeric vars')\n",
    "for var in tqdm(num_vars):\n",
    "  df[var] = df[var].astype(float)\n",
    "\n",
    "print('converting int vars')\n",
    "int_vars = ['school_zip']\n",
    "for var in int_vars:\n",
    "  df[var] = df[var].astype(int)\n",
    "\n",
    "\n",
    "# 3 zips have 0 population, just filter those out\n",
    "df = df[df['total_population'] > 0]\n",
    "# do the same for populations under 19\n",
    "df = df[df['total_pop_under19'] > 0]\n",
    "\n",
    "# convert to date time:\n",
    "df['year_month'] = df['year_month'].map(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "print('Mean imputing a few vars')\n",
    "mean_impute_vars = ['ca_agi_per_returns', 'total_tax_liability']\n",
    "for var in tqdm(mean_impute_vars):\n",
    "  df[var] = df[var].fillna(df[var].mean())\n",
    "\n",
    "# get tax per capita (watch out for inf and nan)\n",
    "df['tax_liability_per_capita'] = df['total_tax_liability'] / df['total_population']\n",
    "\n",
    "# don't need these fields:\n",
    "df = df.drop(columns=['index'])\n",
    "\n",
    "# change \"san diego - imperial\" to just \"san diego\"\n",
    "# the \"-\" symbol was causing issues down the line with modeling\n",
    "df['school_region_name'] = df['school_region_name'].map(lambda x: re.sub('San Diego - Imperial', 'San Diego', x))\n",
    "\n",
    "print(f'ending row count: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of distinct year-mo-zips: 311163\n",
      "count of distinct for all join vars: 311163\n"
     ]
    }
   ],
   "source": [
    "# pre-grouping checks:\n",
    "n = df[['year_month', 'school_zip']].drop_duplicates().shape[0]\n",
    "print(f'count of distinct year-mo-zips: {n}')\n",
    "\n",
    "# including the population counts in this\n",
    "n = df[['year_month', 'school_zip', 'school_county_v2', 'school_region_name',\n",
    "  'pop_under19_male', 'pop_under19_female', 'total_pop_under19', 'pm25', \n",
    "  'ca_agi_per_returns', 'total_tax_liability']]\\\n",
    "  .drop_duplicates().shape[0]\n",
    "print(f'count of distinct for all join vars: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows of grouped df: 311163\n"
     ]
    }
   ],
   "source": [
    "# maybe not all of these should use 'mean', but doing it this way for now. \n",
    "mean_vars = [\n",
    "  # lat/lon (probs dont need these)\n",
    "  #'point_source_lat', 'point_source_lon', 'school_lat', 'school_lon', 'ps_wind_lat', 'ps_wind_lon',\n",
    "\n",
    "  # elevation:\n",
    "  'school_elevation_m', 'ps_elevation_m',\n",
    "\n",
    "  # pop\n",
    "  'population_0_4', 'population_0_4_male', 'population_0_4_female', \n",
    "  'population_5_9', 'population_5_9_male', 'population_5_9_female', \n",
    "  'population_10_14', 'population_10_14_male', 'population_10_14_female', \n",
    "  'population_15_19', 'population_15_19_male','population_15_19_female', \n",
    "  'total_pop_under19', 'pop_under19_male', 'pop_under19_female', \n",
    "  'total_population', 'total_population_male', 'total_population_female', \n",
    "  \n",
    "  # pm2.5 vars\n",
    "  'point_source_pm25_tpy',\n",
    "\n",
    "  # distance/wind/angles\n",
    "  'dist_school_to_ps_m', 'angle_to_school', 'ps_wspd_merge', 'school_wdir_wrt_0n', 'ps_wdir_wrt_0n',\n",
    "  'school_wind_alignment', 'ps_wind_alignment', 'avg_wind_speed', 'avg_wind_alignment', 'avg_wind_alignment_cosine', \n",
    "  'nearby_point_source_count', 'school_wspd',\n",
    "\n",
    "  # tax\n",
    "  'ca_agi_per_returns', 'total_tax_liability', 'tax_liability_per_capita',\n",
    "\n",
    "  # temp\n",
    "  'school_temperature', 'ps_temperature'\n",
    "  ]\n",
    "\n",
    "count_vars = ['cdscode']\n",
    "\n",
    "mean_dict = {var:(var, 'mean') for var in mean_vars}\n",
    "count_dict = {var:(var, 'count') for var in count_vars}\n",
    "agg_dict = {**mean_dict, **count_dict}\n",
    "\n",
    "grp_vars = ['year_month', 'school_zip', 'school_county_v2', 'school_region_name', 'pm25']\n",
    "\n",
    "df_grp = df\\\n",
    "  .groupby(grp_vars)\\\n",
    "  .agg(**agg_dict)\\\n",
    "  .reset_index()\n",
    "\n",
    "df_grp = df_grp.rename(columns = {'cdscode':'school_count'})\n",
    "\n",
    "print(f'Num rows of grouped df: {df_grp.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create rolling avg vars\n",
    "- rolling avg of pm2.5 for prior n months\n",
    "- get best fit regression line of pm2.5 levels over prior n month periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df rolling avg\n",
    "df_avgs = df_grp[['year_month', 'school_zip', 'pm25']].sort_values(['school_zip', 'year_month'])\n",
    "\n",
    "# get rolling n month avg\n",
    "def create_rolling_avg(df, num_months=6):\n",
    "  df[f'pm25_r{num_months}'] = df.groupby('school_zip')['pm25']\\\n",
    "    .apply(lambda x: x.rolling(window=num_months, min_periods=num_months, closed='left').mean())\n",
    "    \n",
    "  return df \n",
    "\n",
    "\n",
    "df_avgs = create_rolling_avg(df_avgs, 1)\n",
    "df_avgs = create_rolling_avg(df_avgs, 6)\n",
    "df_avgs = create_rolling_avg(df_avgs, 9)\n",
    "df_avgs = create_rolling_avg(df_avgs, 12)\n",
    "df_avgs = create_rolling_avg(df_avgs, 24)\n",
    "\n",
    "# count num obs over past n months (don't need this, but keeping it commented just in case)\n",
    "# df_avgs['pm25_6mo_count'] = df_avgs.groupby('school_zip')['pm25'].apply(lambda x: x.rolling(6, 1, closed='left').apply(lambda x: len(x)))\n",
    "# df_avgs['pm25_9mo_count'] = df_avgs.groupby('school_zip')['pm25'].apply(lambda x: x.rolling(9, 1, closed='left').apply(lambda x: len(x)))\n",
    "# df_avgs['pm25_12mo_count'] = df_avgs.groupby('school_zip')['pm25'].apply(lambda x: x.rolling(12, 1, closed='left').apply(lambda x: len(x)))\n",
    "\n",
    "# get pm25 for last month\n",
    "df_avgs = df_avgs.rename(columns={'pm25_r1': 'pm25_last_month'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find trend\n",
    "- This feature will be very correlated with the `pm25_last_month` feature. Discuss with cornelia whether there is an issue with multiple colinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope_pm25_per_month(df, num_months=6):\n",
    "    def calcSlope(y):\n",
    "        regr = LinearRegression()\n",
    "        x_temp = np.array(list(range(len(y)))).reshape(-1, 1)\n",
    "\n",
    "        try:\n",
    "            regr.fit(x_temp, y)\n",
    "            return regr.coef_[0]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    df[f'pm25_slope{num_months}'] = df.groupby('school_zip')['pm25']\\\n",
    "      .apply(lambda x: x.rolling(window=num_months, min_periods=num_months, closed='left')\\\n",
    "      .apply(lambda y: calcSlope(y)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avgs = get_slope_pm25_per_month(df_avgs, 6)\n",
    "df_avgs = get_slope_pm25_per_month(df_avgs, 9)\n",
    "df_avgs = get_slope_pm25_per_month(df_avgs, 12)\n",
    "df_avgs = get_slope_pm25_per_month(df_avgs, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pm2.5 value from 12 months ago\n",
    "df_avgs['pm25_lag_12mo'] = df_avgs.groupby('school_zip')['pm25'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 schools have some time gaps. But only 5 of them. So not too much to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_days\n",
       "31 days      180116\n",
       "30 days      103711\n",
       "28 days       19140\n",
       "29 days        6791\n",
       "396 days          5\n",
       "1127 days         3\n",
       "762 days          2\n",
       "92 days           1\n",
       "397 days          1\n",
       "458 days          1\n",
       "761 days          1\n",
       "1188 days         1\n",
       "1492 days         1\n",
       "1553 days         1\n",
       "2223 days         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# needs to be date/num type\n",
    "df_avgs['num_days'] = df_avgs.groupby('school_zip')['year_month'].apply(lambda x: x.diff())\n",
    "df_avgs.value_counts('num_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the rolling averages back to main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avgs = df_avgs.drop(columns=['pm25', 'num_days'])\n",
    "# df_avgs = df_avgs.drop(columns=['pm25'])\n",
    "df_grp = pd.merge(df_grp, df_avgs, on=['school_zip', 'year_month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical features\n",
    "- School region\n",
    "- Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create year var\n",
    "df_grp['year'] = df_grp['year_month'].dt.year\n",
    "\n",
    "# create month. convert to string and make it two digits so we can one-hot encode\n",
    "df_grp['month'] = df_grp['year_month'].dt.month.map(lambda x: str(x).rjust(2, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_cols = ['school_region_name', 'month']\n",
    "\n",
    "df_one_hot = pd.get_dummies(df_grp[['year_month', 'school_zip'] + encode_cols])\n",
    "df_one_hot.columns = df_one_hot.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_cols = ['school_region_name', 'month']\n",
    "\n",
    "df_one_hot = pd.get_dummies(df_grp[['year_month', 'school_zip'] + encode_cols])\n",
    "df_one_hot.columns = df_one_hot.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()\n",
    "\n",
    "#df_grp = pd.merge(df_grp.drop(columns=encode_cols), df_one_hot, on=['year_month', 'school_zip'], how='left')\n",
    "df_grp = pd.merge(df_grp, df_one_hot, on=['year_month', 'school_zip'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311163"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp.to_csv(os.path.join(data_path, 'modeling_data/modeling_data_2022-10-16.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('miniconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b25cd5a5a3cd1ea9e93fd254f185f4731ffaa4421de0b98534600687fe3ed44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
