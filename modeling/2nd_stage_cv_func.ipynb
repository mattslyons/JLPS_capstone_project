{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Create a function that can be used in the 2nd stage regression to perform a time series cross validation. \n",
    "- Using an expanding window cross validation\n",
    "\n",
    "\n",
    "The 2nd stage regression predicts the medical outcomes using the predicted PM2.5 (and separately with the actual pm2.5), as well as the same fixed effects from the first stage regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional. I'm getting annoying warnings that I just want to ignore:\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import requests\n",
    "import urllib\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep this as false unless you want to save out the fitted model objects and results \n",
    "save_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path\n",
    "\n",
    "Add a new elif section for your path if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local or gdrive\n",
    "path_source = 'work'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  data_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  data_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  data_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our modeling data\n",
    "df = pd.read_csv(os.path.join(data_path, 'modeling_data/modeling_data_joined_11-9.csv'))\n",
    "\n",
    "# read in cornelia's healthcare data\n",
    "df1 = pd.read_csv(os.path.join(data_path, 'medical/hematopoietic_cancers.csv')).iloc[:,1:]\n",
    "df2 = pd.read_csv(os.path.join(data_path, 'medical/pediatric_vasculitis.csv')).iloc[:,1:]\n",
    "df3 = pd.read_csv(os.path.join(data_path, 'medical/type_1_diabetes.csv')).iloc[:,1:]\n",
    "df4 = pd.read_csv(os.path.join(data_path, 'medical/resp_cardio.csv')).iloc[:,1:]\n",
    "df5 = pd.read_csv(os.path.join(data_path, 'medical/injuries_accidents.csv')).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# med data:\n",
    "\n",
    "# get all distinct patzip_year_month\n",
    "all_pats = df1['patzip_year_month'].to_list() + \\\n",
    "  df2['patzip_year_month'].to_list() + \\\n",
    "  df3['patzip_year_month'].to_list() + \\\n",
    "  df4['patzip_year_month'].to_list() + \\\n",
    "  df5['patzip_year_month'].to_list() \n",
    "all_pats = list(set(all_pats))\n",
    "df_med = pd.DataFrame({'patzip_year_month': all_pats})\n",
    "\n",
    "# rename columns more intuitively\n",
    "df1 = df1.rename(columns={'number_of_visits': 'number_of_visits_hem_cancers'})\n",
    "df2 = df2.rename(columns={'number_of_visits': 'number_of_visits_vasc'})\n",
    "df3 = df3.rename(columns={'number_of_visits': 'number_of_visits_diab'})\n",
    "df4 = df4.rename(columns={'number_of_visits': 'number_of_visits_resp_cardio'})\n",
    "df5 = df5.rename(columns={'number_of_visits': 'number_of_visits_injuries'})\n",
    "\n",
    "# now join all the diagnoses on this dataset\n",
    "df_med = df_med\\\n",
    "  .merge(df1, on='patzip_year_month', how='left')\\\n",
    "  .merge(df2, on='patzip_year_month', how='left')\\\n",
    "  .merge(df3, on='patzip_year_month', how='left')\\\n",
    "  .merge(df4, on='patzip_year_month', how='left')\\\n",
    "  .merge(df5, on='patzip_year_month', how='left')\n",
    "\n",
    "# join data\n",
    "df['year_month'] = df['year_month'].map(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df['zip_year_month'] = df['school_zip'].astype(str) + '-' +\\\n",
    "  df['year_month'].dt.year.astype(str) + '-' +\\\n",
    "  df['year_month'].dt.month.astype(str)\n",
    "\n",
    "df = pd.merge(df, df_med, left_on='zip_year_month', right_on='patzip_year_month', how='left')\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# for missing med data, assume there were 0 cases:\n",
    "med_vars = ['hematopoietic_cancers', 'number_of_visits_hem_cancers', \n",
    "  'pediatric_vasculitis', 'number_of_visits_vasc', \n",
    "  'type_1_diabetes', 'number_of_visits_diab',\n",
    "  'resp_cardio', 'number_of_visits_resp_cardio',\n",
    "  'injuries_accidents', 'number_of_visits_injuries'\n",
    "  ]\n",
    "for var in med_vars:\n",
    "  df[var] = df[var].fillna(0)\n",
    "\n",
    "# fixing month datatype\n",
    "df['month'] = df['month'].astype(str)\n",
    "\n",
    "# Create response variables, which is visits / population\n",
    "df['y_hematopoietic'] = 1000 * df['number_of_visits_hem_cancers'] / df['total_pop_under19']\n",
    "df['y_vasculitis'] = 1000 * df['number_of_visits_vasc'] / df['total_pop_under19']\n",
    "df['y_diabetes'] = 1000 * df['number_of_visits_diab'] / df['total_pop_under19']\n",
    "df['y_resp_cardio'] = 1000 * df['number_of_visits_resp_cardio'] / df['total_pop_under19']\n",
    "df['y_injuries'] = 1000 * df['number_of_visits_injuries'] / df['total_pop_under19']\n",
    "\n",
    "# Create an option for a logged version of the treatment var (log(1+x)). this makes it normally distributed \n",
    "df['pm25_log'] = np.log1p(df['pm25'])\n",
    "\n",
    "# create year trend feature\n",
    "df['year_trend'] = df['year'] - 1999\n",
    "\n",
    "# create county_month\n",
    "df['county_month'] = df.apply(lambda df: df['month'].rjust(2, '0') + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# create year_month_county (in case we want to just direclty use this var for the interaction effects)\n",
    "df['year_month_county'] = df.apply(lambda df: str(df['year']) + '_' + df['month'] + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# no need to one hot encode anymore, b/c data is already encoded \n",
    "\n",
    "# filter data to appropriate data range\n",
    "df = df[df.year >= 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split \n",
    "# keep 2018 as the held out test set \n",
    "df_test = df[df.year == 2018]\n",
    "df = df[df.year != 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data on date\n",
    "df = df.sort_values('year_month').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for modeling\n",
    "date_var = 'year_month'\n",
    "num_vars = ['school_elevation_m', 'nearby_point_source_count', 'school_wspd', 'tax_liability_per_capita', 'school_temperature', 'school_count', 'pm25_r6', 'pm25_r12']\n",
    "counties = [i for i in df.columns if re.search('^school_county_v2_', i)]\n",
    "months = [i for i in df.columns if re.search ('^month_', i)]\n",
    "# potentially use county_month instead of the above \n",
    "\n",
    "xvars = num_vars + counties + months \n",
    "yvar = ['y_hematopoietic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation function\n",
    "\n",
    "Note, this function does not yet do the grid search for hyperparams, but just does the CV once. Update this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(df, xvars, yvar, hyperparams = {}, search_type='grid', folds=5, verbosity=0):\n",
    "  ''' \n",
    "  verbosity = 0, 1 or 2. 0 = silent. 1 = . 2 = show results after every cross fold\n",
    "  '''\n",
    "\n",
    "  df = df[xvars + [yvar]]\n",
    "\n",
    "  tss = TimeSeriesSplit(n_splits=folds)\n",
    "  res = tss.split(df)\n",
    "  all_folds = [i for i in res]\n",
    "\n",
    "  res = {'fold':[], 'hyperparams':[], 'rmse_train': [], 'rmse_test': []}\n",
    "\n",
    "  # add outer for loop to do a grid search on all hyperparams\n",
    "  for i,fold in enumerate(all_folds):\n",
    "    df_train = df.loc[fold[0]]\n",
    "    df_test = df.loc[fold[1]]\n",
    "\n",
    "    # convert to xgb types\n",
    "    dmat_train = xgb.DMatrix(df_train[xvars], df_train[yvar])\n",
    "    dmat_test = xgb.DMatrix(df_test[xvars], df_test[yvar])\n",
    "\n",
    "    # fit xgb\n",
    "    hyperparams = { \n",
    "      'max_depth': 10, \n",
    "      'subsample': .8,\n",
    "      'eta': .3,  \n",
    "      'eval_metric': 'rmse', \n",
    "      }\n",
    "    booster = xgb.train(\n",
    "      hyperparams,\n",
    "      dmat_train,\n",
    "      num_boost_round=100,\n",
    "      early_stopping_rounds=15,\n",
    "      evals = [(dmat_train, 'train'), (dmat_test, 'test')], \n",
    "      verbose_eval=False\n",
    "      )\n",
    "\n",
    "    def get_rmse(dmat_train, df_train):\n",
    "      ytrue = df_train[yvar].values.flatten()\n",
    "      yhat = booster.predict(dmat_train)\n",
    "      rmse = np.mean(((ytrue - yhat)**2)**.5)\n",
    "      return rmse \n",
    "    \n",
    "    # save results\n",
    "    res['fold'].append(i)\n",
    "    res['hyperparams'].append(hyperparams)\n",
    "    res['rmse_train'].append(get_rmse(dmat_train, df_train))\n",
    "    res['rmse_test'].append(get_rmse(dmat_test, df_test))\n",
    "  \n",
    "  return res\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold': [0, 1, 2, 3, 4],\n",
       " 'hyperparams': [{'max_depth': 10,\n",
       "   'subsample': 0.8,\n",
       "   'eta': 0.3,\n",
       "   'eval_metric': 'rmse'},\n",
       "  {'max_depth': 10, 'subsample': 0.8, 'eta': 0.3, 'eval_metric': 'rmse'},\n",
       "  {'max_depth': 10, 'subsample': 0.8, 'eta': 0.3, 'eval_metric': 'rmse'},\n",
       "  {'max_depth': 10, 'subsample': 0.8, 'eta': 0.3, 'eval_metric': 'rmse'},\n",
       "  {'max_depth': 10, 'subsample': 0.8, 'eta': 0.3, 'eval_metric': 'rmse'}],\n",
       " 'rmse_train': [0.03660981847353131,\n",
       "  0.07760893534128871,\n",
       "  0.11936492482290355,\n",
       "  0.16205563841875828,\n",
       "  0.20558345013313914],\n",
       " 'rmse_test': [0.1496672829559056,\n",
       "  0.2058065695515374,\n",
       "  0.2948592276933558,\n",
       "  0.33937265398584654,\n",
       "  0.4241127035076277]}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = time_series_cv(df, xvars = num_vars + counties + months, yvar = 'y_hematopoietic')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
