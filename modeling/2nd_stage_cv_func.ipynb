{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Create a function that can be used in the 2nd stage regression to perform a time series cross validation. \n",
    "- Using an expanding window cross validation\n",
    "\n",
    "\n",
    "The 2nd stage regression predicts the medical outcomes using the predicted PM2.5 (and separately with the actual pm2.5), as well as the same fixed effects from the first stage regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional. I'm getting annoying warnings that I just want to ignore:\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import requests\n",
    "import urllib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local or gdrive\n",
    "path_source = 'work'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  data_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  data_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  data_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data/modeling_data/modeling_data_joined_11-9.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/trevorjohnson/Documents/tj/berkeley/w210/JLPS_capstone_project/modeling/2nd_stage_cv_func.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trevorjohnson/Documents/tj/berkeley/w210/JLPS_capstone_project/modeling/2nd_stage_cv_func.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# read in our modeling data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/trevorjohnson/Documents/tj/berkeley/w210/JLPS_capstone_project/modeling/2nd_stage_cv_func.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39m'\u001b[39;49m\u001b[39mmodeling_data/modeling_data_joined_11-9.csv\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trevorjohnson/Documents/tj/berkeley/w210/JLPS_capstone_project/modeling/2nd_stage_cv_func.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# read in cornelia's healthcare data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/trevorjohnson/Documents/tj/berkeley/w210/JLPS_capstone_project/modeling/2nd_stage_cv_func.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m'\u001b[39m\u001b[39mmedical/hematopoietic_cancers.csv\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39miloc[:,\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data/modeling_data/modeling_data_joined_11-9.csv'"
     ]
    }
   ],
   "source": [
    "# read in our modeling data\n",
    "df = pd.read_csv(os.path.join(data_path, 'modeling_data/modeling_data_joined_11-9.csv'))\n",
    "\n",
    "# read in cornelia's healthcare data\n",
    "df1 = pd.read_csv(os.path.join(data_path, 'medical/hematopoietic_cancers.csv')).iloc[:,1:]\n",
    "df2 = pd.read_csv(os.path.join(data_path, 'medical/pediatric_vasculitis.csv')).iloc[:,1:]\n",
    "df3 = pd.read_csv(os.path.join(data_path, 'medical/type_1_diabetes.csv')).iloc[:,1:]\n",
    "df4 = pd.read_csv(os.path.join(data_path, 'medical/resp_cardio.csv')).iloc[:,1:]\n",
    "df5 = pd.read_csv(os.path.join(data_path, 'medical/injuries_accidents.csv')).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# med data:\n",
    "\n",
    "# get all distinct patzip_year_month\n",
    "all_pats = df1['patzip_year_month'].to_list() + \\\n",
    "  df2['patzip_year_month'].to_list() + \\\n",
    "  df3['patzip_year_month'].to_list() + \\\n",
    "  df4['patzip_year_month'].to_list() + \\\n",
    "  df5['patzip_year_month'].to_list() \n",
    "all_pats = list(set(all_pats))\n",
    "df_med = pd.DataFrame({'patzip_year_month': all_pats})\n",
    "\n",
    "# rename columns more intuitively\n",
    "df1 = df1.rename(columns={'number_of_visits': 'number_of_visits_hem_cancers'})\n",
    "df2 = df2.rename(columns={'number_of_visits': 'number_of_visits_vasc'})\n",
    "df3 = df3.rename(columns={'number_of_visits': 'number_of_visits_diab'})\n",
    "df4 = df4.rename(columns={'number_of_visits': 'number_of_visits_resp_cardio'})\n",
    "df5 = df5.rename(columns={'number_of_visits': 'number_of_visits_injuries'})\n",
    "\n",
    "# now join all the diagnoses on this dataset\n",
    "df_med = df_med\\\n",
    "  .merge(df1, on='patzip_year_month', how='left')\\\n",
    "  .merge(df2, on='patzip_year_month', how='left')\\\n",
    "  .merge(df3, on='patzip_year_month', how='left')\\\n",
    "  .merge(df4, on='patzip_year_month', how='left')\\\n",
    "  .merge(df5, on='patzip_year_month', how='left')\n",
    "\n",
    "# join data\n",
    "df['year_month'] = df['year_month'].map(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df['zip_year_month'] = df['school_zip'].astype(str) + '-' +\\\n",
    "  df['year_month'].dt.year.astype(str) + '-' +\\\n",
    "  df['year_month'].dt.month.astype(str)\n",
    "\n",
    "df = pd.merge(df, df_med, left_on='zip_year_month', right_on='patzip_year_month', how='left')\n",
    "\n",
    "# for missing med data, assume there were 0 cases:\n",
    "med_vars = ['hematopoietic_cancers', 'number_of_visits_hem_cancers', \n",
    "  'pediatric_vasculitis', 'number_of_visits_vasc', \n",
    "  'type_1_diabetes', 'number_of_visits_diab',\n",
    "  'resp_cardio', 'number_of_visits_resp_cardio',\n",
    "  'injuries_accidents', 'number_of_visits_injuries'\n",
    "  ]\n",
    "for var in med_vars:\n",
    "  df[var] = df[var].fillna(0)\n",
    "\n",
    "# fixing month datatype\n",
    "df['month'] = df['month'].astype(str)\n",
    "\n",
    "# Create response variables, which is visits / population\n",
    "df['y_hematopoietic'] = 1000 * df['number_of_visits_hem_cancers'] / df['total_pop_under19']\n",
    "df['y_vasculitis'] = 1000 * df['number_of_visits_vasc'] / df['total_pop_under19']\n",
    "df['y_diabetes'] = 1000 * df['number_of_visits_diab'] / df['total_pop_under19']\n",
    "df['y_resp_cardio'] = 1000 * df['number_of_visits_resp_cardio'] / df['total_pop_under19']\n",
    "df['y_injuries'] = 1000 * df['number_of_visits_injuries'] / df['total_pop_under19']\n",
    "\n",
    "# Make treatment var normally distributed by taking log(1+x)\n",
    "df['pm25_log'] = np.log1p(df['pm25'])\n",
    "\n",
    "# create year trend feature\n",
    "df['year_trend'] = df['year'] - 1999\n",
    "\n",
    "# create county_month\n",
    "df['county_month'] = df.apply(lambda df: df['month'].rjust(2, '0') + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# create year_month_county (in case we want to just direclty use this var for the interaction effects)\n",
    "df['year_month_county'] = df.apply(lambda df: str(df['year']) + '_' + df['month'] + '_' + df['school_county_v2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['school_county_v2', 'month', 'county_month']\n",
    "numeric_FE = ['dist_school_to_ps_m', 'point_source_pm25_tpy', 'nearby_point_source_count', 'school_count', 'avg_wind_speed', \n",
    "  'ca_agi_per_returns', 'school_temperature', 'ps_temperature', 'year_trend']\n",
    "instrum = ['avg_wind_alignment_cosine']\n",
    "\n",
    "enc1 = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc1.fit(df[cols_to_encode])\n",
    "\n",
    "if save_results:\n",
    "  enc_file = os.path.join(data_path, '../fitted_models/onehotencoder')\n",
    "  import pickle\n",
    "  with open(enc_file, \"wb\") as f: \n",
    "    pickle.dump(enc1, f)\n",
    "\n",
    "# Load it back in like this:\n",
    "# with open(enc_file, \"rb\") as f: \n",
    "#   enc = pickle.load(f)\n",
    "\n",
    "\n",
    "df_encoded = pd.DataFrame(enc1.transform(df[cols_to_encode]).toarray())\n",
    "\n",
    "all_cols = []\n",
    "for i,col in enumerate(cols_to_encode):\n",
    "  res1 = [col+'_'+j for j in enc1.categories_[i]]\n",
    "  all_cols += res1\n",
    "\n",
    "df_encoded.columns = all_cols\n",
    "df_encoded.columns = df_encoded.columns.str.replace(\"\\.*\\s+\", \"_\").str.lower()\n",
    "\n",
    "# drop baseline columns\n",
    "df_encoded = df_encoded.drop(columns = ['school_county_v2_los_angeles', 'month_1', 'county_month_01_los_angeles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data into right model form\n",
    "# add intercept, instrument, numeric fixed effects, and encoded columns\n",
    "y = df['pm25']\n",
    "df['intercept'] = 1\n",
    "X = pd.concat([df[instrum + numeric_FE + ['intercept']], df_encoded], axis=1)\n",
    "\n",
    "# next, create year_trend * county_month field\n",
    "county_month_cols = [i for i in X.columns if re.search('county_month_', i)]\n",
    "for col in county_month_cols:\n",
    "  X['year_trend_'+col] = X['year_trend'] * X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop instrument, and add x_hat to the X matrix\n",
    "X = X.drop(columns='avg_wind_alignment_cosine')\n",
    "X['yhat_pm25'] = df['yhat_pm25']\n",
    "X = X[['yhat_pm25'] + [i for i in X.columns if not re.search('yhat_pm25', i)]] # put our pred pm25 in front\n",
    "X_truepm25 = pd.concat([df['pm25'], X.drop(columns='yhat_pm25')], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "fit2_hema = sm.OLS(df['y_hematopoietic'], X).fit()\n",
    "\n",
    "# save model\n",
    "if save_results:\n",
    "  fit2_hema.save(os.path.join(fitted_models_path, 'stage2_ols_hematopoietic.pickle'), remove_data=True)\n",
    "\n",
    "# add fitted y value to our dataset\n",
    "df['yhat_hematopoietic'] = fit2_hema.fittedvalues.values\n",
    "\n",
    "# Get residuals\n",
    "resids_and_fitted_hema = pd.DataFrame({'resids': fit2_hema.resid, 'fitted': fit2_hema.fittedvalues.values})\n",
    "\n",
    "print('yhat pm2.5:')\n",
    "print(get_ols_res(fit2_hema).head(1))\n",
    "\n",
    "# fit model with true pm2.5\n",
    "fit2_hema_truepm25 = sm.OLS(df['y_hematopoietic'], X_truepm25).fit()\n",
    "print('\\ntrue pm2.5:')\n",
    "print(get_ols_res(fit2_hema_truepm25).head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
