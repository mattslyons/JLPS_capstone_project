{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Create a function that can be used in the 2nd stage regression to perform a time series cross validation. \n",
    "- Using an expanding window cross validation\n",
    "- Use sklearn API for the xgb function\n",
    "\n",
    "The 2nd stage regression predicts the medical outcomes using the predicted PM2.5 (and separately with the actual pm2.5), as well as the same fixed effects from the first stage regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional. I'm getting annoying warnings that I just want to ignore:\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import requests\n",
    "import urllib\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep this as false unless you want to save out the fitted model objects and results \n",
    "save_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set lag/lead times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations of Instruments and Fixed Effects here:\n",
    "https://docs.google.com/spreadsheets/d/1_MMYeQuxiov2OLE5AX0CE9R1T1mBrk7vpozy2fGjNBg/edit#gid=0 \n",
    "\n",
    "Instruments:\n",
    "\n",
    "- `Izmy_v3_normed_D_and_TPY`\n",
    "- `Izmy_v4_nodist_normed_TPY`\n",
    "- `Izmy_v5_all_normed_but_wspd_ratio`\n",
    "\n",
    "\n",
    "We will use lead and lag of 9 months and 3 months respectively.\n",
    "\n",
    "For fixed effects, we choose from a set of 4 possible combinations outlined here:\n",
    "\n",
    "- https://docs.google.com/spreadsheets/d/1_MMYeQuxiov2OLE5AX0CE9R1T1mBrk7vpozy2fGjNBg/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set instrumental variable version\n",
    "##  Predictor options:\n",
    "# - `Izmy_v3_normed_D_and_TPY`\n",
    "# - `Izmy_v4_nodist_normed_TPY`\n",
    "# - `Izmy_v5_all_normed_but_wspd_ratio`\n",
    "\n",
    "predictor = 'Izmy_v3_normed_D_and_TPY'\n",
    "\n",
    "# set FE to one of 4 sets (int)\n",
    "FE_set_num = 1\n",
    "\n",
    "# sets unique notebooks index (string)\n",
    "notebook_index = \"01\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1\n",
      "Target Name (target_name_s1) = pm25_r9\n",
      "Predictor Name (predictor_name_s1) = Izmy_v3_normed_D_and_TPY_r9\n",
      "\n",
      "Stage 2\n",
      "Health Outcome Lag Input (HO_lag_input) = _fwd3\n"
     ]
    }
   ],
   "source": [
    "lead_time = '9'\n",
    "lag_time = '3'\n",
    "lag_style = 'fwd'\n",
    "\n",
    "# define lead time for IV: 'last_month', 'r6', 'r9', 'r12'\n",
    "IV_lead = \"r\" + str(lead_time)\n",
    "HO_lag = lag_style + str(lag_time)\n",
    "\n",
    "if IV_lead:\n",
    "    IV_lead_input = \"_\" + IV_lead \n",
    "else:\n",
    "    # don't add underscore if empty string\n",
    "    IV_lead_input = IV_lead\n",
    "\n",
    "# define lag time for Health Outcome: '', 'fwd3', 'cent3', 'fwd6', 'cent6', 'fwd12', 'cent12'\n",
    "if HO_lag:\n",
    "    HO_lag_input = \"_\" + HO_lag \n",
    "else:\n",
    "    # don't add underscore if empty string\n",
    "    HO_lag_input = HO_lag\n",
    "\n",
    "# IV options: 1 month, 6 months, 9 months, 12 months\n",
    "IV_window_col = [f'pm25{IV_lead_input}']\n",
    "\n",
    "# health outcome options (fwd or cent): 1 month, 3 months, 6 months, 12 months\n",
    "health_outcome_window_col = [f'y_injuries{HO_lag_input}']\n",
    "\n",
    "filter_cols = IV_window_col + health_outcome_window_col # columns to filter out at the beginning and end of df, before modeling\n",
    "\n",
    "target_name_s1 = f'pm25{IV_lead_input}'\n",
    "predictor_name_s1 = f'{predictor}{IV_lead_input}'\n",
    "\n",
    "print(f\"Stage 1\\nTarget Name (target_name_s1) = {target_name_s1}\\nPredictor Name (predictor_name_s1) = {predictor_name_s1}\")\n",
    "\n",
    "print(f\"\\nStage 2\\nHealth Outcome Lag Input (HO_lag_input) = {HO_lag_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path\n",
    "\n",
    "Add a new elif section for your path if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local or gdrive\n",
    "path_source = 'local_anand'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  data_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  data_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "\n",
    "elif path_source == 'local_anand':\n",
    "  data_path = 'G:\\\\.shortcut-targets-by-id\\\\11wLy1WKwOTcthBs1rpfEzkqax2BZG-6E\\\\W210_Capstone\\\\Data'\n",
    "  fitted_models_path = 'G:\\\\.shortcut-targets-by-id\\\\11wLy1WKwOTcthBs1rpfEzkqax2BZG-6E\\W210_Capstone\\\\fitted_models\\\\2022-11-19\\\\XGB'\n",
    "\n",
    "  out_dir1 = 'G:\\\\.shortcut-targets-by-id\\\\11wLy1WKwOTcthBs1rpfEzkqax2BZG-6E\\W210_Capstone\\\\fitted_models\\\\2022-11-19\\\\XGB'\n",
    "\n",
    "elif path_source == 'local_cornelia':\n",
    "  in_dir_sc = 'C:/Users/cilin/Research/CA_hospitals_capstone/data/'\n",
    "  in_dir_h = 'C:/Users/cilin/Research/CA_hospitals_capstone/output/'\n",
    "  # folder containing stage 1 outputs\n",
    "  out_dir1 = 'C:/Users/cilin/Research/CA_hospitals_capstone/models_s1/'\n",
    "  # folder containing stage 2 outputs\n",
    "  out_dir2 = 'C:/Users/cilin/Research/CA_hospitals_capstone/models_s2/'\n",
    "  # folder containing csvs documenting which fixed effects are in which csv files\n",
    "  out_dir3 = 'C:/Users/cilin/Research/CA_hospitals_capstone/fixed_effects/'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  data_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our modeling data\n",
    "df = pd.read_csv(os.path.join(data_path, 'modeling_data/modeling_data_joined_11-22-top15_4tpy_ds_wind_ratios.csv'))\n",
    "\n",
    "# read in cornelia's healthcare data\n",
    "df1 = pd.read_csv(os.path.join(data_path, 'medical/hematopoietic_cancers.csv')).iloc[:,1:]\n",
    "df2 = pd.read_csv(os.path.join(data_path, 'medical/pediatric_vasculitis.csv')).iloc[:,1:]\n",
    "df3 = pd.read_csv(os.path.join(data_path, 'medical/type_1_diabetes.csv')).iloc[:,1:]\n",
    "df4 = pd.read_csv(os.path.join(data_path, 'medical/resp_cardio.csv')).iloc[:,1:]\n",
    "df5 = pd.read_csv(os.path.join(data_path, 'medical/injuries_accidents.csv')).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_selected_cols(df, cols_to_roll:list = ['Izmy_v1_unnormed'\\\n",
    "    ,'Izmy_v2_nodist_unnormed' \\\n",
    "    ,'Izmy_v3_normed_D_and_TPY' \\\n",
    "    ,'Izmy_v4_nodist_normed_TPY' \\\n",
    "    ,'Izmy_v5_all_normed'\n",
    "    ,'Izmy_v6_unnormed_no_wspd'\n",
    "    ,'Izmy_v7_all_normed_no_wspd'\n",
    "    ,'Izmy_v8_normed_D_and_TPY_no_wspd'\n",
    "    ,'new_alignment_90_high'\n",
    "    ,'avg_temp']\n",
    "    ,rolling_periods:list = [1, 6, 9, 12]):\n",
    "\n",
    "    \"\"\"Generates rolling averages for the input variables over the input time periods.\n",
    "    Inputs: df (pd dataframe): contains the data on a y-m level\n",
    "            cols_to_roll (list): list of columns to generate rolling avgs--must be in df\n",
    "            rolling_periods (list): list of time windows (in months) to roll over\n",
    "            \n",
    "    Outputs: df: Pandas dataframe containing the new columns\n",
    "             all_cols: list of list containing the new columns, separated by input type\"\"\"\n",
    "    \n",
    "    df_int = df.copy().sort_values(['school_zip', 'year_month']).reset_index(drop=True)\n",
    "    \n",
    "    all_cols_int = []\n",
    "\n",
    "    # Roll each variable\n",
    "    for col_index in range(len(cols_to_roll)):\n",
    "        new_cols = []\n",
    "\n",
    "        col_to_roll = cols_to_roll[col_index]\n",
    "        rolling_periods = [1, 6, 9, 12]\n",
    "\n",
    "        for period in rolling_periods:\n",
    "            df_int[f'{col_to_roll}_r{period}'] = df_int.groupby('school_zip')[col_to_roll]\\\n",
    "                .apply(lambda x: x.rolling(window=period, min_periods=period, closed='left').mean())\n",
    "            \n",
    "            new_cols.append(col_to_roll + \"_r\" + str(period))\n",
    "\n",
    "        all_cols_int.append([col_to_roll] + new_cols)\n",
    "        \n",
    "    return df_int, all_cols_int\n",
    "\n",
    "\n",
    "cols_to_roll = [predictor\n",
    "    ,'avg_wspd_top_15'\n",
    "    ,'avg_temp'\n",
    "    ,'diff_temp_s_ps']\n",
    "\n",
    "rolling_periods = [int(lead_time)]\n",
    "\n",
    "df, all_cols = roll_selected_cols(df=df, cols_to_roll=cols_to_roll, rolling_periods=rolling_periods)\n",
    "\n",
    "# rename the last month column just to be consistent and safe\n",
    "df.rename(columns={'pm25_last_month': 'pm25_r1'}, inplace=True)\n",
    "\n",
    "# print shape of data\n",
    "print('Shape of data ', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in nulls conditionally on merged datasets\n",
    "\n",
    "- the problem: for each health outcome, we want to fill in the nulls for a zipcode with 0's only if that row occurred after the first non-zero/not null visit in that zipcode for that health outcome. Keep them as nulls otherwise.\n",
    "\n",
    "- So basically a zipcode will keep the nulls if they're on a date before the first visit seen for that health outcome, nulls will become 0 after the first visit seen for that health outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nans(df, visits_cols = ['visits_hematopoietic_cancers', 'visits_injuries_accidents',\n",
    "       'visits_type_1_diabetes', 'visits_pediatric_vasculitis',\n",
    "       'visits_resp_cardio']):\n",
    "    \"\"\"Function to generate columns in place that replace NaNs with 0's only if that \n",
    "    row occurred after the first non-zero/not null visit in that zipcode for the specific\n",
    "    health outcome. Keeps them as nulls otherwise.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input dataframe\n",
    "        visits_cols (list, optional): list of columns to selectively filter NaNs\n",
    "    Returns:\n",
    "        DataFrame with columns replaced with their NaN-filtered versions\n",
    "    \"\"\"\n",
    "\n",
    "    def get_rowIndex(row):\n",
    "        \"\"\"Function intended for applying across df rows\n",
    "\n",
    "        Args:\n",
    "            row (int): row\n",
    "\n",
    "        Returns:\n",
    "            int: index of row\n",
    "        \"\"\"\n",
    "      \n",
    "        return row.name\n",
    "\n",
    "    def compare_and_replace(orig_visits, dataset_row_idx, school_zip):\n",
    "        \"\"\"Function intended for applying across df rows\n",
    "         Selectively replaces NaNs with 0's\n",
    "        Args:\n",
    "            orig_visits: original column that needs to be filtered\n",
    "            dataset_row_idx: column with row indices for the entire df\n",
    "            school_zip: column with school zips\n",
    "\n",
    "        Returns:\n",
    "            float or NaN\n",
    "        \"\"\"\n",
    "        \n",
    "        # school zip + zip idx\n",
    "        first_val_row_idx = dict_row_idx[school_zip]\n",
    "        zip_idx = dict_zip_idx[school_zip]\n",
    "        max_idx = dict_max_zipindex_per_zip[school_zip]\n",
    "        difference = max_idx - zip_idx + 1\n",
    "\n",
    "        # check the school zip first\n",
    "        if dataset_row_idx < first_val_row_idx:\n",
    "            orig_visits = orig_visits\n",
    "        elif (dataset_row_idx >= first_val_row_idx) and (dataset_row_idx <=  first_val_row_idx + difference):\n",
    "            if pd.isnull(orig_visits):\n",
    "                orig_visits = 0\n",
    "            else:\n",
    "                orig_visits = orig_visits\n",
    "        return orig_visits\n",
    "        \n",
    "    # group df by school_zip, year_month\n",
    "    df_grouped_schools = df.groupby(['school_zip', 'year_month']).tail(1)\n",
    "\n",
    "    #df_grouped_schools['points_rank'] = df.groupby(['team'])['points'].rank('dense', ascending=False)\n",
    "    unique_school_zips = list(df_grouped_schools['school_zip'].unique())\n",
    "\n",
    "    # generate overall row index\n",
    "    df_grouped_schools['rowIndex'] = df_grouped_schools.apply(get_rowIndex, axis=1)\n",
    "\n",
    "    # generate row indices that rest per school zip\n",
    "    df_grouped_schools['zipIndex'] = df_grouped_schools.groupby(['school_zip'])['year_month'].rank('first', ascending=True).astype(int)\n",
    "    df_grouped_schools['zipIndex'] = df_grouped_schools['zipIndex'] - 1\n",
    "\n",
    "    # generate dictionary that gets max index per school zip\n",
    "    dict_max_zipindex_per_zip = {}\n",
    "    for i in unique_school_zips:\n",
    "        dict_max_zipindex_per_zip[i] = df_grouped_schools[df_grouped_schools['school_zip']==i]['zipIndex'].max()\n",
    "\n",
    "    for i in visits_cols:\n",
    "        dict_zip_idx = {}\n",
    "        dict_row_idx = {}\n",
    "        for j in unique_school_zips:\n",
    "            temp = df_grouped_schools[df_grouped_schools['school_zip']==j]\n",
    "            #display(temp)\n",
    "            #temp['rowIndex'] = temp.apply(get_rowIndex, axis=1)\n",
    "            visits_series = pd.Series(temp[i]) # one school zip, filtered to 1 health outcome\n",
    "            #display(visits_series)\n",
    "            bool_not_null = visits_series.notnull()\n",
    "            all_indices_not_null = np.where(bool_not_null)[0]\n",
    "\n",
    "            # save index of the first non-NaN value within the zipcode indices\n",
    "            # if everything every value for zip is NaN, set value to # of records in df\n",
    "            try:\n",
    "                groupby_index = all_indices_not_null[0]\n",
    "            except IndexError:\n",
    "                groupby_index = df_grouped_schools.shape[0]\n",
    "            dict_zip_idx[j] = groupby_index\n",
    "            \n",
    "            # save index of the row from whole dataset; set valye to # of records in df if not\n",
    "            try:\n",
    "                row_idx = temp.loc[temp['zipIndex'] == groupby_index, 'rowIndex'].values[0]\n",
    "            except IndexError:\n",
    "                row_idx = df_grouped_schools.shape[0]\n",
    "            dict_row_idx[j] = row_idx\n",
    "        \n",
    "        df_grouped_schools[i] = df_grouped_schools.apply(lambda row: compare_and_replace(row[i], row['rowIndex'], row['school_zip']), axis=1)\n",
    "\n",
    "    # drop rowIndex and zipIndex cols\n",
    "    df_grouped_schools.drop(columns=['rowIndex', 'zipIndex'], inplace=True)\n",
    "\n",
    "    return df_grouped_schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# med data:\n",
    "\n",
    "# get all distinct patzip_year_month\n",
    "all_pats = df1['patzip_year_month'].to_list() + \\\n",
    "  df2['patzip_year_month'].to_list() + \\\n",
    "  df3['patzip_year_month'].to_list() + \\\n",
    "  df4['patzip_year_month'].to_list() + \\\n",
    "  df5['patzip_year_month'].to_list() \n",
    "all_pats = list(set(all_pats))\n",
    "df_med = pd.DataFrame({'patzip_year_month': all_pats})\n",
    "\n",
    "# rename columns more intuitively\n",
    "df1 = df1.rename(columns={'number_of_visits': 'number_of_visits_hem_cancers'})\n",
    "df2 = df2.rename(columns={'number_of_visits': 'number_of_visits_vasc'})\n",
    "df3 = df3.rename(columns={'number_of_visits': 'number_of_visits_diab'})\n",
    "df4 = df4.rename(columns={'number_of_visits': 'number_of_visits_resp_cardio'})\n",
    "df5 = df5.rename(columns={'number_of_visits': 'number_of_visits_injuries'})\n",
    "\n",
    "# now join all the diagnoses on this dataset\n",
    "df_med = df_med\\\n",
    "  .merge(df1, on='patzip_year_month', how='left')\\\n",
    "  .merge(df2, on='patzip_year_month', how='left')\\\n",
    "  .merge(df3, on='patzip_year_month', how='left')\\\n",
    "  .merge(df4, on='patzip_year_month', how='left')\\\n",
    "  .merge(df5, on='patzip_year_month', how='left')\n",
    "\n",
    "# join data\n",
    "if isinstance(df.year_month[0], str):\n",
    "  # if year month is still a string, convert it to datetime\n",
    "  # don't try if already converted\n",
    "    df['year_month'] = df['year_month'].map(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df['zip_year_month'] = df['school_zip'].astype(str) + '-' +\\\n",
    "  df['year_month'].dt.year.astype(str) + '-' +\\\n",
    "  df['year_month'].dt.month.astype(str)\n",
    "\n",
    "df = pd.merge(df, df_med, left_on='zip_year_month', right_on='patzip_year_month', how='left')\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# for missing med data, assume there were 0 cases:\n",
    "med_vars = ['hematopoietic_cancers', 'number_of_visits_hem_cancers', \n",
    "  'pediatric_vasculitis', 'number_of_visits_vasc', \n",
    "  'type_1_diabetes', 'number_of_visits_diab',\n",
    "  'resp_cardio', 'number_of_visits_resp_cardio',\n",
    "  'injuries_accidents', 'number_of_visits_injuries'\n",
    "  ]\n",
    "\n",
    "\n",
    "# for var in med_vars:\n",
    "#   df[var] = df[var].fillna(0)\n",
    "\n",
    "\n",
    "# df.sort_values(['school_zip', 'year_month'], inplace=True)\n",
    "\n",
    "# Insert code here to populate na's for each HO with 0, only if there was a HO in this zipcode before\n",
    "df = filter_nans(df, visits_cols = ['number_of_visits_hem_cancers', 'number_of_visits_vasc', \n",
    "'number_of_visits_diab', 'number_of_visits_resp_cardio', 'number_of_visits_injuries'])\n",
    "\n",
    "\n",
    "# fixing month datatype\n",
    "df['month'] = df['month'].astype(str)\n",
    "\n",
    "# Create response variables, which is visits / population\n",
    "df['y_hematopoietic'] = 1000 * df['number_of_visits_hem_cancers'] / df['total_pop_under19']\n",
    "df['y_vasculitis'] = 1000 * df['number_of_visits_vasc'] / df['total_pop_under19']\n",
    "df['y_diabetes'] = 1000 * df['number_of_visits_diab'] / df['total_pop_under19']\n",
    "df['y_resp_cardio'] = 1000 * df['number_of_visits_resp_cardio'] / df['total_pop_under19']\n",
    "df['y_injuries'] = 1000 * df['number_of_visits_injuries'] / df['total_pop_under19']\n",
    "\n",
    "# Create an option for a logged version of the treatment var (log(1+x)). this makes it normally distributed \n",
    "df['pm25_log'] = np.log1p(df['pm25'])\n",
    "\n",
    "# create year trend feature\n",
    "df['year_trend'] = df['year'] - 1999\n",
    "\n",
    "# create county_month\n",
    "df['county_month'] = df.apply(lambda df: df['month'].rjust(2, '0') + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# create year_month_county (in case we want to just direclty use this var for the interaction effects)\n",
    "df['year_month_county'] = df.apply(lambda df: str(df['year']) + '_' + df['month'] + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# no need to one hot encode anymore, b/c data is already encoded \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Rolling HO Sum Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data on date\n",
    "df = df.sort_values('year_month').reset_index(drop=True)\n",
    "\n",
    "# train/test split \n",
    "# keep 2018 as the held out test set \n",
    "df_test = df[df.year == 2018]\n",
    "df = df[df.year != 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns added for health outcomes using 3 month window:\n",
      "['number_of_visits_hem_cancers_fwd3', 'y_hematopoietic_fwd3', 'number_of_visits_hem_cancers_cent3', 'y_hematopoietic_cent3', 'number_of_visits_vasc_fwd3', 'y_vasculitis_fwd3', 'number_of_visits_vasc_cent3', 'y_vasculitis_cent3', 'number_of_visits_diab_fwd3', 'y_diabetes_fwd3', 'number_of_visits_diab_cent3', 'y_diabetes_cent3', 'number_of_visits_resp_cardio_fwd3', 'y_resp_cardio_fwd3', 'number_of_visits_resp_cardio_cent3', 'y_resp_cardio_cent3', 'number_of_visits_injuries_fwd3', 'y_injuries_fwd3', 'number_of_visits_injuries_cent3', 'y_injuries_cent3']\n",
      "\n",
      "Columns added for health outcomes using 6 month window:\n",
      "['number_of_visits_hem_cancers_fwd6', 'y_hematopoietic_fwd6', 'number_of_visits_hem_cancers_cent6', 'y_hematopoietic_cent6', 'number_of_visits_vasc_fwd6', 'y_vasculitis_fwd6', 'number_of_visits_vasc_cent6', 'y_vasculitis_cent6', 'number_of_visits_diab_fwd6', 'y_diabetes_fwd6', 'number_of_visits_diab_cent6', 'y_diabetes_cent6', 'number_of_visits_resp_cardio_fwd6', 'y_resp_cardio_fwd6', 'number_of_visits_resp_cardio_cent6', 'y_resp_cardio_cent6', 'number_of_visits_injuries_fwd6', 'y_injuries_fwd6', 'number_of_visits_injuries_cent6', 'y_injuries_cent6']\n",
      "\n",
      "Columns added for health outcomes using 12 month window:\n",
      "['number_of_visits_hem_cancers_fwd12', 'y_hematopoietic_fwd12', 'number_of_visits_hem_cancers_cent12', 'y_hematopoietic_cent12', 'number_of_visits_vasc_fwd12', 'y_vasculitis_fwd12', 'number_of_visits_vasc_cent12', 'y_vasculitis_cent12', 'number_of_visits_diab_fwd12', 'y_diabetes_fwd12', 'number_of_visits_diab_cent12', 'y_diabetes_cent12', 'number_of_visits_resp_cardio_fwd12', 'y_resp_cardio_fwd12', 'number_of_visits_resp_cardio_cent12', 'y_resp_cardio_cent12', 'number_of_visits_injuries_fwd12', 'y_injuries_fwd12', 'number_of_visits_injuries_cent12', 'y_injuries_cent12']\n"
     ]
    }
   ],
   "source": [
    "# get rolling n month sum\n",
    "def create_rolling_sum(df, var_name:str = 'number_of_visits_hem_cancers', num_months=3, center_arg:bool = False):\n",
    "  \"\"\"\n",
    "    Creates rolling sums for the number of visits for a given health outcome. \n",
    "    Overwrite your dataframe with the output.\n",
    "    Function saves the result as a column into the dataframe with subscripts \n",
    "    - '{var_name}_fwd{number of months}' for forward sums\n",
    "    - '{var_name}_cent{number of months}' for centered sums\n",
    "\n",
    "    Function includes the current month as one of the months in num_months.\n",
    "\n",
    "    Dataframe input MUST be sorted by ['school_zip', 'year_month'] ahead of time.\n",
    "\n",
    "    `df = df.sort_values(['school_zip', 'year_month'])`\n",
    "\n",
    "    Suggested: filter out tail end of dates so rolling averages are not filled with imputed values.\n",
    "\n",
    "  Args:\n",
    "      `df` (dataframe): dataframe having columns for 'school_zip', datetime 'year_month', and number of visits. Dataframe must be sorted by \n",
    "      `var_name` (str, optional): health outcome number of visits. Defaults to 'number_of_visits_hem_cancers'.\n",
    "      `num_months` (int, optional): Number of months to take rolling sum over. Defaults to 3.\n",
    "      `center_arg` (bool, optional): If this sum should be centered on current month. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "      `df_int`: returns dataframe with column added\n",
    "  \"\"\"\n",
    "  df_int = df.copy().sort_values(['school_zip', 'year_month']).reset_index(drop=True)\n",
    "  \n",
    "  if center_arg:\n",
    "    df_int[f'{var_name}_cent{num_months}'] = df_int.groupby('school_zip')[var_name]\\\n",
    "                                      .apply(lambda x:x.rolling(num_months, center=True).sum())\n",
    "  else:\n",
    "    df_int[f'{var_name}_fwd{num_months}'] = df_int.groupby('school_zip')[var_name]\\\n",
    "                                      .apply(lambda x:x.rolling(num_months).sum().shift(1-num_months))\n",
    "\n",
    "  \n",
    "  return df_int \n",
    "\n",
    "\n",
    "df = df.sort_values(['school_zip', 'year_month'])\n",
    "starting_cols = list(df.columns)\n",
    "\n",
    "num_visits_col_names = ['number_of_visits_hem_cancers', \n",
    "  'number_of_visits_vasc', \n",
    "  'number_of_visits_diab',\n",
    "  'number_of_visits_resp_cardio',\n",
    "  'number_of_visits_injuries'\n",
    "  ]\n",
    "\n",
    "y_col_names = ['y_hematopoietic', \n",
    "  'y_vasculitis', \n",
    "  'y_diabetes',\n",
    "  'y_resp_cardio',\n",
    "  'y_injuries'\n",
    "  ]\n",
    "\n",
    "# 3 months ---\n",
    "n = 3 # specify number of months\n",
    "\n",
    "for health_outcome_y_col, health_outcome_visits_col in zip(y_col_names, num_visits_col_names):\n",
    "    # forward looking columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=False)\n",
    "    df[f'{health_outcome_y_col}_fwd{n}'] = 1000 * df[f'{health_outcome_visits_col}_fwd{n}'] / df['total_pop_under19']\n",
    "\n",
    "    # centered columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=True)\n",
    "    df[f'{health_outcome_y_col}_cent{n}'] = 1000 * df[f'{health_outcome_visits_col}_cent{n}'] / df['total_pop_under19']\n",
    "\n",
    "\n",
    "# print columns added\n",
    "ending_cols = list(df.columns)\n",
    "window_3months_columns = [c for c in ending_cols if c not in starting_cols]\n",
    "print(f\"\\nColumns added for health outcomes using {n} month window:\\n{window_3months_columns}\")\n",
    "starting_cols = list(df.columns)\n",
    "\n",
    "\n",
    "# filter data to appropriate data range\n",
    "df = df[df.year >= 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for modeling\n",
    "date_var = 'year_month' \n",
    "zip_var = 'school_zip'\n",
    "y_var_s2 = 'y_hematopoietic' + HO_lag_input\n",
    "\n",
    "# stage 1 variables\n",
    "instruments_cols = [predictor]\n",
    "\n",
    "stage_1_IVs = [s + IV_lead_input for s in instruments_cols]\n",
    "stage_1_target = [target_name_s1]\n",
    "\n",
    "# stage 2 variables\n",
    "stage_2_HO_targets = [s + HO_lag_input for s in y_col_names]\n",
    "\n",
    "num_vars = ['school_elevation_m', 'nearby_point_source_count', 'school_wspd', \\\n",
    "            'tax_liability_per_capita', 'school_temperature', 'school_count', 'pm25_r6', 'pm25_r12']\n",
    "counties = [i for i in df.columns if re.search('^school_county_v2_', i)]\n",
    "months = [i for i in df.columns if re.search ('^month_', i)]\n",
    "# potentially use county_month instead of the above \n",
    "\n",
    "xvars = num_vars + counties + months \n",
    "yvar = [y_var_s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Stage 1 Variables---\n",
      "\n",
      "target_name_s1: pm25_r9\n",
      "\n",
      "predictor_name_s1: Izmy_v3_normed_D_and_TPY_r9\n",
      "\n",
      "Fixed Effects (fixed_effects_cols): ['avg_temp_r9', 'avg_elevation_diff_m', 'school_county_v2_alameda', 'school_county_v2_alpine', 'school_county_v2_amador', 'school_county_v2_butte', 'school_county_v2_calaveras', 'school_county_v2_colusa', 'school_county_v2_contra_costa', 'school_county_v2_del_norte', 'school_county_v2_el_dorado', 'school_county_v2_fresno', 'school_county_v2_glenn', 'school_county_v2_humboldt', 'school_county_v2_imperial', 'school_county_v2_inyo', 'school_county_v2_kern', 'school_county_v2_kings', 'school_county_v2_lake', 'school_county_v2_lassen', 'school_county_v2_los_angeles', 'school_county_v2_madera', 'school_county_v2_marin', 'school_county_v2_mariposa', 'school_county_v2_mendocino', 'school_county_v2_merced', 'school_county_v2_modoc', 'school_county_v2_mono', 'school_county_v2_monterey', 'school_county_v2_napa', 'school_county_v2_nevada', 'school_county_v2_orange', 'school_county_v2_placer', 'school_county_v2_plumas', 'school_county_v2_riverside', 'school_county_v2_sacramento', 'school_county_v2_san_benito', 'school_county_v2_san_bernardino', 'school_county_v2_san_diego', 'school_county_v2_san_francisco', 'school_county_v2_san_joaquin', 'school_county_v2_san_luis_obispo', 'school_county_v2_san_mateo', 'school_county_v2_santa_barbara', 'school_county_v2_santa_clara', 'school_county_v2_santa_cruz', 'school_county_v2_shasta', 'school_county_v2_sierra', 'school_county_v2_siskiyou', 'school_county_v2_solano', 'school_county_v2_sonoma', 'school_county_v2_stanislaus', 'school_county_v2_sutter', 'school_county_v2_tehama', 'school_county_v2_trinity', 'school_county_v2_tulare', 'school_county_v2_tuolumne', 'school_county_v2_ventura', 'school_county_v2_yolo', 'school_county_v2_yuba', 'month_01', 'month_02', 'month_03', 'month_04', 'month_05', 'month_06', 'month_07', 'month_08', 'month_09', 'month_10', 'month_11', 'month_12', 'year_trend']\n",
      "\n",
      "Saving predictions (target_name_s1_predictions) as `pm25_r9_hat` and as `y_hat`\n",
      "Size of df before filtering for modeling: (262674, 305)\n",
      "Size of df after filtering for modeling: (262450, 305)\n"
     ]
    }
   ],
   "source": [
    "# specify the fixed effects column here\n",
    "fixed_effects_cols = ['avg_temp_r9', 'avg_elevation_diff_m'] + counties + months + ['year_trend']\n",
    "\n",
    "# check if all these columns are in the dataframe\n",
    "in_col_list = [True if i in df.columns else i for i in fixed_effects_cols ]\n",
    "\n",
    "print(f\"{in_col_list}\")\n",
    "\n",
    "print(f\"\\nStage 1 Variables---\\n\")\n",
    "print(f\"target_name_s1: {target_name_s1}\\n\")\n",
    "print(f\"predictor_name_s1: {predictor_name_s1}\\n\")\n",
    "print(f\"Fixed Effects (fixed_effects_cols): {fixed_effects_cols}\\n\")\n",
    "\n",
    "target_name_s1_predictions = target_name_s1 + \"_hat\"\n",
    "print(f\"Saving predictions (target_name_s1_predictions) as `{target_name_s1_predictions}`\")\n",
    "\n",
    "# create a df for modeling stage 1: drops nulls in all columns used\n",
    "df_model_s1 = df.dropna(subset=([target_name_s1, predictor_name_s1] + fixed_effects_cols))\n",
    "\n",
    "print(f\"Size of df before filtering for modeling: {df.shape}\")\n",
    "print(f\"Size of df after filtering for modeling: {df_model_s1.shape}\")\n",
    "\n",
    "\n",
    "X_s1 = df_model_s1[[predictor_name_s1] + fixed_effects_cols]\n",
    "y_s1 = df_model_s1[target_name_s1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to sort the values and use the index to sort things\n",
    "df_model_s1.sort_values(by=['year_month'], inplace=True)\n",
    "df_model_s1 = df_model_s1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn api version\n",
    "def time_series_cv(\n",
    "  df: pd.DataFrame, \n",
    "  xvars: list, \n",
    "  yvar: str, \n",
    "  hyperparams: dict = {'max_depth': [1, 5, 10], 'subsample': [.8, 1], 'eta': [.1, .3]}, \n",
    "  search_type='grid', \n",
    "  folds=5, \n",
    "  verbose=1):\n",
    "\n",
    "  ''' \n",
    "  Inputs:\n",
    "  - df: dataframe of your training data\n",
    "  - xvars: a list of all the xvars to pass to xgboost\n",
    "  - yvar: string of your target variable\n",
    "  - verbose: optionality for diff amounts of printouts. Can be 0, 1, 2. 0 = silent, 1 = update after each fold, 2 = update after every single hyperparam combination. \n",
    "  - hyperparams: this must be a dictionary of lists. So each key is a xgb hyperparam, then it must have a list of values to tune with. \n",
    "    See the default for an example. Can put in an arbitrary number of hyperparam options. \n",
    "  \n",
    "  Output:\n",
    "  - dictionary with the following keys: ['fold', 'hyperparams', 'rmse_train', 'rmse_test']. \n",
    "  eval(best_hyperparams)\n",
    "  - dictionary with the best hyperparameters to retrain model\n",
    "  '''\n",
    "\n",
    "  # this dictionary will hold all the final results\n",
    "  final_res = {'fold':[], 'hyperparams':[], 'rmse_train': [], 'rmse_test': [],\n",
    "                'huber_loss_train': [], 'huber_loss_test': []}\n",
    "\n",
    "  # get only necessary fields in df\n",
    "  df = df[xvars + [yvar]]\n",
    "\n",
    "  # set up the time series split class, to do an expanding window cross fold. \n",
    "  tss = TimeSeriesSplit(n_splits=folds)\n",
    "  tss_folds = tss.split(df)\n",
    "  all_folds = [i for i in tss_folds]\n",
    "\n",
    "  # get all combinations of hyperparams\n",
    "  def expand_grid(hyperparams):\n",
    "    keys = list(hyperparams.keys())\n",
    "    hyperparams_df = pd.DataFrame(np.array(np.meshgrid(*[hyperparams[key_i] for key_i in keys])).T.reshape(-1, len(keys)))\n",
    "    hyperparams_df.columns = keys \n",
    "    return hyperparams_df\n",
    "\n",
    "  df_hyperparams = expand_grid(hyperparams)\n",
    "\n",
    "  # loss functions\n",
    "  def get_rmse(df_train, model):\n",
    "    ytrue = df_train[yvar].values.flatten()\n",
    "    yhat = model.predict(df_train.drop(columns=yvar))\n",
    "    rmse = np.mean(((ytrue - yhat)**2)**.5)\n",
    "    return rmse \n",
    "  \n",
    "  def get_huber_loss(df_train, model):\n",
    "    # # Let the delta for Huber Loss be 2*standard deviation of non-zero entries for the y variable\n",
    "    # twice_std = 2 * df_train[df_train[yvar] > 0][yvar].std  \n",
    "\n",
    "    # Let the delta for Huber Loss be 2*standard deviation of the y variable\n",
    "    twice_std = 2 * df_train[yvar].std() \n",
    "\n",
    "    def huber_loss(y_actual,y_predicted,delta):\n",
    "      # https://towardsdatascience.com/understanding-loss-functions-the-smart-way-904266e9393\n",
    "      # approaches MSE for small error an approaches MAE in case of outliers.\n",
    "      delta = 5\n",
    "      total_points = y_actual.size\n",
    "      total_error = 0\n",
    "      for i in range(total_points):\n",
    "        error = np.absolute(y_predicted[i] - y_actual[i])\n",
    "        if error < delta:\n",
    "          huber_error = (error*error)/2\n",
    "        else:\n",
    "          huber_error = delta*(error - (0.5*delta))\n",
    "        total_error+=huber_error\n",
    "      total_huber_error = total_error/total_points\n",
    "      return total_huber_error  # mean huber_loss\n",
    "\n",
    "    ytrue = df_train[yvar].values.flatten()\n",
    "    yhat = model.predict(df_train.drop(columns=yvar))\n",
    "\n",
    "    huber_loss_val = huber_loss(y_actual=ytrue, y_predicted=yhat, delta=twice_std)\n",
    "\n",
    "    return huber_loss_val\n",
    "\n",
    "  # loop over each expanding time series window\n",
    "  for fold_count,fold in enumerate(all_folds):\n",
    "    if verbose > 0:\n",
    "      print('Working on fold {}/{}'.format(fold_count+1, folds))\n",
    "\n",
    "    df_train = df.loc[fold[0]]\n",
    "    df_test = df.loc[fold[1]]\n",
    "\n",
    "    # within each time series cross fold, perform a grid search with all hyperparam combinations and evaluate results. \n",
    "    if search_type == 'grid':\n",
    "      for param_set_i in range(df_hyperparams.shape[0]):\n",
    "        hyperparams_i = {x:y for x,y in zip(df_hyperparams.columns, df_hyperparams.loc[param_set_i].to_list())}\n",
    "        \n",
    "        # fix datatype for some vars\n",
    "        def fix_int(var, hyperparams_i):\n",
    "          if var in hyperparams_i.keys():\n",
    "            hyperparams_i[var] = int(hyperparams_i[var])\n",
    "          \n",
    "          return hyperparams_i\n",
    "        \n",
    "        hyperparams_i = fix_int('max_depth', hyperparams_i)\n",
    "        hyperparams_i = fix_int('n_estimators', hyperparams_i)\n",
    "\n",
    "        # if adding hyperparams based on integeters, do this fix_int so it isnt converted to float\n",
    "        \n",
    "\n",
    "        # fit xgb\n",
    "        xgb_reg = xgb.XGBRegressor(booster=\"gbtree\", **hyperparams_i, verbosity=0, random_state=20)\n",
    "        xgb_reg = xgb_reg.fit(X=df_train[xvars], y=df_train[yvar], eval_set=[(df_test[xvars], df_test[yvar])], verbose=0)\n",
    "        \n",
    "        # save results\n",
    "        rmse_train = get_rmse(df_train, xgb_reg)\n",
    "        rmse_test = get_rmse(df_test, xgb_reg)\n",
    "        huber_train = get_huber_loss(df_train, xgb_reg)\n",
    "        huber_test = get_huber_loss(df_test, xgb_reg)\n",
    "        final_res['fold'].append(fold_count)\n",
    "        final_res['hyperparams'].append(hyperparams_i)\n",
    "        final_res['rmse_train'].append(rmse_train)\n",
    "        final_res['rmse_test'].append(rmse_test)\n",
    "        final_res['huber_loss_train'].append(huber_train)\n",
    "        final_res['huber_loss_test'].append(huber_test)\n",
    "\n",
    "        if verbose == 2:\n",
    "          print('{}: rmse train: {:.3f}, rmse test: {:.3f}'.format(hyperparams_i, rmse_train, rmse_test))\n",
    "\n",
    "    elif search_type == 'random': \n",
    "      pass \n",
    "      # haven't done this yet\n",
    "  \n",
    "  # print out final best hyperparams before returning the output\n",
    "  output2 = pd.DataFrame({\n",
    "    'hyperparams': final_res['hyperparams'],\n",
    "    'fold': final_res['fold'],\n",
    "    'rmse_train': final_res['rmse_train'],\n",
    "    'rmse_test': final_res['rmse_test']\n",
    "  })\n",
    "  output2['hyperparams'] = output2['hyperparams'].astype(str)\n",
    "  output2 = output2.groupby('hyperparams')[['rmse_train', 'rmse_test']].mean().reset_index().sort_values('rmse_test')\n",
    "  print('best hyperparams: {}'.format(output2.iloc[0,0]))\n",
    "  print(f\"best RMSE test: {output2.iloc[0]['rmse_test']}\")\n",
    "\n",
    "  best_hyperparams = output2.iloc[0,0]\n",
    "\n",
    "  \n",
    "  return final_res, eval(best_hyperparams)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Sklearn CV function for Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s1, best_params_s1 = time_series_cv(df_model_s1, \n",
    "  xvars = ([predictor_name_s1] + fixed_effects_cols), \n",
    "  yvar = target_name_s1, \n",
    "  hyperparams = {'max_depth': [2, 5], 'subsample': [.8, 1], 'eta': [.1], 'n_estimators': [100, 200]}, \n",
    "  search_type = 'grid', \n",
    "  folds = 5, \n",
    "  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross val for the best hyperparams \n",
    "\n",
    "Note, I didn't run this again bc it takes forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters to search over\n",
    "params_stage1 = {'max_depth': [3,6,10, 15, 18],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'n_estimators': [100, 140, 180],\n",
    "                'colsample_bytree': [1]}\n",
    "\n",
    "xgbr_s1 = xgb.XGBRegressor(seed = 20)   # give a seed for reproducibility\n",
    "\n",
    "# how to force gridsearch to take 1 cv split: https://stackoverflow.com/a/44682305\n",
    "clf_s1 = GridSearchCV(estimator=xgbr_s1, \n",
    "                   param_grid=params_stage1,\n",
    "                   scoring='neg_mean_squared_error', \n",
    "                   verbose=2,\n",
    "                   cv=[(slice(None), slice(None))])\n",
    "clf_s1.fit(X_s1, y_s1)\n",
    "print(\"Best parameters:\", clf_s1.best_params_)\n",
    "print(\"Lowest RMSE: \", (-clf_s1.best_score_)**(1/2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best result preds\n",
    "y_hat = clf_s1.best_estimator_.predict(X_s1)\n",
    "rmse_val = np.mean(((y - y_hat)**2)**.5)\n",
    "print(rmse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit one more model w/ best hyperparams for Stage 1\n",
    "\n",
    "Once we have our best parameters for stage 1, just hard code the parameters below and don't run hyperparameter tuning for stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify best hyperparams for Stage 1\n",
    "\n",
    "# hard code this because we don't want to retune many times on Cornelia's PC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit one more model w/ best hyperparams\n",
    "xgbr_s1_best = xgb.XGBRegressor(**best_params_s1, random_state=20)\n",
    "xgbr_s1_best.fit(X_s1, y_s1)\n",
    "y_hat = xgbr_s1_best.predict(X_s1)\n",
    "\n",
    "# make sure its the same\n",
    "rmse_val = np.mean(((y_s1 - y_hat)**2)**.5)\n",
    "print(f\"RMSE of the best Stage 1 Model: {rmse_val}\")\n",
    "\n",
    "df_model_s1[target_name_s1+\"_hat\"] = y_hat # generate predicted pm2.5\n",
    "\n",
    "columns = [target_name_s1, target_name_s1+'_hat']\n",
    "\n",
    "# compute visits by patzip_year_month\n",
    "fig, axes = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(10, 5))\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    sns.histplot(\n",
    "            df_model_s1[columns[idx]],\n",
    "            ax=ax\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the df_model for Stage 2: `df_model_s2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name of the target variable for Stage 2: {y_var_s2}\\n\")\n",
    "\n",
    "predictor_name_s2 = target_name_s1 + \"_hat\"\n",
    "\n",
    "print(f\"Name of the predictor variable for Stage 2: {predictor_name_s2}\\n\")\n",
    "\n",
    "# drop na's in columns of interes, sort by year_month and reset index\n",
    "df_model_s2 = df_model_s1.dropna(subset=([y_var_s2, predictor_name_s2] + fixed_effects_cols))\n",
    "df_model_s2 = df_model_s2.sort_values('year_month').reset_index(drop=True)\n",
    "\n",
    "print(f\"Size of 2nd stage df before filtering for modeling: {df_model_s1.shape}\")\n",
    "print(f\"Size of 2nd stage df after filtering for modeling: {df_model_s2.shape}\")\n",
    "\n",
    "X_s2 = df_model_s2[[predictor_name_s2] + fixed_effects_cols]\n",
    "y_s2 = df_model_s2[y_var_s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1/5\n",
      "Working on fold 2/5\n",
      "Working on fold 3/5\n",
      "Working on fold 4/5\n",
      "Working on fold 5/5\n",
      "best hyperparams: {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}\n",
      "best RMSE test: 1.0383948782849888\n"
     ]
    }
   ],
   "source": [
    "output_s2, best_params_s2 = time_series_cv(df_model_s2, \n",
    "  xvars = ([predictor_name_s2] + fixed_effects_cols), \n",
    "  yvar = y_var_s2, \n",
    "  hyperparams = {'max_depth': [2, 5], 'subsample': [.8, 1], 'eta': [.1], 'n_estimators': [100, 200]}, \n",
    "  search_type = 'grid', \n",
    "  folds = 5, \n",
    "  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 1\n",
      "{'fold': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4], 'hyperparams': [{'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}, {'max_depth': 2, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1, 'n_estimators': 200}, {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 200}], 'rmse_train': [0.5270570602389184, 0.5267665295952021, 0.45423537609939557, 0.46227178262582197, 0.517232612189757, 0.517565974029089, 0.4040042485774871, 0.4180288186816825, 0.6030189142828044, 0.6028902440140591, 0.5499027379638435, 0.547505599992802, 0.5961349455133951, 0.5948495819690259, 0.5065046734965801, 0.5080394969472061, 0.6802478172841319, 0.6817495354168966, 0.6059179076049108, 0.6073247938764419, 0.6753754253739044, 0.6741179861072782, 0.5695369954383398, 0.5739014545795543, 0.8160534932217234, 0.8104150130882375, 0.7161444430549143, 0.7146133960275808, 0.8116199342652699, 0.8016401707272108, 0.6722240505670298, 0.6700147165973486, 0.8983622899051703, 0.8992553240564063, 0.7963495902828066, 0.8055356921893175, 0.8950083937796615, 0.9010007405569791, 0.7571952657206261, 0.7636446374533832], 'rmse_test': [0.661617715631965, 0.6397237099112191, 0.7215986843962132, 0.7311513563255042, 0.6863451168152449, 0.6628381966208201, 0.7523952111346347, 0.7905226354337264, 0.8653987790353367, 0.870608583477044, 0.9543078309396794, 1.012196168306537, 0.9048057584410276, 0.9278880191730973, 1.059283865899154, 1.1024169748261579, 1.0299800819849838, 0.9801037510320397, 1.0880171884848924, 1.0733861205931325, 1.073477072274436, 1.0481215171040863, 1.1150707601253897, 1.0948012135322265, 1.8000058137987838, 1.3440647765289766, 1.711567094625262, 1.3140552262910825, 2.438834558886297, 1.431385163869687, 1.8761377998354583, 1.379088351259411, 1.3437745720922671, 1.3574735704756642, 1.3786633168550275, 1.3572086786779782, 1.359800251959592, 1.3742232352237602, 1.398499781231135, 1.3869391499766166], 'huber_loss_train': [0.30001789813350815, 0.30143058918691046, 0.21152274165318086, 0.22308704026420198, 0.2876323914625498, 0.28891415472911836, 0.1653671119509916, 0.18092997854076448, 0.5109894951855953, 0.5097914881376279, 0.388823155729879, 0.3849572672866967, 0.4926075970247563, 0.4904593944974811, 0.3037693489977304, 0.310730895216557, 0.7753847162893821, 0.7720830131829303, 0.5491569741330022, 0.5522772147372064, 0.752354576712226, 0.7431080499793584, 0.45516502531069025, 0.47816032147396786, 1.209937412789733, 1.1618319999399667, 0.904511426446307, 0.8876349823452362, 1.1946443463559997, 1.1372046749736504, 0.7528450245538834, 0.7495214797696227, 1.429808645110118, 1.446474521619284, 1.1289385339884326, 1.1555786814194242, 1.4108256847201777, 1.452278797205898, 0.9911541074728089, 1.0182591845361049], 'huber_loss_test': [0.795257633706498, 0.7887773596197829, 0.8629903266715209, 0.9001153092959331, 0.8470942387488425, 0.8195091521211879, 0.8967445768156477, 0.9811467250418352, 1.4407492833662428, 1.4882242106802663, 1.6850618434905131, 1.761960878459335, 1.5557327345780763, 1.6450558411282652, 1.9111602262817646, 1.920148389633527, 2.2109580689656116, 2.086441857179293, 2.606545376010631, 2.552229595903305, 2.3937676776991936, 2.36100252821674, 2.6912949383524554, 2.6521720858228894, 5.36883877084499, 3.0001974141893832, 4.932561079170708, 2.9969341962492986, 8.542733121137823, 3.436862661793608, 5.668349487564113, 3.3132850555470417, 3.3299681071007545, 3.3514511426980254, 3.5781089208273453, 3.4265102029084904, 3.368996307069097, 3.3878277135965127, 3.6688356593438733, 3.558187381037876]}\n",
      "\n",
      "\n",
      " -------------------------------------------------- \n",
      "\n",
      "\n",
      "output 2\n",
      "{'max_depth': 2, 'subsample': 1.0, 'eta': 0.1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# see full outputs\n",
    "print('output 1')\n",
    "print(output_s2[0])\n",
    "print('\\n\\n -------------------------------------------------- \\n\\n')\n",
    "print('output 2')\n",
    "print(output_s2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train final Stage 2 Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using best stage 2's hyper parameters\n",
    "# fit one more model w/ best hyperparams\n",
    "xgbr_s2_best = xgb.XGBRegressor(**best_params_s2, random_state=20)\n",
    "xgbr_s2_best.fit(X_s2, y_s2)\n",
    "y_s2_hat = xgbr_s2_best.predict(X_s2)\n",
    "\n",
    "# make sure its the same\n",
    "rmse_val_s2 = np.mean(((y_s2 - y_s2_hat)**2)**.5)\n",
    "print(f\"RMSE of the best Stage 2 Model: {rmse_val_s2}\")\n",
    "\n",
    "# generate predicted health outcomes\n",
    "df_model_s2[y_var_s2+\"_hat\"] = y_s2_hat\n",
    "\n",
    "columns = [y_var_s2, y_var_s2+\"_hat\"]\n",
    "\n",
    "# compute visits by patzip_year_month\n",
    "fig, axes = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(10, 5))\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    sns.histplot(\n",
    "            df_model_s2[columns[idx]],\n",
    "            ax=ax\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicted Health Outcome to df_model_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Generation: Make the 1%, 10%, 25% mitigation pm25_hat columns\n",
    "\n",
    "Make these columns, run the s2 XGB on these columns to get their new HO predicted values\n",
    "\n",
    "Save out csv with columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Column of 0.99*df_model_s2[predictor_name_s2]\n",
    "reduced_pm25_col_name = predictor_name_s2 + \"_1%_reduction\"\n",
    "df_model_s2[reduced_pm25_col_name] = 0.99 * df_model_s2[predictor_name_s2]\n",
    "X_temp = df_model_s2[[reduced_pm25_col_name] + fixed_effects_cols]\n",
    "\n",
    "# Predict Health Outcome for this column, add as column to df_model_s2\n",
    "df_model_s2[y_var_s2 + \"_hat\" + \"_1%_reduction\"] = xgbr_s2_best.predict(X_temp)\n",
    "\n",
    "\n",
    "# Make Column of 0.90*df_model_s2[predictor_name_s2]\n",
    "reduced_pm25_col_name = predictor_name_s2 + \"_10%_reduction\"\n",
    "df_model_s2[reduced_pm25_col_name] = 0.90 * df_model_s2[predictor_name_s2]\n",
    "X_temp = df_model_s2[[reduced_pm25_col_name] + fixed_effects_cols]\n",
    "\n",
    "# Predict Health Outcome for this column, add as column to df_model_s2\n",
    "df_model_s2[y_var_s2 + \"_hat\" + \"_10%_reduction\"] = xgbr_s2_best.predict(X_temp)\n",
    "\n",
    "\n",
    "# Make Column of 0.75*df_model_s2[predictor_name_s2]\n",
    "reduced_pm25_col_name = predictor_name_s2 + \"_25%_reduction\"\n",
    "df_model_s2[reduced_pm25_col_name] = 0.75 * df_model_s2[predictor_name_s2]\n",
    "X_temp = df_model_s2[[reduced_pm25_col_name] + fixed_effects_cols]\n",
    "\n",
    "# Predict Health Outcome for this column, add as column to df_model_s2\n",
    "df_model_s2[y_var_s2 + \"_hat\" + \"_25%_reduction\"] = xgbr_s2_best.predict(X_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Column of 1.01*df_model_s2[predictor_name_s2]\n",
    "reduced_pm25_col_name = predictor_name_s2 + \"_1%_increase\"\n",
    "df_model_s2[reduced_pm25_col_name] = 0.99 * df_model_s2[predictor_name_s2]\n",
    "X_temp = df_model_s2[[reduced_pm25_col_name] + fixed_effects_cols]\n",
    "\n",
    "# Predict Health Outcome for this column, add as column to df_model_s2\n",
    "df_model_s2[y_var_s2 + \"_hat\" + \"_1%_increase\"] = xgbr_s2_best.predict(X_temp)\n",
    "\n",
    "\n",
    "# Make Column of 1.10*df_model_s2[predictor_name_s2]\n",
    "reduced_pm25_col_name = predictor_name_s2 + \"_10%_increase\"\n",
    "df_model_s2[reduced_pm25_col_name] = 0.90 * df_model_s2[predictor_name_s2]\n",
    "X_temp = df_model_s2[[reduced_pm25_col_name] + fixed_effects_cols]\n",
    "\n",
    "# Predict Health Outcome for this column, add as column to df_model_s2\n",
    "df_model_s2[y_var_s2 + \"_hat\" + \"_10%_increase\"] = xgbr_s2_best.predict(X_temp)\n",
    "\n",
    "\n",
    "# Make Column of 1.25*df_model_s2[predictor_name_s2]\n",
    "reduced_pm25_col_name = predictor_name_s2 + \"_25%_increase\"\n",
    "df_model_s2[reduced_pm25_col_name] = 0.75 * df_model_s2[predictor_name_s2]\n",
    "X_temp = df_model_s2[[reduced_pm25_col_name] + fixed_effects_cols]\n",
    "\n",
    "# Predict Health Outcome for this column, add as column to df_model_s2\n",
    "df_model_s2[y_var_s2 + \"_hat\" + \"_25%_increase\"] = xgbr_s2_best.predict(X_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Stage 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_s2.to_csv(os.path.join(out_dir1, saved_stage2_dataset_name + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('w210_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b37bc99320db5ac87e487a0a73a079f690341dc989821c54010d9f7a6cf99c3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
