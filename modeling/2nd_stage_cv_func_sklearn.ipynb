{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Create a function that can be used in the 2nd stage regression to perform a time series cross validation. \n",
    "- Using an expanding window cross validation\n",
    "- Use sklearn API for the xgb function\n",
    "\n",
    "The 2nd stage regression predicts the medical outcomes using the predicted PM2.5 (and separately with the actual pm2.5), as well as the same fixed effects from the first stage regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional. I'm getting annoying warnings that I just want to ignore:\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import requests\n",
    "import urllib\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep this as false unless you want to save out the fitted model objects and results \n",
    "save_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set lag/lead times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1\n",
      "Target Name (target_name_s1) = pm25_r9\n",
      "Predictor Name (predictor_name_s1) = Izmy_v3_normed_D_and_TPY_r9\n",
      "\n",
      "Stage 2\n",
      "Health Outcome Lag Input (HO_lag_input) = _fwd3\n"
     ]
    }
   ],
   "source": [
    "predictor = 'Izmy_v3_normed_D_and_TPY'\n",
    "lead_time = '9'\n",
    "lag_time = '3'\n",
    "lag_style = 'fwd'\n",
    "\n",
    "# define lead time for IV: 'last_month', 'r6', 'r9', 'r12'\n",
    "IV_lead = \"r\" + str(lead_time)\n",
    "HO_lag = lag_style + str(lag_time)\n",
    "\n",
    "if IV_lead:\n",
    "    IV_lead_input = \"_\" + IV_lead \n",
    "else:\n",
    "    # don't add underscore if empty string\n",
    "    IV_lead_input = IV_lead\n",
    "\n",
    "# define lag time for Health Outcome: '', 'fwd3', 'cent3', 'fwd6', 'cent6', 'fwd12', 'cent12'\n",
    "if HO_lag:\n",
    "    HO_lag_input = \"_\" + HO_lag \n",
    "else:\n",
    "    # don't add underscore if empty string\n",
    "    HO_lag_input = HO_lag\n",
    "\n",
    "# IV options: 1 month, 6 months, 9 months, 12 months\n",
    "IV_window_col = [f'pm25{IV_lead_input}']\n",
    "\n",
    "# health outcome options (fwd or cent): 1 month, 3 months, 6 months, 12 months\n",
    "health_outcome_window_col = [f'y_injuries{HO_lag_input}']\n",
    "\n",
    "filter_cols = IV_window_col + health_outcome_window_col # columns to filter out at the beginning and end of df, before modeling\n",
    "\n",
    "target_name_s1 = f'pm25{IV_lead_input}'\n",
    "predictor_name_s1 = f'{predictor}{IV_lead_input}'\n",
    "\n",
    "print(f\"Stage 1\\nTarget Name (target_name_s1) = {target_name_s1}\\nPredictor Name (predictor_name_s1) = {predictor_name_s1}\")\n",
    "\n",
    "print(f\"\\nStage 2\\nHealth Outcome Lag Input (HO_lag_input) = {HO_lag_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path\n",
    "\n",
    "Add a new elif section for your path if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local or gdrive\n",
    "path_source = 'local'\n",
    "\n",
    "if path_source == 'gdrive':\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  data_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/content/gdrive/MyDrive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "  \n",
    "elif path_source == 'local':\n",
    "  data_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/tj/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'\n",
    "\n",
    "elif path_source == 'work':\n",
    "  data_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/Data'\n",
    "  fitted_models_path = '/Users/trevorjohnson/trevorj@berkeley.edu - Google Drive/My Drive/Classes/W210_capstone/W210_Capstone/fitted_models/2022-10-23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our modeling data\n",
    "df = pd.read_csv(os.path.join(data_path, 'modeling_data/modeling_data_joined_11-17-top15_4tpy_ds_wind_ratios.csv'))\n",
    "\n",
    "# read in cornelia's healthcare data\n",
    "df1 = pd.read_csv(os.path.join(data_path, 'medical/hematopoietic_cancers.csv')).iloc[:,1:]\n",
    "df2 = pd.read_csv(os.path.join(data_path, 'medical/pediatric_vasculitis.csv')).iloc[:,1:]\n",
    "df3 = pd.read_csv(os.path.join(data_path, 'medical/type_1_diabetes.csv')).iloc[:,1:]\n",
    "df4 = pd.read_csv(os.path.join(data_path, 'medical/resp_cardio.csv')).iloc[:,1:]\n",
    "df5 = pd.read_csv(os.path.join(data_path, 'medical/injuries_accidents.csv')).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make missing columns that will be added by Matt later\n",
    "df['avg_temp'] = ((df['school_temperature'] + df['ps_temperature']) / 2)\n",
    "df['diff_temp_s_ps'] = (df['school_temperature'] - df['ps_temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_selected_cols(df, cols_to_roll:list = ['Izmy_v1_unnormed'\\\n",
    "    ,'Izmy_v2_nodist_unnormed' \\\n",
    "    ,'Izmy_v3_normed_D_and_TPY' \\\n",
    "    ,'Izmy_v4_nodist_normed_TPY' \\\n",
    "    ,'Izmy_v5_all_normed'\n",
    "    ,'Izmy_v6_unnormed_no_wspd'\n",
    "    ,'Izmy_v7_all_normed_no_wspd'\n",
    "    ,'Izmy_v8_normed_D_and_TPY_no_wspd'\n",
    "    ,'new_alignment_90_high']\n",
    "    ,rolling_periods:list = [1, 6, 9, 12]):\n",
    "\n",
    "    \"\"\"Generates rolling averages for the input variables over the input time periods.\n",
    "    Inputs: df (pd dataframe): contains the data on a y-m level\n",
    "            cols_to_roll (list): list of columns to generate rolling avgs--must be in df\n",
    "            rolling_periods (list): list of time windows (in months) to roll over\n",
    "            \n",
    "    Outputs: df: Pandas dataframe containing the new columns\n",
    "             all_cols: list of list containing the new columns, separated by input type\"\"\"\n",
    "    \n",
    "    df_int = df.copy().sort_values(['school_zip', 'year_month']).reset_index(drop=True)\n",
    "    \n",
    "    all_cols_int = []\n",
    "\n",
    "    # Roll each variable\n",
    "    for col_index in range(len(cols_to_roll)):\n",
    "        new_cols = []\n",
    "\n",
    "        col_to_roll = cols_to_roll[col_index]\n",
    "        rolling_periods = [1, 6, 9, 12]\n",
    "\n",
    "        for period in rolling_periods:\n",
    "            df_int[f'{col_to_roll}_r{period}'] = df_int.groupby('school_zip')[col_to_roll]\\\n",
    "                .apply(lambda x: x.rolling(window=period, min_periods=period, closed='left').mean())\n",
    "            \n",
    "            new_cols.append(col_to_roll + \"_r\" + str(period))\n",
    "\n",
    "        all_cols_int.append([col_to_roll] + new_cols)\n",
    "        \n",
    "    return df_int, all_cols_int\n",
    "\n",
    "\n",
    "cols_to_roll = ['Izmy_v1_unnormed'\\\n",
    "    ,'Izmy_v2_nodist_unnormed' \\\n",
    "    ,'Izmy_v3_normed_D_and_TPY' \\\n",
    "    ,'Izmy_v4_nodist_normed_TPY' \\\n",
    "    ,'Izmy_v5_all_normed_but_wspd_ratio'\n",
    "    ,'Izmy_v6_unnormed_no_wspd'\n",
    "    ,'Izmy_v7_all_normed_no_wspd'\n",
    "    ,'Izmy_v8_normed_D_and_TPY_no_wspd'\n",
    "    ,'new_alignment_90_high'\n",
    "    ,'school_temperature'\n",
    "    ,'ps_temperature'\n",
    "    ,'avg_temp'\n",
    "    ,'diff_temp_s_ps'\n",
    "    ,'avg_count_ps_within_5km'\n",
    "    ,'avg_school_wspd'\n",
    "    ,'avg_ps_wspd'\n",
    "    ,'ps_pm25_tpy_top_20'\n",
    "    ,'school_to_ps_geod_dist_m_top_20'\n",
    "    ,'avg_wspd_top_20']\n",
    "\n",
    "rolling_periods = [1, 6, 9, 12]\n",
    "\n",
    "df, all_cols = roll_selected_cols(df=df, cols_to_roll=cols_to_roll, rolling_periods=rolling_periods)\n",
    "\n",
    "# rename the last month column just to be consistent and safe\n",
    "df.rename(columns={'pm25_last_month': 'pm25_r1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in nulls conditionally on merged datasets\n",
    "\n",
    "- the problem: for each health outcome, we want to fill in the nulls for a zipcode with 0's only if that row occurred after the first non-zero/not null visit in that zipcode for that health outcome. Keep them as nulls otherwise.\n",
    "\n",
    "- So basically a zipcode will keep the nulls if they're on a date before the first visit seen for that health outcome, nulls will become 0 after the first visit seen for that health outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nans(df, visits_cols = ['visits_hematopoietic_cancers', 'visits_injuries_accidents',\n",
    "       'visits_type_1_diabetes', 'visits_pediatric_vasculitis',\n",
    "       'visits_resp_cardio']):\n",
    "    \"\"\"Function to generate columns in place that replace NaNs with 0's only if that \n",
    "    row occurred after the first non-zero/not null visit in that zipcode for the specific\n",
    "    health outcome. Keeps them as nulls otherwise.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input dataframe\n",
    "        visits_cols (list, optional): list of columns to selectively filter NaNs\n",
    "    Returns:\n",
    "        DataFrame with columns replaced with their NaN-filtered versions\n",
    "    \"\"\"\n",
    "\n",
    "    def get_rowIndex(row):\n",
    "        \"\"\"Function intended for applying across df rows\n",
    "\n",
    "        Args:\n",
    "            row (int): row\n",
    "\n",
    "        Returns:\n",
    "            int: index of row\n",
    "        \"\"\"\n",
    "      \n",
    "        return row.name\n",
    "\n",
    "    def compare_and_replace(orig_visits, dataset_row_idx, school_zip):\n",
    "        \"\"\"Function intended for applying across df rows\n",
    "         Selectively replaces NaNs with 0's\n",
    "        Args:\n",
    "            orig_visits: original column that needs to be filtered\n",
    "            dataset_row_idx: column with row indices for the entire df\n",
    "            school_zip: column with school zips\n",
    "\n",
    "        Returns:\n",
    "            float or NaN\n",
    "        \"\"\"\n",
    "        \n",
    "        # school zip + zip idx\n",
    "        first_val_row_idx = dict_row_idx[school_zip]\n",
    "        zip_idx = dict_zip_idx[school_zip]\n",
    "        max_idx = dict_max_zipindex_per_zip[school_zip]\n",
    "        difference = max_idx - zip_idx + 1\n",
    "\n",
    "        # check the school zip first\n",
    "        if dataset_row_idx < first_val_row_idx:\n",
    "            orig_visits = orig_visits\n",
    "        elif (dataset_row_idx >= first_val_row_idx) and (dataset_row_idx <=  first_val_row_idx + difference):\n",
    "            if pd.isnull(orig_visits):\n",
    "                orig_visits = 0\n",
    "            else:\n",
    "                orig_visits = orig_visits\n",
    "        return orig_visits\n",
    "        \n",
    "    # group df by school_zip, year_month\n",
    "    df_grouped_schools = df.groupby(['school_zip', 'year_month']).tail(1)\n",
    "\n",
    "    #df_grouped_schools['points_rank'] = df.groupby(['team'])['points'].rank('dense', ascending=False)\n",
    "    unique_school_zips = list(df_grouped_schools['school_zip'].unique())\n",
    "\n",
    "    # generate overall row index\n",
    "    df_grouped_schools['rowIndex'] = df_grouped_schools.apply(get_rowIndex, axis=1)\n",
    "\n",
    "    # generate row indices that rest per school zip\n",
    "    df_grouped_schools['zipIndex'] = df_grouped_schools.groupby(['school_zip'])['year_month'].rank('first', ascending=True).astype(int)\n",
    "    df_grouped_schools['zipIndex'] = df_grouped_schools['zipIndex'] - 1\n",
    "\n",
    "    # generate dictionary that gets max index per school zip\n",
    "    dict_max_zipindex_per_zip = {}\n",
    "    for i in unique_school_zips:\n",
    "        dict_max_zipindex_per_zip[i] = df_grouped_schools[df_grouped_schools['school_zip']==i]['zipIndex'].max()\n",
    "\n",
    "    for i in visits_cols:\n",
    "        dict_zip_idx = {}\n",
    "        dict_row_idx = {}\n",
    "        for j in unique_school_zips:\n",
    "            temp = df_grouped_schools[df_grouped_schools['school_zip']==j]\n",
    "            #display(temp)\n",
    "            #temp['rowIndex'] = temp.apply(get_rowIndex, axis=1)\n",
    "            visits_series = pd.Series(temp[i]) # one school zip, filtered to 1 health outcome\n",
    "            #display(visits_series)\n",
    "            bool_not_null = visits_series.notnull()\n",
    "            all_indices_not_null = np.where(bool_not_null)[0]\n",
    "\n",
    "            # save index of the first non-NaN value within the zipcode indices\n",
    "            # if everything every value for zip is NaN, set value to # of records in df\n",
    "            try:\n",
    "                groupby_index = all_indices_not_null[0]\n",
    "            except IndexError:\n",
    "                groupby_index = df_grouped_schools.shape[0]\n",
    "            dict_zip_idx[j] = groupby_index\n",
    "            \n",
    "            # save index of the row from whole dataset; set valye to # of records in df if not\n",
    "            try:\n",
    "                row_idx = temp.loc[temp['zipIndex'] == groupby_index, 'rowIndex'].values[0]\n",
    "            except IndexError:\n",
    "                row_idx = df_grouped_schools.shape[0]\n",
    "            dict_row_idx[j] = row_idx\n",
    "        \n",
    "        df_grouped_schools[i] = df_grouped_schools.apply(lambda row: compare_and_replace(row[i], row['rowIndex'], row['school_zip']), axis=1)\n",
    "\n",
    "    # drop rowIndex and zipIndex cols\n",
    "    df_grouped_schools.drop(columns=['rowIndex', 'zipIndex'], inplace=True)\n",
    "\n",
    "    return df_grouped_schools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# med data:\n",
    "\n",
    "# get all distinct patzip_year_month\n",
    "all_pats = df1['patzip_year_month'].to_list() + \\\n",
    "  df2['patzip_year_month'].to_list() + \\\n",
    "  df3['patzip_year_month'].to_list() + \\\n",
    "  df4['patzip_year_month'].to_list() + \\\n",
    "  df5['patzip_year_month'].to_list() \n",
    "all_pats = list(set(all_pats))\n",
    "df_med = pd.DataFrame({'patzip_year_month': all_pats})\n",
    "\n",
    "# rename columns more intuitively\n",
    "df1 = df1.rename(columns={'number_of_visits': 'number_of_visits_hem_cancers'})\n",
    "df2 = df2.rename(columns={'number_of_visits': 'number_of_visits_vasc'})\n",
    "df3 = df3.rename(columns={'number_of_visits': 'number_of_visits_diab'})\n",
    "df4 = df4.rename(columns={'number_of_visits': 'number_of_visits_resp_cardio'})\n",
    "df5 = df5.rename(columns={'number_of_visits': 'number_of_visits_injuries'})\n",
    "\n",
    "# now join all the diagnoses on this dataset\n",
    "df_med = df_med\\\n",
    "  .merge(df1, on='patzip_year_month', how='left')\\\n",
    "  .merge(df2, on='patzip_year_month', how='left')\\\n",
    "  .merge(df3, on='patzip_year_month', how='left')\\\n",
    "  .merge(df4, on='patzip_year_month', how='left')\\\n",
    "  .merge(df5, on='patzip_year_month', how='left')\n",
    "\n",
    "# join data\n",
    "if isinstance(df.year_month[0], str):\n",
    "  # if year month is still a string, convert it to datetime\n",
    "  # don't try if already converted\n",
    "    df['year_month'] = df['year_month'].map(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "df['zip_year_month'] = df['school_zip'].astype(str) + '-' +\\\n",
    "  df['year_month'].dt.year.astype(str) + '-' +\\\n",
    "  df['year_month'].dt.month.astype(str)\n",
    "\n",
    "df = pd.merge(df, df_med, left_on='zip_year_month', right_on='patzip_year_month', how='left')\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# for missing med data, assume there were 0 cases:\n",
    "med_vars = ['hematopoietic_cancers', 'number_of_visits_hem_cancers', \n",
    "  'pediatric_vasculitis', 'number_of_visits_vasc', \n",
    "  'type_1_diabetes', 'number_of_visits_diab',\n",
    "  'resp_cardio', 'number_of_visits_resp_cardio',\n",
    "  'injuries_accidents', 'number_of_visits_injuries'\n",
    "  ]\n",
    "\n",
    "\n",
    "# for var in med_vars:\n",
    "#   df[var] = df[var].fillna(0)\n",
    "\n",
    "\n",
    "# df.sort_values(['school_zip', 'year_month'], inplace=True)\n",
    "\n",
    "# Insert code here to populate na's for each HO with 0, only if there was a HO in this zipcode before\n",
    "df = filter_nans(df, visits_cols = ['number_of_visits_hem_cancers', 'number_of_visits_vasc', \n",
    "'number_of_visits_diab', 'number_of_visits_resp_cardio', 'number_of_visits_injuries'])\n",
    "\n",
    "\n",
    "# fixing month datatype\n",
    "df['month'] = df['month'].astype(str)\n",
    "\n",
    "# Create response variables, which is visits / population\n",
    "df['y_hematopoietic'] = 1000 * df['number_of_visits_hem_cancers'] / df['total_pop_under19']\n",
    "df['y_vasculitis'] = 1000 * df['number_of_visits_vasc'] / df['total_pop_under19']\n",
    "df['y_diabetes'] = 1000 * df['number_of_visits_diab'] / df['total_pop_under19']\n",
    "df['y_resp_cardio'] = 1000 * df['number_of_visits_resp_cardio'] / df['total_pop_under19']\n",
    "df['y_injuries'] = 1000 * df['number_of_visits_injuries'] / df['total_pop_under19']\n",
    "\n",
    "# Create an option for a logged version of the treatment var (log(1+x)). this makes it normally distributed \n",
    "df['pm25_log'] = np.log1p(df['pm25'])\n",
    "\n",
    "# create year trend feature\n",
    "df['year_trend'] = df['year'] - 1999\n",
    "\n",
    "# create county_month\n",
    "df['county_month'] = df.apply(lambda df: df['month'].rjust(2, '0') + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# create year_month_county (in case we want to just direclty use this var for the interaction effects)\n",
    "df['year_month_county'] = df.apply(lambda df: str(df['year']) + '_' + df['month'] + '_' + df['school_county_v2'], axis=1)\n",
    "\n",
    "# no need to one hot encode anymore, b/c data is already encoded \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Rolling HO Sum Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data on date\n",
    "df = df.sort_values('year_month').reset_index(drop=True)\n",
    "\n",
    "# train/test split \n",
    "# keep 2018 as the held out test set \n",
    "df_test = df[df.year == 2018]\n",
    "df = df[df.year != 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns added for health outcomes using 3 month window:\n",
      "['number_of_visits_hem_cancers_fwd3', 'y_hematopoietic_fwd3', 'number_of_visits_hem_cancers_cent3', 'y_hematopoietic_cent3', 'number_of_visits_vasc_fwd3', 'y_vasculitis_fwd3', 'number_of_visits_vasc_cent3', 'y_vasculitis_cent3', 'number_of_visits_diab_fwd3', 'y_diabetes_fwd3', 'number_of_visits_diab_cent3', 'y_diabetes_cent3', 'number_of_visits_resp_cardio_fwd3', 'y_resp_cardio_fwd3', 'number_of_visits_resp_cardio_cent3', 'y_resp_cardio_cent3', 'number_of_visits_injuries_fwd3', 'y_injuries_fwd3', 'number_of_visits_injuries_cent3', 'y_injuries_cent3']\n",
      "\n",
      "Columns added for health outcomes using 6 month window:\n",
      "['number_of_visits_hem_cancers_fwd6', 'y_hematopoietic_fwd6', 'number_of_visits_hem_cancers_cent6', 'y_hematopoietic_cent6', 'number_of_visits_vasc_fwd6', 'y_vasculitis_fwd6', 'number_of_visits_vasc_cent6', 'y_vasculitis_cent6', 'number_of_visits_diab_fwd6', 'y_diabetes_fwd6', 'number_of_visits_diab_cent6', 'y_diabetes_cent6', 'number_of_visits_resp_cardio_fwd6', 'y_resp_cardio_fwd6', 'number_of_visits_resp_cardio_cent6', 'y_resp_cardio_cent6', 'number_of_visits_injuries_fwd6', 'y_injuries_fwd6', 'number_of_visits_injuries_cent6', 'y_injuries_cent6']\n",
      "\n",
      "Columns added for health outcomes using 12 month window:\n",
      "['number_of_visits_hem_cancers_fwd12', 'y_hematopoietic_fwd12', 'number_of_visits_hem_cancers_cent12', 'y_hematopoietic_cent12', 'number_of_visits_vasc_fwd12', 'y_vasculitis_fwd12', 'number_of_visits_vasc_cent12', 'y_vasculitis_cent12', 'number_of_visits_diab_fwd12', 'y_diabetes_fwd12', 'number_of_visits_diab_cent12', 'y_diabetes_cent12', 'number_of_visits_resp_cardio_fwd12', 'y_resp_cardio_fwd12', 'number_of_visits_resp_cardio_cent12', 'y_resp_cardio_cent12', 'number_of_visits_injuries_fwd12', 'y_injuries_fwd12', 'number_of_visits_injuries_cent12', 'y_injuries_cent12']\n"
     ]
    }
   ],
   "source": [
    "# get rolling n month sum\n",
    "def create_rolling_sum(df, var_name:str = 'number_of_visits_hem_cancers', num_months=3, center_arg:bool = False):\n",
    "  \"\"\"\n",
    "    Creates rolling sums for the number of visits for a given health outcome. \n",
    "    Overwrite your dataframe with the output.\n",
    "    Function saves the result as a column into the dataframe with subscripts \n",
    "    - '{var_name}_fwd{number of months}' for forward sums\n",
    "    - '{var_name}_cent{number of months}' for centered sums\n",
    "\n",
    "    Function includes the current month as one of the months in num_months.\n",
    "\n",
    "    Dataframe input MUST be sorted by ['school_zip', 'year_month'] ahead of time.\n",
    "\n",
    "    `df = df.sort_values(['school_zip', 'year_month'])`\n",
    "\n",
    "    Suggested: filter out tail end of dates so rolling averages are not filled with imputed values.\n",
    "\n",
    "  Args:\n",
    "      `df` (dataframe): dataframe having columns for 'school_zip', datetime 'year_month', and number of visits. Dataframe must be sorted by \n",
    "      `var_name` (str, optional): health outcome number of visits. Defaults to 'number_of_visits_hem_cancers'.\n",
    "      `num_months` (int, optional): Number of months to take rolling sum over. Defaults to 3.\n",
    "      `center_arg` (bool, optional): If this sum should be centered on current month. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "      `df_int`: returns dataframe with column added\n",
    "  \"\"\"\n",
    "  df_int = df.copy().sort_values(['school_zip', 'year_month']).reset_index(drop=True)\n",
    "  \n",
    "  if center_arg:\n",
    "    df_int[f'{var_name}_cent{num_months}'] = df_int.groupby('school_zip')[var_name]\\\n",
    "                                      .apply(lambda x:x.rolling(num_months, center=True).sum())\n",
    "  else:\n",
    "    df_int[f'{var_name}_fwd{num_months}'] = df_int.groupby('school_zip')[var_name]\\\n",
    "                                      .apply(lambda x:x.rolling(num_months).sum().shift(1-num_months))\n",
    "\n",
    "  \n",
    "  return df_int \n",
    "\n",
    "\n",
    "df = df.sort_values(['school_zip', 'year_month'])\n",
    "starting_cols = list(df.columns)\n",
    "\n",
    "num_visits_col_names = ['number_of_visits_hem_cancers', \n",
    "  'number_of_visits_vasc', \n",
    "  'number_of_visits_diab',\n",
    "  'number_of_visits_resp_cardio',\n",
    "  'number_of_visits_injuries'\n",
    "  ]\n",
    "\n",
    "y_col_names = ['y_hematopoietic', \n",
    "  'y_vasculitis', \n",
    "  'y_diabetes',\n",
    "  'y_resp_cardio',\n",
    "  'y_injuries'\n",
    "  ]\n",
    "\n",
    "# 3 months ---\n",
    "n = 3 # specify number of months\n",
    "\n",
    "for health_outcome_y_col, health_outcome_visits_col in zip(y_col_names, num_visits_col_names):\n",
    "    # forward looking columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=False)\n",
    "    df[f'{health_outcome_y_col}_fwd{n}'] = 1000 * df[f'{health_outcome_visits_col}_fwd{n}'] / df['total_pop_under19']\n",
    "\n",
    "    # centered columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=True)\n",
    "    df[f'{health_outcome_y_col}_cent{n}'] = 1000 * df[f'{health_outcome_visits_col}_cent{n}'] / df['total_pop_under19']\n",
    "\n",
    "\n",
    "# print columns added\n",
    "ending_cols = list(df.columns)\n",
    "window_3months_columns = [c for c in ending_cols if c not in starting_cols]\n",
    "print(f\"\\nColumns added for health outcomes using 3 month window:\\n{window_3months_columns}\")\n",
    "starting_cols = list(df.columns)\n",
    "\n",
    "# 6 months ---\n",
    "n = 6 # specify number of months\n",
    "\n",
    "for health_outcome_y_col, health_outcome_visits_col in zip(y_col_names, num_visits_col_names):\n",
    "    # forward looking columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=False)\n",
    "    df[f'{health_outcome_y_col}_fwd{n}'] = 1000 * df[f'{health_outcome_visits_col}_fwd{n}'] / df['total_pop_under19']\n",
    "\n",
    "    # centered columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=True)\n",
    "    df[f'{health_outcome_y_col}_cent{n}'] = 1000 * df[f'{health_outcome_visits_col}_cent{n}'] / df['total_pop_under19']\n",
    "\n",
    "\n",
    "# print columns added\n",
    "ending_cols = list(df.columns)\n",
    "window_6months_columns = [c for c in ending_cols if c not in starting_cols]\n",
    "print(f\"\\nColumns added for health outcomes using 6 month window:\\n{window_6months_columns}\")\n",
    "starting_cols = list(df.columns)\n",
    "\n",
    "\n",
    "# 12 months ---\n",
    "n = 12\n",
    "\n",
    "for health_outcome_y_col, health_outcome_visits_col in zip(y_col_names, num_visits_col_names):\n",
    "    # forward looking columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=False)\n",
    "    df[f'{health_outcome_y_col}_fwd{n}'] = 1000 * df[f'{health_outcome_visits_col}_fwd{n}'] / df['total_pop_under19']\n",
    "\n",
    "    # centered columns\n",
    "    df = create_rolling_sum(df=df, var_name=health_outcome_visits_col, num_months=n, center_arg=True)\n",
    "    df[f'{health_outcome_y_col}_cent{n}'] = 1000 * df[f'{health_outcome_visits_col}_cent{n}'] / df['total_pop_under19']\n",
    "\n",
    "\n",
    "ending_cols = list(df.columns)\n",
    "window_12months_columns = [c for c in ending_cols if c not in starting_cols]\n",
    "print(f\"\\nColumns added for health outcomes using 12 month window:\\n{window_12months_columns}\")\n",
    "starting_cols = list(df.columns)\n",
    "\n",
    "# filter data to appropriate data range\n",
    "df = df[df.year >= 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables for modeling\n",
    "date_var = 'year_month' \n",
    "zip_var = 'school_zip'\n",
    "y_var_s2 = 'y_hematopoietic' + HO_lag_input\n",
    "\n",
    "# stage 1 variables\n",
    "instruments_cols = ['Izmy_v1_unnormed',\\\n",
    "                    'Izmy_v2_nodist_unnormed', \\\n",
    "                    'Izmy_v3_normed_D_and_TPY',\n",
    "                    'Izmy_v4_nodist_normed_TPY',\n",
    "                    'Izmy_v5_all_normed_but_wspd_ratio',\n",
    "                    'Izmy_v6_unnormed_no_wspd',\n",
    "                    'Izmy_v7_all_normed_no_wspd',\n",
    "                    'Izmy_v8_normed_D_and_TPY_no_wspd',\n",
    "                    'new_alignment_90_high']\n",
    "\n",
    "stage_1_IVs = [s + IV_lead_input for s in instruments_cols]\n",
    "stage_1_target = [target_name_s1]\n",
    "\n",
    "# stage 2 variables\n",
    "stage_2_HO_targets = [s + HO_lag_input for s in y_col_names]\n",
    "\n",
    "num_vars = ['school_elevation_m', 'nearby_point_source_count', 'school_wspd', \\\n",
    "            'tax_liability_per_capita', 'school_temperature', 'school_count', 'pm25_r6', 'pm25_r12']\n",
    "counties = [i for i in df.columns if re.search('^school_county_v2_', i)]\n",
    "months = [i for i in df.columns if re.search ('^month_', i)]\n",
    "# potentially use county_month instead of the above \n",
    "\n",
    "xvars = num_vars + counties + months \n",
    "yvar = [y_var_s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb stage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "\n",
      "Stage 1 Variables---\n",
      "\n",
      "target_name_s1: pm25_r9\n",
      "\n",
      "predictor_name_s1: Izmy_v3_normed_D_and_TPY_r9\n",
      "\n",
      "Fixed Effects (fixed_effects_cols): ['avg_temp_r9', 'avg_elevation_diff_m', 'school_county_v2_alameda', 'school_county_v2_alpine', 'school_county_v2_amador', 'school_county_v2_butte', 'school_county_v2_calaveras', 'school_county_v2_colusa', 'school_county_v2_contra_costa', 'school_county_v2_del_norte', 'school_county_v2_el_dorado', 'school_county_v2_fresno', 'school_county_v2_glenn', 'school_county_v2_humboldt', 'school_county_v2_imperial', 'school_county_v2_inyo', 'school_county_v2_kern', 'school_county_v2_kings', 'school_county_v2_lake', 'school_county_v2_lassen', 'school_county_v2_los_angeles', 'school_county_v2_madera', 'school_county_v2_marin', 'school_county_v2_mariposa', 'school_county_v2_mendocino', 'school_county_v2_merced', 'school_county_v2_modoc', 'school_county_v2_mono', 'school_county_v2_monterey', 'school_county_v2_napa', 'school_county_v2_nevada', 'school_county_v2_orange', 'school_county_v2_placer', 'school_county_v2_plumas', 'school_county_v2_riverside', 'school_county_v2_sacramento', 'school_county_v2_san_benito', 'school_county_v2_san_bernardino', 'school_county_v2_san_diego', 'school_county_v2_san_francisco', 'school_county_v2_san_joaquin', 'school_county_v2_san_luis_obispo', 'school_county_v2_san_mateo', 'school_county_v2_santa_barbara', 'school_county_v2_santa_clara', 'school_county_v2_santa_cruz', 'school_county_v2_shasta', 'school_county_v2_sierra', 'school_county_v2_siskiyou', 'school_county_v2_solano', 'school_county_v2_sonoma', 'school_county_v2_stanislaus', 'school_county_v2_sutter', 'school_county_v2_tehama', 'school_county_v2_trinity', 'school_county_v2_tulare', 'school_county_v2_tuolumne', 'school_county_v2_ventura', 'school_county_v2_yolo', 'school_county_v2_yuba', 'month_01', 'month_02', 'month_03', 'month_04', 'month_05', 'month_06', 'month_07', 'month_08', 'month_09', 'month_10', 'month_11', 'month_12', 'year_trend']\n",
      "\n",
      "Saving predictions (target_name_s1_predictions) as `pm25_r9_hat` and as `y_hat`\n",
      "Size of df before filtering for modeling: (262674, 305)\n",
      "Size of df after filtering for modeling: (262450, 305)\n"
     ]
    }
   ],
   "source": [
    "# specify the fixed effects column here\n",
    "fixed_effects_cols = ['avg_temp_r9', 'avg_elevation_diff_m'] + counties + months + ['year_trend']\n",
    "\n",
    "# check if all these columns are in the dataframe\n",
    "in_col_list = [True if i in df.columns else i for i in fixed_effects_cols ]\n",
    "\n",
    "print(f\"{in_col_list}\")\n",
    "\n",
    "print(f\"\\nStage 1 Variables---\\n\")\n",
    "print(f\"target_name_s1: {target_name_s1}\\n\")\n",
    "print(f\"predictor_name_s1: {predictor_name_s1}\\n\")\n",
    "print(f\"Fixed Effects (fixed_effects_cols): {fixed_effects_cols}\\n\")\n",
    "\n",
    "target_name_s1_predictions = target_name_s1 + \"_hat\"\n",
    "print(f\"Saving predictions (target_name_s1_predictions) as `{target_name_s1_predictions}` and as `y_hat`\")\n",
    "\n",
    "# create a df for modeling stage 1: drops nulls in all columns used\n",
    "df_model_s1 = df.dropna(subset=([target_name_s1, predictor_name_s1] + fixed_effects_cols))\n",
    "\n",
    "print(f\"Size of df before filtering for modeling: {df.shape}\")\n",
    "print(f\"Size of df after filtering for modeling: {df_model_s1.shape}\")\n",
    "\n",
    "\n",
    "X = df_model_s1[[predictor_name_s1] + fixed_effects_cols]\n",
    "y = df_model_s1[target_name_s1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross val for the best hyperparams \n",
    "\n",
    "Note, I didn't run this again bc it takes forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters to search over\n",
    "params_stage1 = {'max_depth': [3,6,10, 15, 18],\n",
    "                'learning_rate': [0.01, 0.05, 0.1],\n",
    "                'n_estimators': [100, 140, 180],\n",
    "                'colsample_bytree': [1]}\n",
    "\n",
    "xgbr_s1 = xgb.XGBRegressor(seed = 20)   # give a seed for reproducibility\n",
    "\n",
    "# how to force gridsearch to take 1 cv split: https://stackoverflow.com/a/44682305\n",
    "clf_s1 = GridSearchCV(estimator=xgbr_s1, \n",
    "                   param_grid=params_stage1,\n",
    "                   scoring='neg_mean_squared_error', \n",
    "                   verbose=2,\n",
    "                   cv=[(slice(None), slice(None))])\n",
    "clf_s1.fit(X, y)\n",
    "print(\"Best parameters:\", clf_s1.best_params_)\n",
    "print(\"Lowest RMSE: \", (-clf_s1.best_score_)**(1/2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best result preds\n",
    "y_hat = clf_s1.best_estimator_.predict(X)\n",
    "rmse_val = np.mean(((y - y_hat)**2)**.5)\n",
    "print(rmse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit one more model w/ best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit one more model w/ best hyperparams\n",
    "xgbr_s1_best = xgb.XGBRegressor(**clf_s1.best_params_, random_state=20)\n",
    "xgbr_s1_best.fit(X, y)\n",
    "y_hat = xgbr_s1_best.predict(X)\n",
    "\n",
    "# make sure its the same\n",
    "rmse_val = np.mean(((y - y_hat)**2)**.5)\n",
    "print(rmse_val)\n",
    "\n",
    "df_model_s1[target_name_s1+\"_hat\"] = y_hat\n",
    "\n",
    "columns = [target_name_s1, target_name_s1+'_hat']\n",
    "\n",
    "# compute visits by patzip_year_month\n",
    "fig, axes = plt.subplots(1, 2, sharex=False, sharey=False, figsize=(10, 5))\n",
    "\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    sns.histplot(\n",
    "            df_model_s1[columns[idx]],\n",
    "            ax=ax\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset \n",
    "df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[11:17:38] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000016abf4260 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000016ac5898c xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4000\n  [bt] (2) 3   libxgboost.dylib                    0x000000016ac57890 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, xgboost::StringView, xgboost::StringView) + 164\n  [bt] (3) 4   libxgboost.dylib                    0x000000016abf55f4 XGDMatrixSetInfoFromInterface + 224\n  [bt] (4) 5   libffi.dylib                        0x00000001ab624050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001ab62cae8 ffi_call_int + 1208\n  [bt] (6) 7   _ctypes.cpython-311-darwin.so       0x0000000102b780e0 _ctypes_callproc + 1372\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102b71f5c PyCFuncPtr_call + 204\n  [bt] (8) 9   Python                              0x000000010332a568 _PyObject_MakeTpCall + 128\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 86\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m# fit xgb\u001b[39;00m\n\u001b[1;32m     79\u001b[0m xgb_reg \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(\n\u001b[1;32m     80\u001b[0m     booster\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgbtree\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     \n\u001b[1;32m     82\u001b[0m     \u001b[39m# our hyperparams\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhyperparams_i,\n\u001b[1;32m     84\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m xgb_reg \u001b[39m=\u001b[39m xgb_reg\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mdf_train[xvars], y\u001b[39m=\u001b[39;49mdf_train[yvar], eval_set\u001b[39m=\u001b[39;49m(df_test[xvars], df_test[yvar]))\n\u001b[1;32m     87\u001b[0m xgb_reg\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:1014\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m   1013\u001b[0m     evals_result: TrainingCallback\u001b[39m.\u001b[39mEvalsLog \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1014\u001b[0m     train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1015\u001b[0m         missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m   1016\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1017\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   1018\u001b[0m         group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1019\u001b[0m         qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1020\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1021\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1022\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m   1023\u001b[0m         eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m   1024\u001b[0m         sample_weight_eval_set\u001b[39m=\u001b[39;49msample_weight_eval_set,\n\u001b[1;32m   1025\u001b[0m         base_margin_eval_set\u001b[39m=\u001b[39;49mbase_margin_eval_set,\n\u001b[1;32m   1026\u001b[0m         eval_group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1027\u001b[0m         eval_qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1028\u001b[0m         create_dmatrix\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dmatrix,\n\u001b[1;32m   1029\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[1;32m   1030\u001b[0m         feature_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_types,\n\u001b[1;32m   1031\u001b[0m     )\n\u001b[1;32m   1032\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:448\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    429\u001b[0m     missing: \u001b[39mfloat\u001b[39m,\n\u001b[1;32m    430\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    445\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m    446\u001b[0m     \u001b[39m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     train_dmatrix \u001b[39m=\u001b[39m create_dmatrix(\n\u001b[1;32m    449\u001b[0m         data\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    450\u001b[0m         label\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    451\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    452\u001b[0m         qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    453\u001b[0m         weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    454\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    455\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    456\u001b[0m         missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[1;32m    457\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    458\u001b[0m         feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    459\u001b[0m         ref\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    462\u001b[0m     n_validation \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(eval_set)\n\u001b[1;32m    464\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:934\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m    933\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[39mreturn\u001b[39;00m DMatrix(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:754\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n\u001b[0;32m--> 754\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_info(\n\u001b[1;32m    755\u001b[0m     label\u001b[39m=\u001b[39;49mlabel,\n\u001b[1;32m    756\u001b[0m     weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m    757\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    758\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    759\u001b[0m     qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    760\u001b[0m     label_lower_bound\u001b[39m=\u001b[39;49mlabel_lower_bound,\n\u001b[1;32m    761\u001b[0m     label_upper_bound\u001b[39m=\u001b[39;49mlabel_upper_bound,\n\u001b[1;32m    762\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    763\u001b[0m )\n\u001b[1;32m    765\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names \u001b[39m=\u001b[39m feature_names\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:819\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 819\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_label(label)\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:950\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[39m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[1;32m    944\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[39m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m--> 950\u001b[0m dispatch_meta_backend(\u001b[39mself\u001b[39;49m, label, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/data.py:1121\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m-> 1121\u001b[0m     _meta_from_pandas_series(data, name, dtype, handle)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[39mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/data.py:430\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[0;34m(data, name, dtype, handle)\u001b[0m\n\u001b[1;32m    428\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto_dense()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 430\u001b[0m _meta_from_numpy(data, name, dtype, handle)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/data.py:1037\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMasked array is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1036\u001b[0m interface_str \u001b[39m=\u001b[39m _array_interface(data)\n\u001b[0;32m-> 1037\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [11:17:38] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000016abf4260 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000016ac5898c xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4000\n  [bt] (2) 3   libxgboost.dylib                    0x000000016ac57890 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, xgboost::StringView, xgboost::StringView) + 164\n  [bt] (3) 4   libxgboost.dylib                    0x000000016abf55f4 XGDMatrixSetInfoFromInterface + 224\n  [bt] (4) 5   libffi.dylib                        0x00000001ab624050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001ab62cae8 ffi_call_int + 1208\n  [bt] (6) 7   _ctypes.cpython-311-darwin.so       0x0000000102b780e0 _ctypes_callproc + 1372\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102b71f5c PyCFuncPtr_call + 204\n  [bt] (8) 9   Python                              0x000000010332a568 _PyObject_MakeTpCall + 128\n\n"
     ]
    }
   ],
   "source": [
    "xvars = num_vars + counties + months; yvar = 'y_hematopoietic' \n",
    "hyperparams = {'max_depth': [1, 5], 'subsample': [.8, 1], 'eta': [.1], 'lambda': [1]}; search_type = 'grid'; folds = 5; verbose=1\n",
    "\n",
    "\n",
    "\n",
    "final_res = {'fold':[], 'hyperparams':[], 'rmse_train': [], 'rmse_test': [],\n",
    "                'huber_loss_train': [], 'huber_loss_test': []}\n",
    "df = df[xvars + [yvar]]\n",
    "\n",
    "# set up the time series split class, to do an expanding window cross fold. \n",
    "tss = TimeSeriesSplit(n_splits=folds)\n",
    "tss_folds = tss.split(df)\n",
    "all_folds = [i for i in tss_folds]\n",
    "\n",
    "# get all combinations of hyperparams\n",
    "def expand_grid(hyperparams):\n",
    "  keys = list(hyperparams.keys())\n",
    "  hyperparams_df = pd.DataFrame(np.array(np.meshgrid(*[hyperparams[key_i] for key_i in keys])).T.reshape(-1, len(keys)))\n",
    "  hyperparams_df.columns = keys \n",
    "  return hyperparams_df\n",
    "\n",
    "df_hyperparams = expand_grid(hyperparams)\n",
    "\n",
    "# loss functions\n",
    "def get_rmse(dmat_train, df_train):\n",
    "  ytrue = df_train[yvar].values.flatten()\n",
    "  yhat = booster.predict(dmat_train)\n",
    "  rmse = np.mean(((ytrue - yhat)**2)**.5)\n",
    "  return rmse \n",
    "\n",
    "def get_huber_loss(dmat_train, df_train):\n",
    "  # # Let the delta for Huber Loss be 2*standard deviation of non-zero entries for the y variable\n",
    "  # twice_std = 2 * df_train[df_train[yvar] > 0][yvar].std  \n",
    "\n",
    "  # Let the delta for Huber Loss be 2*standard deviation of the y variable\n",
    "  twice_std = 2 * df_train[yvar].std() \n",
    "\n",
    "  def huber_loss(y_actual,y_predicted,delta):\n",
    "    # https://towardsdatascience.com/understanding-loss-functions-the-smart-way-904266e9393\n",
    "    # approaches MSE for small error an approaches MAE in case of outliers.\n",
    "    delta = 5\n",
    "    total_points = y_actual.size\n",
    "    total_error = 0\n",
    "    for i in range(total_points):\n",
    "      error = np.absolute(y_predicted[i] - y_actual[i])\n",
    "      if error < delta:\n",
    "        huber_error = (error*error)/2\n",
    "      else:\n",
    "        huber_error = delta*(error - (0.5*delta))\n",
    "      total_error+=huber_error\n",
    "    total_huber_error = total_error/total_points\n",
    "    return total_huber_error  # mean huber_loss\n",
    "\n",
    "  ytrue = df_train[yvar].values.flatten()\n",
    "  yhat = booster.predict(dmat_train)\n",
    "\n",
    "  huber_loss_val = huber_loss(y_actual=ytrue, y_predicted=yhat, delta=twice_std)\n",
    "\n",
    "  return huber_loss_val\n",
    "\n",
    "\n",
    "# test out one fold\n",
    "df_train = df.loc[all_folds[0][0]]\n",
    "df_test = df.loc[all_folds[0][1]]\n",
    "\n",
    "# convert to xgb types\n",
    "# dmat_train = xgb.DMatrix(df_train[xvars], df_train[yvar])\n",
    "# dmat_test = xgb.DMatrix(df_test[xvars], df_test[yvar])\n",
    "\n",
    "# loop through all hyperparams, but just do one for testing\n",
    "# hyperparams_i = {x:y for x,y in zip(df_hyperparams.columns, df_hyperparams.loc[param_set_i].to_list())}\n",
    "hyperparams_i = {'n_estimators': 100, 'max_depth': 3, 'learning_rate': .05}\n",
    "        \n",
    "# fix datatype for some vars\n",
    "if 'max_depth' in hyperparams_i.keys():\n",
    "  hyperparams_i['max_depth'] = int(hyperparams_i['max_depth'])\n",
    "\n",
    "# fit xgb\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    \n",
    "    # our hyperparams\n",
    "    **hyperparams_i,\n",
    ")\n",
    "\n",
    "xgb_reg = xgb_reg.fit(X=df_train[xvars], y=df_train[yvar], eval_set=(df_test[xvars], df_test[yvar]))\n",
    "xgb_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    float64\n",
       "5      int64\n",
       "Name: type, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.dtypes.to_frame().reset_index()\n",
    "temp.columns = ['col', 'type']\n",
    "temp['type'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save results\n",
    "rmse_train = get_rmse(dmat_train, df_train)\n",
    "rmse_test = get_rmse(dmat_test, df_test)\n",
    "huber_train = get_huber_loss(dmat_train, df_train)\n",
    "huber_test = get_huber_loss(dmat_test, df_test)\n",
    "final_res['fold'].append(fold_count)\n",
    "final_res['hyperparams'].append(hyperparams_i)\n",
    "final_res['rmse_train'].append(rmse_train)\n",
    "final_res['rmse_test'].append(rmse_test)\n",
    "final_res['huber_loss_train'].append(huber_train)\n",
    "final_res['huber_loss_test'].append(huber_test)\n",
    "\n",
    "if verbose == 2:\n",
    "  print('{}: rmse train: {:.3f}, rmse test: {:.3f}'.format(hyperparams_i, rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anand version\n",
    "def time_series_cv(\n",
    "  df: pd.DataFrame, \n",
    "  xvars: list, \n",
    "  yvar: str, \n",
    "  hyperparams: dict = {'max_depth': [1, 5, 10], 'subsample': [.8, 1], 'eta': [.1, .3]}, \n",
    "  search_type='grid', \n",
    "  folds=5, \n",
    "  verbose=1):\n",
    "\n",
    "  ''' \n",
    "  Inputs:\n",
    "  - df: dataframe of your training data\n",
    "  - xvars: a list of all the xvars to pass to xgboost\n",
    "  - yvar: string of your target variable\n",
    "  - verbose: optionality for diff amounts of printouts. Can be 0, 1, 2. 0 = silent, 1 = update after each fold, 2 = update after every single hyperparam combination. \n",
    "  - hyperparams: this must be a dictionary of lists. So each key is a xgb hyperparam, then it must have a list of values to tune with. \n",
    "    See the default for an example. Can put in an arbitrary number of hyperparam options. \n",
    "  \n",
    "  Output:\n",
    "  - dictionary with the following keys: ['fold', 'hyperparams', 'rmse_train', 'rmse_test']. \n",
    "  '''\n",
    "\n",
    "  # this dictionary will hold all the final results\n",
    "  final_res = {'fold':[], 'hyperparams':[], 'rmse_train': [], 'rmse_test': [],\n",
    "                'huber_loss_train': [], 'huber_loss_test': []}\n",
    "\n",
    "  # get only necessary fields in df\n",
    "  df = df[xvars + [yvar]]\n",
    "\n",
    "  # set up the time series split class, to do an expanding window cross fold. \n",
    "  tss = TimeSeriesSplit(n_splits=folds)\n",
    "  tss_folds = tss.split(df)\n",
    "  all_folds = [i for i in tss_folds]\n",
    "\n",
    "  # get all combinations of hyperparams\n",
    "  def expand_grid(hyperparams):\n",
    "    keys = list(hyperparams.keys())\n",
    "    hyperparams_df = pd.DataFrame(np.array(np.meshgrid(*[hyperparams[key_i] for key_i in keys])).T.reshape(-1, len(keys)))\n",
    "    hyperparams_df.columns = keys \n",
    "    return hyperparams_df\n",
    "\n",
    "  df_hyperparams = expand_grid(hyperparams)\n",
    "\n",
    "  # loss functions\n",
    "  def get_rmse(dmat_train, df_train):\n",
    "    ytrue = df_train[yvar].values.flatten()\n",
    "    yhat = booster.predict(dmat_train)\n",
    "    rmse = np.mean(((ytrue - yhat)**2)**.5)\n",
    "    return rmse \n",
    "  \n",
    "  def get_huber_loss(dmat_train, df_train):\n",
    "    # # Let the delta for Huber Loss be 2*standard deviation of non-zero entries for the y variable\n",
    "    # twice_std = 2 * df_train[df_train[yvar] > 0][yvar].std  \n",
    "\n",
    "    # Let the delta for Huber Loss be 2*standard deviation of the y variable\n",
    "    twice_std = 2 * df_train[yvar].std() \n",
    "\n",
    "    def huber_loss(y_actual,y_predicted,delta):\n",
    "      # https://towardsdatascience.com/understanding-loss-functions-the-smart-way-904266e9393\n",
    "      # approaches MSE for small error an approaches MAE in case of outliers.\n",
    "      delta = 5\n",
    "      total_points = y_actual.size\n",
    "      total_error = 0\n",
    "      for i in range(total_points):\n",
    "        error = np.absolute(y_predicted[i] - y_actual[i])\n",
    "        if error < delta:\n",
    "          huber_error = (error*error)/2\n",
    "        else:\n",
    "          huber_error = delta*(error - (0.5*delta))\n",
    "        total_error+=huber_error\n",
    "      total_huber_error = total_error/total_points\n",
    "      return total_huber_error  # mean huber_loss\n",
    "\n",
    "    ytrue = df_train[yvar].values.flatten()\n",
    "    yhat = booster.predict(dmat_train)\n",
    "\n",
    "    huber_loss_val = huber_loss(y_actual=ytrue, y_predicted=yhat, delta=twice_std)\n",
    "\n",
    "    return huber_loss_val\n",
    "\n",
    "  # loop over each expanding time series window\n",
    "  for fold_count,fold in enumerate(all_folds):\n",
    "    if verbose > 0:\n",
    "      print('Working on fold {}/{}'.format(fold_count+1, folds))\n",
    "\n",
    "    df_train = df.loc[fold[0]]\n",
    "    df_test = df.loc[fold[1]]\n",
    "\n",
    "    # convert to xgb types\n",
    "    dmat_train = xgb.DMatrix(df_train[xvars], df_train[yvar])\n",
    "    dmat_test = xgb.DMatrix(df_test[xvars], df_test[yvar])\n",
    "\n",
    "    # within each time series cross fold, perform a grid search with all hyperparam combinations and evaluate results. \n",
    "    if search_type == 'grid':\n",
    "      for param_set_i in range(df_hyperparams.shape[0]):\n",
    "        hyperparams_i = {x:y for x,y in zip(df_hyperparams.columns, df_hyperparams.loc[param_set_i].to_list())}\n",
    "        \n",
    "        # fix datatype for some vars\n",
    "        if 'max_depth' in hyperparams_i.keys():\n",
    "          hyperparams_i['max_depth'] = int(hyperparams_i['max_depth'])\n",
    "\n",
    "        # fit xgb\n",
    "        booster = xgb.train(\n",
    "          hyperparams_i,\n",
    "          dmat_train,\n",
    "          num_boost_round=180, \n",
    "          early_stopping_rounds=15,\n",
    "          evals = [(dmat_train, 'train'), (dmat_test, 'test')], \n",
    "          verbose_eval=False)\n",
    "        \n",
    "        # save results\n",
    "        rmse_train = get_rmse(dmat_train, df_train)\n",
    "        rmse_test = get_rmse(dmat_test, df_test)\n",
    "        huber_train = get_huber_loss(dmat_train, df_train)\n",
    "        huber_test = get_huber_loss(dmat_test, df_test)\n",
    "        final_res['fold'].append(fold_count)\n",
    "        final_res['hyperparams'].append(hyperparams_i)\n",
    "        final_res['rmse_train'].append(rmse_train)\n",
    "        final_res['rmse_test'].append(rmse_test)\n",
    "        final_res['huber_loss_train'].append(huber_train)\n",
    "        final_res['huber_loss_test'].append(huber_test)\n",
    "\n",
    "        if verbose == 2:\n",
    "          print('{}: rmse train: {:.3f}, rmse test: {:.3f}'.format(hyperparams_i, rmse_train, rmse_test))\n",
    "\n",
    "    elif search_type == 'random': \n",
    "      pass \n",
    "      # haven't done this yet\n",
    "  \n",
    "  # print out final best hyperparams before returning the output\n",
    "  output2 = pd.DataFrame({\n",
    "    'hyperparams': final_res['hyperparams'],\n",
    "    'fold': final_res['fold'],\n",
    "    'rmse_train': final_res['rmse_train'],\n",
    "    'rmse_test': final_res['rmse_test']\n",
    "  })\n",
    "  output2['hyperparams'] = output2['hyperparams'].astype(str)\n",
    "  output2 = output2.groupby('hyperparams')[['rmse_train', 'rmse_test']].mean().reset_index().sort_values('rmse_test')\n",
    "  print('best hyperparams: {}'.format(output2.iloc[0,0]))\n",
    "  print(f\"best RMSE test: {output2.iloc[0]['rmse_test']}\")\n",
    "\n",
    "  best_hyperparams = output2.iloc[0,0]\n",
    "\n",
    "  \n",
    "  return final_res, eval(best_hyperparams)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(\n",
    "  df: pd.DataFrame, \n",
    "  xvars: list, \n",
    "  yvar: str, \n",
    "  hyperparams: dict = {'max_depth': [1, 5, 10], 'subsample': [.8, 1], 'eta': [.1, .3]}, \n",
    "  search_type='grid', \n",
    "  folds=5, \n",
    "  verbose=1):\n",
    "\n",
    "  ''' \n",
    "  Inputs:\n",
    "  - df: dataframe of your training data\n",
    "  - xvars: a list of all the xvars to pass to xgboost\n",
    "  - yvar: string of your target variable\n",
    "  - verbose: optionality for diff amounts of printouts. Can be 0, 1, 2. 0 = silent, 1 = update after each fold, 2 = update after every single hyperparam combination. \n",
    "  - hyperparams: this must be a dictionary of lists. So each key is a xgb hyperparam, then it must have a list of values to tune with. \n",
    "    See the default for an example. Can put in an arbitrary number of hyperparam options. \n",
    "  \n",
    "  Output:\n",
    "  - dictionary with the following keys: ['fold', 'hyperparams', 'rmse_train', 'rmse_test']. \n",
    "  '''\n",
    "\n",
    "  # this dictionary will hold all the final results\n",
    "  final_res = {'fold':[], 'hyperparams':[], 'rmse_train': [], 'rmse_test': []}\n",
    "\n",
    "  # get only necessary fields in df\n",
    "  df = df[xvars + [yvar]]\n",
    "\n",
    "  # set up the time series split class, to do an expanding window cross fold. \n",
    "  tss = TimeSeriesSplit(n_splits=folds)\n",
    "  tss_folds = tss.split(df)\n",
    "  all_folds = [i for i in tss_folds]\n",
    "\n",
    "  # get all combinations of hyperparams\n",
    "  def expand_grid(hyperparams):\n",
    "    keys = list(hyperparams.keys())\n",
    "    hyperparams_df = pd.DataFrame(np.array(np.meshgrid(*[hyperparams[key_i] for key_i in keys])).T.reshape(-1, len(keys)))\n",
    "    hyperparams_df.columns = keys \n",
    "    return hyperparams_df\n",
    "\n",
    "  df_hyperparams = expand_grid(hyperparams)\n",
    "\n",
    "  # function to use later\n",
    "  def get_rmse(dmat_train, df_train):\n",
    "    ytrue = df_train[yvar].values.flatten()\n",
    "    yhat = booster.predict(dmat_train)\n",
    "    rmse = np.mean(((ytrue - yhat)**2)**.5)\n",
    "    return rmse \n",
    "\n",
    "  # loop over each expanding time series window\n",
    "  for fold_count,fold in enumerate(all_folds):\n",
    "    if verbose > 0:\n",
    "      print('Working on fold {}/{}'.format(fold_count+1, folds))\n",
    "\n",
    "    df_train = df.loc[fold[0]]\n",
    "    df_test = df.loc[fold[1]]\n",
    "\n",
    "    # convert to xgb types\n",
    "    dmat_train = xgb.DMatrix(df_train[xvars], df_train[yvar])\n",
    "    dmat_test = xgb.DMatrix(df_test[xvars], df_test[yvar])\n",
    "\n",
    "    # within each time series cross fold, perform a grid search with all hyperparam combinations and evaluate results. \n",
    "    if search_type == 'grid':\n",
    "      for param_set_i in range(df_hyperparams.shape[0]):\n",
    "        hyperparams_i = {x:y for x,y in zip(df_hyperparams.columns, df_hyperparams.loc[param_set_i].to_list())}\n",
    "        \n",
    "        # fix datatype for some vars\n",
    "        if 'max_depth' in hyperparams_i.keys():\n",
    "          hyperparams_i['max_depth'] = int(hyperparams_i['max_depth'])\n",
    "\n",
    "        # fit xgb\n",
    "        booster = xgb.train(\n",
    "          hyperparams_i,\n",
    "          dmat_train,\n",
    "          num_boost_round=100, \n",
    "          early_stopping_rounds=15,\n",
    "          evals = [(dmat_train, 'train'), (dmat_test, 'test')], \n",
    "          verbose_eval=False)\n",
    "        \n",
    "        # save results\n",
    "        rmse_train = get_rmse(dmat_train, df_train)\n",
    "        rmse_test = get_rmse(dmat_test, df_test)\n",
    "        final_res['fold'].append(fold_count)\n",
    "        final_res['hyperparams'].append(hyperparams_i)\n",
    "        final_res['rmse_train'].append(rmse_train)\n",
    "        final_res['rmse_test'].append(rmse_test)\n",
    "\n",
    "        if verbose == 2:\n",
    "          print('{}: rmse train: {:.3f}, rmse test: {:.3f}'.format(hyperparams_i, rmse_train, rmse_test))\n",
    "\n",
    "    elif search_type == 'random': \n",
    "      pass \n",
    "      # haven't done this yet\n",
    "  \n",
    "  # print out final best hyperparams before returning the output\n",
    "  output2 = pd.DataFrame({\n",
    "    'hyperparams': final_res['hyperparams'],\n",
    "    'fold': final_res['fold'],\n",
    "    'rmse_train': final_res['rmse_train'],\n",
    "    'rmse_test': final_res['rmse_test']\n",
    "  })\n",
    "  output2['hyperparams'] = output2['hyperparams'].astype(str)\n",
    "  output2 = output2.groupby('hyperparams')[['rmse_train', 'rmse_test']].mean().reset_index().sort_values('rmse_test')\n",
    "  print('best hyperparams: {}'.format(output2.iloc[0,0]))\n",
    "\n",
    "  \n",
    "  return final_res\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold 1/5\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[10:59:02] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000016abf4260 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000016ac5898c xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4000\n  [bt] (2) 3   libxgboost.dylib                    0x000000016ac57890 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, xgboost::StringView, xgboost::StringView) + 164\n  [bt] (3) 4   libxgboost.dylib                    0x000000016abf55f4 XGDMatrixSetInfoFromInterface + 224\n  [bt] (4) 5   libffi.dylib                        0x00000001ab624050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001ab62cae8 ffi_call_int + 1208\n  [bt] (6) 7   _ctypes.cpython-311-darwin.so       0x0000000102b780e0 _ctypes_callproc + 1372\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102b71f5c PyCFuncPtr_call + 204\n  [bt] (8) 9   Python                              0x000000010332a568 _PyObject_MakeTpCall + 128\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[39m=\u001b[39m time_series_cv(df, xvars \u001b[39m=\u001b[39;49m num_vars \u001b[39m+\u001b[39;49m counties \u001b[39m+\u001b[39;49m months, yvar \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39my_hematopoietic\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      2\u001b[0m   hyperparams \u001b[39m=\u001b[39;49m {\u001b[39m'\u001b[39;49m\u001b[39mmax_depth\u001b[39;49m\u001b[39m'\u001b[39;49m: [\u001b[39m1\u001b[39;49m, \u001b[39m5\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39msubsample\u001b[39;49m\u001b[39m'\u001b[39;49m: [\u001b[39m.8\u001b[39;49m, \u001b[39m1\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39meta\u001b[39;49m\u001b[39m'\u001b[39;49m: [\u001b[39m.1\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39mlambda\u001b[39;49m\u001b[39m'\u001b[39;49m: [\u001b[39m1\u001b[39;49m]}, \n\u001b[1;32m      3\u001b[0m   search_type \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mgrid\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      4\u001b[0m   folds \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m   verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [82], line 91\u001b[0m, in \u001b[0;36mtime_series_cv\u001b[0;34m(df, xvars, yvar, hyperparams, search_type, folds, verbose)\u001b[0m\n\u001b[1;32m     88\u001b[0m df_test \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[fold[\u001b[39m1\u001b[39m]]\n\u001b[1;32m     90\u001b[0m \u001b[39m# convert to xgb types\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m dmat_train \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mDMatrix(df_train[xvars], df_train[yvar])\n\u001b[1;32m     92\u001b[0m dmat_test \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(df_test[xvars], df_test[yvar])\n\u001b[1;32m     94\u001b[0m \u001b[39m# within each time series cross fold, perform a grid search with all hyperparam combinations and evaluate results. \u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:754\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n\u001b[0;32m--> 754\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_info(\n\u001b[1;32m    755\u001b[0m     label\u001b[39m=\u001b[39;49mlabel,\n\u001b[1;32m    756\u001b[0m     weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m    757\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    758\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    759\u001b[0m     qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    760\u001b[0m     label_lower_bound\u001b[39m=\u001b[39;49mlabel_lower_bound,\n\u001b[1;32m    761\u001b[0m     label_upper_bound\u001b[39m=\u001b[39;49mlabel_upper_bound,\n\u001b[1;32m    762\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    763\u001b[0m )\n\u001b[1;32m    765\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names \u001b[39m=\u001b[39m feature_names\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:819\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 819\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_label(label)\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:950\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[39m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[1;32m    944\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[39m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m--> 950\u001b[0m dispatch_meta_backend(\u001b[39mself\u001b[39;49m, label, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/data.py:1121\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m-> 1121\u001b[0m     _meta_from_pandas_series(data, name, dtype, handle)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[39mif\u001b[39;00m _is_dlpack(data):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/data.py:430\u001b[0m, in \u001b[0;36m_meta_from_pandas_series\u001b[0;34m(data, name, dtype, handle)\u001b[0m\n\u001b[1;32m    428\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto_dense()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 430\u001b[0m _meta_from_numpy(data, name, dtype, handle)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/data.py:1037\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMasked array is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1036\u001b[0m interface_str \u001b[39m=\u001b[39m _array_interface(data)\n\u001b[0;32m-> 1037\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [10:59:02] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/data/data.cc:461: Check failed: valid: Label contains NaN, infinity or a value too large.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000016abf4260 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000016ac5898c xgboost::MetaInfo::SetInfoFromHost(xgboost::GenericParameter const&, xgboost::StringView, xgboost::Json) + 4000\n  [bt] (2) 3   libxgboost.dylib                    0x000000016ac57890 xgboost::MetaInfo::SetInfo(xgboost::GenericParameter const&, xgboost::StringView, xgboost::StringView) + 164\n  [bt] (3) 4   libxgboost.dylib                    0x000000016abf55f4 XGDMatrixSetInfoFromInterface + 224\n  [bt] (4) 5   libffi.dylib                        0x00000001ab624050 ffi_call_SYSV + 80\n  [bt] (5) 6   libffi.dylib                        0x00000001ab62cae8 ffi_call_int + 1208\n  [bt] (6) 7   _ctypes.cpython-311-darwin.so       0x0000000102b780e0 _ctypes_callproc + 1372\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x0000000102b71f5c PyCFuncPtr_call + 204\n  [bt] (8) 9   Python                              0x000000010332a568 _PyObject_MakeTpCall + 128\n\n"
     ]
    }
   ],
   "source": [
    "output = time_series_cv(df, xvars = num_vars + counties + months, yvar = 'y_hematopoietic', \n",
    "  hyperparams = {'max_depth': [1, 5], 'subsample': [.8, 1], 'eta': [.1], 'lambda': [1]}, \n",
    "  search_type = 'grid', \n",
    "  folds = 5, \n",
    "  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "\n",
    "Organize the results manually. But I put this in the function to spit the best result at the end anyways. \n",
    "\n",
    "But this shows how you can manipulate and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>fold</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.1,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098889</td>\n",
       "      <td>0.131185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.096023</td>\n",
       "      <td>0.128173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 0.8, 'eta': 0.1,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.124585</td>\n",
       "      <td>0.174960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148360</td>\n",
       "      <td>0.198804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090540</td>\n",
       "      <td>0.122365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.255362</td>\n",
       "      <td>0.403228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.254656</td>\n",
       "      <td>0.389784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 1.0, 'eta': 0.3,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.254884</td>\n",
       "      <td>0.387370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.238637</td>\n",
       "      <td>0.399839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.3,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.240153</td>\n",
       "      <td>0.401812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hyperparams  fold  rmse_train  \\\n",
       "0   {'max_depth': 1, 'subsample': 0.8, 'eta': 0.1,...     0    0.098889   \n",
       "1   {'max_depth': 1, 'subsample': 1.0, 'eta': 0.1,...     0    0.096023   \n",
       "2   {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1,...     0    0.124585   \n",
       "3   {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...     0    0.148360   \n",
       "4   {'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...     0    0.090540   \n",
       "..                                                ...   ...         ...   \n",
       "75  {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...     4    0.255362   \n",
       "76  {'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...     4    0.254656   \n",
       "77  {'max_depth': 1, 'subsample': 1.0, 'eta': 0.3,...     4    0.254884   \n",
       "78  {'max_depth': 5, 'subsample': 0.8, 'eta': 0.3,...     4    0.238637   \n",
       "79  {'max_depth': 5, 'subsample': 1.0, 'eta': 0.3,...     4    0.240153   \n",
       "\n",
       "    rmse_test  \n",
       "0    0.131185  \n",
       "1    0.128173  \n",
       "2    0.174960  \n",
       "3    0.198804  \n",
       "4    0.122365  \n",
       "..        ...  \n",
       "75   0.403228  \n",
       "76   0.389784  \n",
       "77   0.387370  \n",
       "78   0.399839  \n",
       "79   0.401812  \n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2 = pd.DataFrame({\n",
    "  'hyperparams': output['hyperparams'],\n",
    "  'fold': output['fold'],\n",
    "  'rmse_train': output['rmse_train'],\n",
    "  'rmse_test': output['rmse_test']\n",
    "})\n",
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperparams</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.1,...</td>\n",
       "      <td>0.185906</td>\n",
       "      <td>0.272150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.1,...</td>\n",
       "      <td>0.185906</td>\n",
       "      <td>0.272149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>0.168777</td>\n",
       "      <td>0.256342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>0.168776</td>\n",
       "      <td>0.256342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>0.185906</td>\n",
       "      <td>0.271774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>0.185906</td>\n",
       "      <td>0.271774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 1.0, 'eta': 0.3,...</td>\n",
       "      <td>0.169190</td>\n",
       "      <td>0.255671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'max_depth': 1, 'subsample': 1.0, 'eta': 0.3,...</td>\n",
       "      <td>0.169190</td>\n",
       "      <td>0.255670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 0.8, 'eta': 0.1,...</td>\n",
       "      <td>0.201704</td>\n",
       "      <td>0.300596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 0.8, 'eta': 0.1,...</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>0.300380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>0.151557</td>\n",
       "      <td>0.267600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 0.8, 'eta': 0.3,...</td>\n",
       "      <td>0.152497</td>\n",
       "      <td>0.267273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>0.197805</td>\n",
       "      <td>0.297465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...</td>\n",
       "      <td>0.196764</td>\n",
       "      <td>0.296322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.3,...</td>\n",
       "      <td>0.154071</td>\n",
       "      <td>0.270412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'max_depth': 5, 'subsample': 1.0, 'eta': 0.3,...</td>\n",
       "      <td>0.154371</td>\n",
       "      <td>0.269291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hyperparams  rmse_train  rmse_test\n",
       "0   {'max_depth': 1, 'subsample': 0.8, 'eta': 0.1,...    0.185906   0.272150\n",
       "1   {'max_depth': 1, 'subsample': 0.8, 'eta': 0.1,...    0.185906   0.272149\n",
       "2   {'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...    0.168777   0.256342\n",
       "3   {'max_depth': 1, 'subsample': 0.8, 'eta': 0.3,...    0.168776   0.256342\n",
       "4   {'max_depth': 1, 'subsample': 1.0, 'eta': 0.1,...    0.185906   0.271774\n",
       "5   {'max_depth': 1, 'subsample': 1.0, 'eta': 0.1,...    0.185906   0.271774\n",
       "6   {'max_depth': 1, 'subsample': 1.0, 'eta': 0.3,...    0.169190   0.255671\n",
       "7   {'max_depth': 1, 'subsample': 1.0, 'eta': 0.3,...    0.169190   0.255670\n",
       "8   {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1,...    0.201704   0.300596\n",
       "9   {'max_depth': 5, 'subsample': 0.8, 'eta': 0.1,...    0.202083   0.300380\n",
       "10  {'max_depth': 5, 'subsample': 0.8, 'eta': 0.3,...    0.151557   0.267600\n",
       "11  {'max_depth': 5, 'subsample': 0.8, 'eta': 0.3,...    0.152497   0.267273\n",
       "12  {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...    0.197805   0.297465\n",
       "13  {'max_depth': 5, 'subsample': 1.0, 'eta': 0.1,...    0.196764   0.296322\n",
       "14  {'max_depth': 5, 'subsample': 1.0, 'eta': 0.3,...    0.154071   0.270412\n",
       "15  {'max_depth': 5, 'subsample': 1.0, 'eta': 0.3,...    0.154371   0.269291"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2['hyperparams'] = output2['hyperparams'].astype(str)\n",
    "output_grp = output2.groupby('hyperparams')[['rmse_train', 'rmse_test']].mean().reset_index()\n",
    "output_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparams: {'max_depth': 1, 'subsample': 0.8, 'eta': 0.1, 'lambda': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print('best hyperparams: {}'.format(output_grp.iloc[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
